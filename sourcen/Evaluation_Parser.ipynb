{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8acde04b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:192: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:280: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:281: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:283: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:284: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:289: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:296: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:297: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:298: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:302: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:303: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:311: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:327: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:331: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:332: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:335: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:336: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:337: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:338: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:339: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:340: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:341: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:354: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:355: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:192: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:280: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:281: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:283: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:284: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:289: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:296: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:297: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:298: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:302: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:303: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:311: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:327: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:331: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:332: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:335: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:336: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:337: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:338: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:339: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:340: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:341: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:354: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:355: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:192: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  editorRegEx = \"\\s*(\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\\s*\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:280: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  doiUrlRegEx1 = \"https:\\/\\/doi\\.org(\\/[^\\s]*)?$\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:281: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  doiUrlRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org)?([^\\s]*)+$\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:283: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  httpsDomainRegEx1 = \"https:\\/\\/doi\\.org\\/\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:284: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  httpsDomainRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org\\/)?\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:289: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  urlRegEx = \"(URL:|url:)?\\s*https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?::\\d+)?(?:/[^\\s]*)?\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:296: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  monthYearRegex = \"(January|Jan\\.?|February|Feb\\.?|March|Mar\\.?|April|Apr\\.?\" \\\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:297: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"|May|May\\.?|June|Jun\\.?|July|Jul\\.?|August|Aug\\.?|September|Sep\\.?|Sept\\.?|October|\" \\\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:298: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"Oct\\.?|November|Nov\\.?|December|Dec\\.?)\\s\\d{4}\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:302: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  yearRegEx1 = \"(\\.|,)? \\(\\d{4}\\)(\\.|,|:)\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:303: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  yearRegEx2 = \"(\\.|,) \\d{4}(\\.|,|;)\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:311: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  pageRegEx = \"(?:pp\\.? )?\\d+(-|–)\\d+\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:327: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  volumeAndNumberRegex = \"(\\d+\\(\\d+\\)|\\d+\\.\\d+)\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:331: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  startIndex, endIndex, volume = getIndexOfSubstring(volumeNumber, [\"\\d+\"])\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:332: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  startIndex, endIndex, number = getIndexOfSubstring(volumeNumber, [\"\\d+\"], True)\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:335: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  volumeRegEx = \"(V|v)ol\\. \\d+\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:336: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  volumeRegEx2 = \", \\d+,\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:337: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  volumeRegEx3 = \",? \\d+(:|\\.)\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:338: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  number1RegEx = \"no\\. \\d+\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:339: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  number2RegEx = \"Issue \\d+\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:340: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  number3RegEx = \", \\d+,\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:341: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  number4RegEx = \"\\.\\d+\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:354: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  editionRegEx1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
      "C:\\Users\\David\\AppData\\Local\\Temp\\ipykernel_8800\\2542721119.py:355: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  editionRegEx2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
      "C:\\Users\\David\\miniconda3\\envs\\bibTexConverter\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Roher Parser (vgl. auch die ggf. ausführlicher annotierte Datei Parser.ipynb)\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "torch.cuda.is_available()\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "def custom_strip(text, replaceCharacter = []):\n",
    "    allowed_chars = string.punctuation + string.whitespace + \"“\" + \"”\"\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return text.strip(allowed_chars)\n",
    "\n",
    "\n",
    "def getIndexOfSubstring(text, regEx = [], reverse = False):\n",
    "    #if reverse = False then it finds the first occurance of a given regEx.\n",
    "    #if reverse = True, then it finds the last occurance of a given regEx.\n",
    "    #beceause the occurance with the max length is taken, it always chooses the regex that covers the most letters\n",
    "    length = 0\n",
    "    matches = []\n",
    "    substring = \"\"\n",
    "    #print(f'regEx: {regEx}')\n",
    "    #print(f'text: {text}')\n",
    "    for regExElement in regEx:\n",
    "        matches = list(re.finditer(regExElement, text))\n",
    "        #print(f'matches: {matches}')\n",
    "        if matches:\n",
    "            if reverse:\n",
    "                match = matches[-1]\n",
    "            else:\n",
    "                match = matches[0]\n",
    "            buffer = match.end() - match.start()\n",
    "            if buffer > length:\n",
    "                length = buffer\n",
    "                startIndex = match.start()\n",
    "                endIndex = match.end()\n",
    "                substring = text[match.start():match.end()]\n",
    "    if substring != \"\":\n",
    "        return startIndex, endIndex, substring   \n",
    "    return -1, -1, substring\n",
    "\n",
    "def replaceSubstring (startIndex, endIndex, text, substituteString, ignorePunctuation = [\"&\", \"(\", \")\"]):\n",
    "    #The regex also checks for punctuation so that it is particularly precise. \n",
    "    #The cut text however should in normale mode be without the front delimiter of the bibTex fields in the bibiography, \n",
    "    #so that future regex are not affected. But the last delimiter belongs to the cut word so this should be removed\n",
    "    if endIndex > 0:\n",
    "        startIndexReplace = 0\n",
    "        endIndexReplace = 0\n",
    "        if startIndex > 0:\n",
    "            for i in range(startIndex, -1, -1):\n",
    "                if isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    startIndexReplace = i + 1\n",
    "                    break\n",
    "        else:\n",
    "            startIndexReplace = 0            \n",
    "        #print(f' replaceSubstring, startIndexReplace={{{startIndexReplace}}}')\n",
    "        if endIndex < len(text):\n",
    "            for i in range(endIndex-1, len(text), 1):\n",
    "                if isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    endIndexReplace = i + 1\n",
    "                    break\n",
    "                elif i == len(text)-1:\n",
    "                    endIndexReplace = len(text)\n",
    "        else:\n",
    "            endIndexReplace = len(text)\n",
    "        #print(f' replaceSubstring, endIndexReplace={{{endIndexReplace}}}')\n",
    "        if endIndexReplace > 0:\n",
    "            changedText = text[0:startIndexReplace] + substituteString + text[endIndexReplace:len(text)]\n",
    "            return changedText, text[startIndexReplace:endIndexReplace]\n",
    "    return text, \"\"\n",
    "\n",
    "def is_SurenameFirst(names):\n",
    "    splitedNames = names.split(\" \")\n",
    "    #print(f'is_SurenameFirst: {splitedNames}')\n",
    "    #regex wie w+ erkennt bspw. KEIN è \n",
    "    if splitedNames[0].endswith(\".\"):\n",
    "        return True\n",
    "    splitedNames = names.split(\",\")\n",
    "    if all(\" \" in item.strip() for item in splitedNames):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def is_NameShortened(df_PER):\n",
    "    for index in df_PER.index.values.tolist():\n",
    "        if \".\" == text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def isSpeceficPunctuation(text, replaceCharacter = []):\n",
    "    allowed_chars = string.punctuation + string.whitespace\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return all(char in allowed_chars for char in text)\n",
    "\n",
    "def is_Editor(editorRegEx, textBetweenNames, startIndexTextBetweenNames, markerBehind = True):\n",
    "    startSubstring, endSubstring, substring = getIndexOfSubstring(textBetweenNames, [editorRegEx])\n",
    "    if startIndexTextBetweenNames > -1 and markerBehind:\n",
    "        if isSpeceficPunctuation(textBetweenNames[startIndexTextBetweenNames:startSubstring], [\"&\"]):\n",
    "            return True, startSubstring + startIndexTextBetweenNames, endSubstring + startIndexTextBetweenNames\n",
    "    elif startIndexTextBetweenNames > -1:\n",
    "        if isSpeceficPunctuation(textBetweenNames[endSubstring:startIndexTextBetweenNames], [\"&\"]):\n",
    "            return True, startSubstring + startIndexTextBetweenNames, endSubstring + startIndexTextBetweenNames\n",
    "    return False, -1, -1\n",
    "\n",
    "def processNames(authors):\n",
    "    finalAuthors = \"\"\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "    surenameFirst = is_SurenameFirst(authors.strip())\n",
    "    authors = custom_strip(authors)\n",
    "    #print(f\"processNames: {authors}\")\n",
    "    if surenameFirst:\n",
    "        startIndex, endIndex, andInAuthors = getIndexOfSubstring(authors, search_terms)\n",
    "        #print(\"Fall surenameFirst\".format(authors))\n",
    "        #hier völlig egal, ob er einzelne Initialen in ein eigenes Word gesteckt hat, obwohl es noch Nachnamen gib\n",
    "        if startIndex >= 0:\n",
    "            authors = authors.replace(andInAuthors, \" and \")\n",
    "            #print(f'authors: {authors}')\n",
    "            finalAuthors = authors.replace(\", \", \" and \")\n",
    "        else:\n",
    "            finalAuthors = authors\n",
    "    elif \"., \" in authors:\n",
    "        #print(\"Fall ., {0}\".format(authors))\n",
    "        search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        authors = authors.replace(\"., \", \"#., \")\n",
    "        if andInAuthors != \"\":\n",
    "            authors = authors.replace(andInAuthors, \"#., \")\n",
    "        authors = authors.split(\"., \")\n",
    "        authors = [name.replace(\"#\",\".\") for name in authors]\n",
    "        authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "        for author in authors[:-1]:\n",
    "            buffer = author.split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "        buffer = authors[-1].split(\", \")\n",
    "        finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "    elif \", \" in authors:\n",
    "        #print(\"Fall , {0}\".format(authors))\n",
    "        search_terms = [\", and \", \", & \", \" and \", \" & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        if andInAuthors != \"\":  \n",
    "            authors = authors.replace(andInAuthors, \", \")\n",
    "        authors = authors.split(\", \")\n",
    "        for i in range(0, len(authors) - 3, 2):\n",
    "            finalAuthors = finalAuthors + authors[i+1] + \" \" + authors[i] + \" and \"\n",
    "        finalAuthors = finalAuthors + authors[len(authors) - 1] + \" \" + authors[len(authors) - 2]\n",
    "    return custom_strip(finalAuthors)\n",
    "\n",
    "def getAuthors(text):\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    \n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    authorsDetected = False\n",
    "    setChainStart = True\n",
    "    startIndexAuthors = -1\n",
    "    endIndexAuthors = -1\n",
    "    chainStartIndex = -1\n",
    "    changedText = \"\"\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    \n",
    "    if not df_PER.empty and df_PER[\"start\"].iloc[0] == 0:\n",
    "        for index in index_df_PER_List:\n",
    "            #beachte: Hiermit lese ich immer schon vor!\n",
    "            if index < len(index_df_PER_List) - 1:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "            else:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "            firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "            onlyAnd = textBetweenNames == andTyp\n",
    "            if setChainStart: \n",
    "                chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            if not onlyPunctuation and not onlyAnd:\n",
    "                setChainStart = True\n",
    "                startIndexAuthors = chainStartIndex\n",
    "                endIndexAuthors = df_PER[\"end\"].iloc[index]\n",
    "                break\n",
    "        if startIndexAuthors > -1:\n",
    "            changedText, author = replaceSubstring(startIndexAuthors, endIndexAuthors, text, \".\")\n",
    "            author = processNames(author)\n",
    "            return changedText, author\n",
    "    return text, \"\"\n",
    "\n",
    "def getEditors(text):\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    editorRegEx = \"\\s*(\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\\s*\"\n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    setChainStart = True\n",
    "    isEditor = False\n",
    "    startIndexEditors = -1\n",
    "    endIndexEditors = -1\n",
    "    chainStartIndex = -1\n",
    "    startIndexEditorMarker = -1\n",
    "    endIndexEditorMarker = -1\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    if not df_PER.empty:\n",
    "        for index in index_df_PER_List:\n",
    "            if index < len(index_df_PER_List) - 1:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "            else:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "            firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "            onlyAnd = textBetweenNames == andTyp\n",
    "            #if true, that a new chain of Authors begins. An Author Chain is for example \"Name1, Name2 and Name3\"\n",
    "            if setChainStart: \n",
    "                chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            #If the following if-STatement is true, than the chain has reached an end\n",
    "            if not onlyPunctuation and not onlyAnd:\n",
    "                setChainStart = True\n",
    "                #editors can be the first Part of an literature reference\n",
    "                textFromStartUntilFirstName = text[0:df_PER[\"start\"].iloc[0]]\n",
    "                isEditor, startIndexEditorMarker, endIndexEditorMarker = is_Editor(editorRegEx, textFromStartUntilFirstName, 0, False)\n",
    "                #print(f' getEditors, startIndexEditorMarker : {startIndexEditorMarker}')\n",
    "                if startIndexEditorMarker == -1:\n",
    "                    isEditor, startIndexEditorMarker, endIndexEditorMarker = is_Editor(editorRegEx, textBetweenNames, df_PER[\"end\"].iloc[index])                         \n",
    "                    #print(f' getEditors, startIndexEditorMarker : {startIndexEditorMarker}')\n",
    "                if isEditor:\n",
    "                    startIndexEditors = chainStartIndex\n",
    "                    endIndexEditors = df_PER[\"end\"].iloc[index]\n",
    "                    break\n",
    "    #print(f'getAuthorsAndEditors: return: {[startIndexAuthors,endIndexAuthors],[startIndexEditors, endIndexEditors]}')\n",
    "    if startIndexEditors > -1:\n",
    "        changedText, editor = replaceSubstring(startIndexEditors, endIndexEditors, text, \".\")\n",
    "        editor = processNames(editor)\n",
    "        if startIndexEditorMarker > -1:\n",
    "            if startIndexEditorMarker > startIndexEditors:\n",
    "                startIndexEditorMarker = startIndexEditorMarker - (len(text) - len(changedText))\n",
    "                endIndexEditorMarker = endIndexEditorMarker - (len(text) - len(changedText))\n",
    "            changedText, buffer = replaceSubstring(startIndexEditorMarker, endIndexEditorMarker, changedText, \".\")\n",
    "        startIndexIn = 0 \n",
    "        if startIndexEditorMarker < startIndexEditors:\n",
    "            startIndexEditors = startIndexEditors - (endIndexEditorMarker - startIndexEditorMarker)\n",
    "            endIndexEditors = endIndexEditors - (endIndexEditorMarker - startIndexEditorMarker)\n",
    "        for i in range(startIndexEditors-1, -1, -1):\n",
    "            if isSpeceficPunctuation(changedText[i], [\":\", \" \"]):\n",
    "                startIndexIn = i + 1\n",
    "                break\n",
    "        #print(f' getEditors, startIndexIn : {startIndexIn}')\n",
    "        #print(f' getEditors, startIndexEditors : {startIndexEditors}')\n",
    "        changedText, replacedEditorMarker = replaceSubstring(startIndexIn, startIndexEditors, changedText, \".\")\n",
    "        return changedText, editor\n",
    "    return text, \"\"\n",
    "    \n",
    "def getPersonTags(text):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty: \n",
    "        return df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getORGTag(text, score):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"ORG\") & (df_outputs[\"score\"] >= score)].reset_index(drop=True).tail(1)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getLOCTag(text):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"LOC\")].reset_index(drop=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getDoi(text):\n",
    "    doiUrlRegEx1 = \"https:\\/\\/doi\\.org(\\/[^\\s]*)?$\"\n",
    "    doiUrlRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org)?([^\\s]*)+$\"\n",
    "    changedText, doi  = getSubstringByRegEx(text, [doiUrlRegEx1, doiUrlRegEx2])\n",
    "    httpsDomainRegEx1 = \"https:\\/\\/doi\\.org\\/\"\n",
    "    httpsDomainRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org\\/)?\"\n",
    "    doi, httpsDomain = getSubstringByRegEx(doi, [httpsDomainRegEx1, httpsDomainRegEx2])\n",
    "    return changedText, custom_strip(doi)\n",
    "\n",
    "def getURL(text):\n",
    "    urlRegEx = \"(URL:|url:)?\\s*https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?::\\d+)?(?:/[^\\s]*)?\"\n",
    "    changedText, url = getSubstringByRegEx(text, [urlRegEx])\n",
    "    urlPrefixRegEx = r\"(url:\\s*|URL:\\s*)\"\n",
    "    url = re.sub(urlPrefixRegEx, '', url).strip()\n",
    "    return changedText, custom_strip(url)\n",
    "\n",
    "def getDate(text):\n",
    "    monthYearRegex = \"(January|Jan\\.?|February|Feb\\.?|March|Mar\\.?|April|Apr\\.?\" \\\n",
    "    \"|May|May\\.?|June|Jun\\.?|July|Jul\\.?|August|Aug\\.?|September|Sep\\.?|Sept\\.?|October|\" \\\n",
    "    \"Oct\\.?|November|Nov\\.?|December|Dec\\.?)\\s\\d{4}\"\n",
    "    changedText, monthYear  = getSubstringByRegEx(text, [monthYearRegex])\n",
    "    #print(f' getDate, text={{{text}}}')\n",
    "    if monthYear == \"\":\n",
    "        yearRegEx1 = \"(\\.|,)? \\(\\d{4}\\)(\\.|,|:)\"\n",
    "        yearRegEx2 = \"(\\.|,) \\d{4}(\\.|,|;)\"\n",
    "        changedText, year  = getSubstringByRegEx(text, [yearRegEx1, yearRegEx2])\n",
    "        #print(f' getDate, changedText={{{changedText}}}')\n",
    "        return changedText, \"\", f'{year}'\n",
    "    monthYear = monthYear.split(' ')\n",
    "    return changedText, f'{monthYear[0]}', f'{monthYear[1]}'\n",
    "\n",
    "def getPage(text):\n",
    "    pageRegEx = \"(?:pp\\.? )?\\d+(-|–)\\d+\"\n",
    "    changedText, pages = getSubstringByRegEx(text, [pageRegEx])\n",
    "    if pages != \"\":\n",
    "        pages = re.search(r'\\d+(-|–)\\d+', pages).group()\n",
    "    return changedText, custom_strip(pages)\n",
    "\n",
    "def getSubstringByRegEx(text, regex = []):\n",
    "    startIndex, endIndex, substring = getIndexOfSubstring(text, regex, True)\n",
    "    #print(f' getSubstringByRegEx, startIndex={{{startIndex}}}')\n",
    "    #print(f' getSubstringByRegEx, startIndex={{{endIndex}}}')\n",
    "    #print(f' getSubstringByRegEx, text={{{substring}}}')\n",
    "    changedText, substring = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "    #print(f' getSubstringByRegEx, changedText={{{changedText}}}')\n",
    "    return changedText, custom_strip(substring)\n",
    "\n",
    "def getVolumeNumber(text):\n",
    "    volumeAndNumberRegex = \"(\\d+\\(\\d+\\)|\\d+\\.\\d+)\"\n",
    "    startIndex, endIndex, substring = getIndexOfSubstring(text, [volumeAndNumberRegex], True)\n",
    "    if startIndex > -1:\n",
    "        changedText, volumeNumber = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        startIndex, endIndex, volume = getIndexOfSubstring(volumeNumber, [\"\\d+\"])\n",
    "        startIndex, endIndex, number = getIndexOfSubstring(volumeNumber, [\"\\d+\"], True)\n",
    "        return changedText, volume, number\n",
    "    else:\n",
    "        volumeRegEx = \"(V|v)ol\\. \\d+\"\n",
    "        volumeRegEx2 = \", \\d+,\"\n",
    "        volumeRegEx3 = \",? \\d+(:|\\.)\"\n",
    "        number1RegEx = \"no\\. \\d+\"\n",
    "        number2RegEx = \"Issue \\d+\"\n",
    "        number3RegEx = \", \\d+,\"\n",
    "        number4RegEx = \"\\.\\d+\"\n",
    "        startIndex, endIndex, volume = getIndexOfSubstring(text, [volumeRegEx, volumeRegEx2, volumeRegEx3], True)\n",
    "        changedText, substring = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        #print(f' getVolumeNumber, changedText={{{changedText}}}')\n",
    "        startIndex, endIndex, number = getIndexOfSubstring(changedText, [number1RegEx, number2RegEx, number3RegEx, number4RegEx], True)\n",
    "        changedText, substring = replaceSubstring(startIndex, endIndex, changedText, \"\")\n",
    "        if volume != \"\":\n",
    "            volume = re.search(r'\\d+', volume).group(0)\n",
    "        if number != \"\":\n",
    "            number = re.search(r'\\d+', number).group(0)\n",
    "        return changedText, volume, number\n",
    "\n",
    "def getEdition(text):\n",
    "    editionRegEx1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "    editionRegEx2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "    changedText, edition = getSubstringByRegEx(text, [editionRegEx1, editionRegEx2])\n",
    "    if edition != \"\":\n",
    "        edition = re.search(r'\\d+', edition).group()\n",
    "    return changedText, custom_strip(edition)\n",
    "\n",
    "def getAddress(text):\n",
    "    df_LOC = getLOCTag(text)\n",
    "    #print(f' df_LOC, df_LOC={{{df_LOC}}}')\n",
    "    addressFound = False\n",
    "    index_df_Loc_List = df_LOC.index.values.tolist()\n",
    "    textBetweenAddress = \"\"\n",
    "    setChainStart = True\n",
    "    startIndex = 0\n",
    "    endIndex = 0\n",
    "    if not df_LOC.empty:\n",
    "        for index in reversed(index_df_Loc_List):\n",
    "            if index < len(index_df_Loc_List) and index > 0:\n",
    "                textBetweenAddress = text[df_LOC[\"end\"].iloc[index-1]:df_LOC[\"start\"].iloc[index]]\n",
    "            else:\n",
    "                textBetweenAddress = text[:df_LOC[\"start\"].iloc[index]]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenAddress, [])\n",
    "            #wenn true, dann beginnt eine neue Autorenkette\n",
    "            if setChainStart: \n",
    "                chainEnIndex = df_LOC[\"end\"].iloc[index]\n",
    "                #Solange das auf False, sollen der Substring erweitert werden, also start bleibt konstant\n",
    "                setChainStart = False\n",
    "            #Dann ist die Addressenkette zu Ende\n",
    "            if not onlyPunctuation:\n",
    "                startIndex = df_LOC[\"start\"].iloc[index]\n",
    "                endIndex = chainEnIndex\n",
    "                break\n",
    "        address = text[startIndex:endIndex]\n",
    "        #If the chained range determined by the tagger corresponds to a string \n",
    "        #that is only delimited by punctuation before and after, then it is most likely a publisher.\n",
    "        #startIndex - 2 because of a space inbetween\n",
    "        #print(f' getAddress, text={{{text}}}')\n",
    "        if startIndex > 2 and endIndex < len(text) - 1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                addressFound = True\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                addressFound = True\n",
    "        if addressFound:\n",
    "            changedText, address = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "            return changedText, custom_strip(address)\n",
    "    return text, \"\"  \n",
    "\n",
    "def getPublisher(text, doi):\n",
    "    publisher = \"\"\n",
    "    if doi != \"\":\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            publisher = data['message'].get('publisher', 'Publisher not found')\n",
    "    if publisher != \"\":\n",
    "        startIndex, endIndex, publisher = getIndexOfSubstring(text, [publisher], True)\n",
    "        #double check\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, publisher\n",
    "        changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        if publisher != \"\":\n",
    "            return changedText, custom_strip(publisher)\n",
    "    df_ORG = getORGTag(text, 0.8)\n",
    "    if not df_ORG.empty:\n",
    "        startIndex = df_ORG[\"start\"].iloc[0]\n",
    "        endIndex = df_ORG[\"end\"].iloc[0]\n",
    "        publisher = text[startIndex:endIndex]\n",
    "        #If the range determined by the tagger corresponds to a string \n",
    "        #that is only delimited by punctuation before and after, then it is most likely a publisher.\n",
    "        #startIndex - 2 because of a space inbetween\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "    return text, \"\"\n",
    "    \n",
    "def getTitel(text):\n",
    "    #print(f' getTitel 1, text={{{text}}}')\n",
    "    ignoreCharacters = [\"?\", \"!\", \"(\", \")\", \"“\", \"”\", \"\\\"\"]\n",
    "    text = custom_strip(text, ignoreCharacters)\n",
    "    ignoreCharacters = [\"?\", \":\", \"-\", \"(\", \")\", \"“\", \"”\", \"\\\"\"]\n",
    "    limit = len(text) - 1\n",
    "    i = 0\n",
    "    maxIndex = 0\n",
    "    #remove pairs of punctuation marks \n",
    "    while i < limit:\n",
    "        if (i + 2 < limit) and not (text[i] == \",\" and text[i+1] == \" \" and not isSpeceficPunctuation(text[i+2])):\n",
    "            if isSpeceficPunctuation(text[i], ignoreCharacters) and isSpeceficPunctuation(text[i+1], ignoreCharacters):\n",
    "                text = text[:i] + \".\" + text[i+2:]\n",
    "                i = i - 1\n",
    "                limit = limit - 1\n",
    "        i = i +1\n",
    "    \n",
    "    #print(f' getTitel 2, text={{{text}}}')\n",
    "    ignoreCharacters = [\"?\", \"!\", \"(\", \")\"]\n",
    "    if text[0] == \"“\":\n",
    "        text = text.rsplit('”', 1)\n",
    "        return custom_strip(text[0], ignoreCharacters), custom_strip(text[1], ignoreCharacters), \"\"\n",
    "    elif text[0] == \"\\\"\":\n",
    "        text = text.rsplit(\"\\\"\", 1)\n",
    "        return custom_strip(text[0], ignoreCharacters), custom_strip(text[1], ignoreCharacters), \"\"\n",
    "    elif text.count(\".\") == 1:\n",
    "        #print(f' getTitel 3, text={{{text}}}')\n",
    "        text = text.split(\".\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    elif text.count(\".\") == 2:\n",
    "        text = text.split(\".\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), custom_strip(text[2])\n",
    "    else:\n",
    "        for index, element in enumerate(text):\n",
    "            if isSpeceficPunctuation(element, [\".\", \",\", \" \", \"(\", \")\", \":\"]):\n",
    "                if maxIndex < index:\n",
    "                    maxIndex = index\n",
    "        if maxIndex > 0:\n",
    "            text = text.split(text[maxIndex])\n",
    "            return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    if maxIndex == 0 and text.count(\",\") == 1: \n",
    "        #print(f' getTitel 4, text={{{text}}}')\n",
    "        text = text.split(\",\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    elif maxIndex == 0 and text.count(\",\") > 1:\n",
    "        text = text.rsplit(',', 1)\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    return custom_strip(text), \"\", \"\"\n",
    "    \n",
    "    \n",
    "def getKey(author, year):\n",
    "    lastNameFirstAuthor = author.split(\" and \")[0].strip().split(\" \")[-1]\n",
    "    return f'{lastNameFirstAuthor}_{year}'\n",
    "\n",
    "\n",
    "def create_bibtex(text):\n",
    "    address = \"\"\n",
    "    author = \"\"\n",
    "    booktitle = \"\"\n",
    "    chapter = \"\"\n",
    "    doi = \"\"\n",
    "    edition = \"\"\n",
    "    editor = \"\"\n",
    "    howpublished = \"\"\n",
    "    isbn = \"\"\n",
    "    journal = \"\"\n",
    "    key = \"\"\n",
    "    month = \"\"\n",
    "    note = \"\"\n",
    "    number = \"\"\n",
    "    organization = \"\"\n",
    "    pages = \"\"\n",
    "    publisher = \"\"\n",
    "    school = \"\"\n",
    "    series = \"\"\n",
    "    title = \"\"\n",
    "    url = \"\"\n",
    "    volume = \"\"\n",
    "    year = \"\"\n",
    "    key = \"\"\n",
    "    isBook = False\n",
    "    isProceedings = False\n",
    "    isInProceedings = False\n",
    "    isIncollection = False\n",
    "    isArticle = False\n",
    "    \n",
    "    text, author = getAuthors(text)\n",
    "    text, editor = getEditors(text)\n",
    "    #print(f' main, text={{{text}}}')\n",
    "    text, doi = getDoi(text)\n",
    "    text, url = getURL(text)\n",
    "    text, month, year = getDate(text)\n",
    "    #print(f' main 2, text={{{text}}}')\n",
    "    text, page = getPage(text)\n",
    "    #print(f' main 3, text={{{text}}}')\n",
    "    text, volume, number = getVolumeNumber(text)\n",
    "    text, edition = getEdition(text)\n",
    "    text, address = getAddress(text)\n",
    "    text, publisher = getPublisher(text, doi)\n",
    "    school = publisher\n",
    "    #print(f' main 4, text={{{text}}}')\n",
    "    title, booktitle, series = getTitel(text)\n",
    "    journal = booktitle\n",
    "    if author != \"\":\n",
    "        key = getKey(author, year)\n",
    "    else:\n",
    "        key = getKey(editor, year)\n",
    "    \n",
    "    bookFields = [author, title, publisher, year, volume, number, \\\n",
    "                  series, address, edition, month, note, key, editor, \\\n",
    "                  howpublished, organization, chapter, pages, isbn, url]\n",
    "    inproceedingsFields = [author, title, booktitle, year, editor, volume, \\\n",
    "                            number, series, pages, address, month, organization, \\\n",
    "                            publisher, note, key, doi, url]\n",
    "    proceedingsFields = [title, year, editor, volume, number, series, \\\n",
    "                          address, month, organization, publisher, note, key, doi, url]\n",
    "    incollectionFields = [author, title, booktitle, publisher, year, editor, \\\n",
    "                           volume, number, series, chapter, pages, address, \\\n",
    "                           edition, month, note, key, doi, url]\n",
    "    articleFields = [author, title, journal, year, volume, number, \\\n",
    "                      pages, month, note, key, doi, url]\n",
    "    phdthesisFields = [author, title, publisher, year, address, month, \\\n",
    "                        note, key, doi, url]\n",
    "    \n",
    "    bookFieldsString = [\"author\", \"title\", \"publisher\", \"year\", \"volume\", \"number\", \\\n",
    "                  \"series\", \"address\", \"edition\", \"month\", \"note\", \"key\", \"editor\", \\\n",
    "                  \"howpublished\", \"organization\", \"chapter\", \"pages\", \"isbn\", \"url\"]\n",
    "    inproceedingsFieldsString  = [\"author\", \"title\", \"booktitle\", \"year\", \"editor\", \"volume\", \\\n",
    "                            \"number\", \"series\", \"pages\", \"address\", \"month\", \"organization\", \\\n",
    "                            \"publisher\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    proceedingsFieldsString  = [\"title\", \"year\", \"editor\", \"volume\", \"number\", \"series\", \\\n",
    "                          \"address\", \"month\", \"organization\", \"publisher\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    incollectionFieldsString  = [\"author\", \"title\", \"booktitle\", \"publisher\", \"year\", \"editor\", \\\n",
    "                           \"volume\", \"number\", \"series\", \"chapter\", \"pages\", \"address\", \\\n",
    "                           \"edition\", \"month\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    articleFieldsString  = [\"author\", \"title\", \"journal\", \"year\", \"volume\", \"number\", \\\n",
    "                      \"pages\", \"month\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    phdthesisFieldsString  = [\"author\", \"title\", \"school\", \"year\", \"address\", \"month\", \\\n",
    "                        \"note\", \"key\", \"doi\", \"url\"]\n",
    "\n",
    "    models = [\n",
    "        \"LaLaf93/proceedings_recognizer\",\n",
    "        \"LaLaf93/inproceedings_recognizer\",\n",
    "        \"LaLaf93/book_recognizer\",\n",
    "        \"LaLaf93/incollection_recognizer\",\n",
    "        \"LaLaf93/article_recognizer\",\n",
    "        \"LaLaf93/phdthesis_recognizer\"\n",
    "    ]\n",
    "\n",
    "    labels = [\n",
    "        \"proceedings\",\n",
    "        \"inproceedings\",\n",
    "        \"book\",\n",
    "        \"incollection\",\n",
    "        \"article\",\n",
    "        \"phdthesis\"\n",
    "    ]\n",
    "    \n",
    "    classifierDict = {}\n",
    "\n",
    "    for model, label in zip(models, labels):\n",
    "        result = pipeline(\"text-classification\", model=model)(text)[0]\n",
    "        classifierDict[label] = result\n",
    "    \n",
    "    #print(f'classifierDict: {classifierDict}')\n",
    "    literatureType = \"\"\n",
    "    highestScore = 0\n",
    "    highetsScoreLabel = \"\"\n",
    "    for entry in classifierDict.values():\n",
    "        if entry['score'] > highestScore and not entry['label'].startswith('NON'):\n",
    "            highestScore = entry['score']\n",
    "            highetsScoreLabel = entry['label']\n",
    "    literatureType = highetsScoreLabel\n",
    "    #print(literatureType)\n",
    "    if literatureType == \"\":\n",
    "        for entry in classifierDict.values():\n",
    "            lowestScore = 1\n",
    "            lowestScoreLabel = \"\"\n",
    "            if entry['score'] < lowestScore and entry['label'].startswith('NON'):\n",
    "                lowestScore = entry['score']\n",
    "                lowestScoreLabel = entry['label'].replace('NON', '')\n",
    "        literatureType = lowestScoreLabel\n",
    "\n",
    "    \n",
    "    bibTex = \"@\"\n",
    "    if literatureType == \"book\":\n",
    "        zippedFieldsValues = zip(bookFieldsString, bookFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"book{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"proceedings\":\n",
    "        zippedFieldsValues = zip(proceedingsFieldsString, proceedingsFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"proceedings{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"inproceedings\":\n",
    "        zippedFieldsValues = zip(inproceedingsFieldsString, inproceedingsFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"inproceedings{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"incollection\":\n",
    "        zippedFieldsValues = zip(incollectionFieldsString, incollectionFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"incollection{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"article\":\n",
    "        zippedFieldsValues = zip(articleFieldsString, articleFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"article{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    else:\n",
    "        zippedFieldsValues = zip(phdthesisFieldsString, phdthesisFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"phdthesis{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    \n",
    "    #Idee: Mit Pos-Tagging herausfinden, wo Nomen etc. vorkommen und dann titel und Booktitel eingrenzen\n",
    "    bibTex += '}'\n",
    "\n",
    "    return bibTex \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1358ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@article{Rozovskaya_2017, \n",
      "author={Alla Rozovskaya and Dan Roth and Mark Sammons},\n",
      "title={Adapting to learner errors with minimal supervision},\n",
      "journal={Computational Linguistics},\n",
      "year={2017},\n",
      "volume={43},\n",
      "number={4},\n",
      "pages={},\n",
      "month={},\n",
      "note={},\n",
      "key={Rozovskaya_2017},\n",
      "doi={10.1162/COLI_a_00299},\n",
      "url={https://aclanthology.org/J17-4002},\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Beispiele\n",
    "\n",
    "text=\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "#text = \"M. A. Nielsen and I. L. Chuang, “Quantum Computation and Quantum Information,” in Handbook of Quantum Information Science, vol. 4, C. H. Bennett and D. P. DiVincenzo, Eds. Berlin, Germany: Springer, 2026, pp. 250–300. doi: 10.1007/springerreference-303198.\"\n",
    "#text = \"Nielsen, M. A.; Chuang, I. L. Quantum Computation and Quantum Information. In Handbook of Quantum Information Science; Bennett, C. H., DiVincenzo, D. P., Eds.; Quantum Science and Technology; Springer: Berlin, Germany, 2026; Vol. 4, pp 250–300. https://doi.org/10.1007/springerreference-303198.\"\n",
    "\n",
    "#BUG: startIndexReplace={-1} ist hier bei getYear! Deswegen doppelter String drin\n",
    "text = \"\"\"Alahmed, Y., Abadla, R., Badri, A. A., & Ameen, N. (2023). “How Does ChatGPT Work” Examining Functionality, To The Creative AI CHATGPT on X’s (Twitter) Platform. 2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS), 1–7. https://doi.org/10.1109/SNAMS60348.2023.10375450\"\"\"\n",
    "text = \"David Mertz, Regular Expression Puzzles and AI Coding Assistants: 24 puzzles solved by the author, with and without assistance from Copilot, ChatGPT and more , Manning, 2023.\"\n",
    "text = \"\"\"Mohammed Baziyad, Ibrahim Kamel, and Tamer Rabie. 2023. On the Linguistic Limitations of ChatGPT: An Experimental Case Study. In 2023 International Symposium on Networks, Computers and Communications (ISNCC), 1–6. DOI:https://doi.org/10.1109/ISNCC58260.2023.10323661\"\"\"\n",
    "#text = \"\"\"K. M. Caramancion, \"Harnessing the Power of ChatGPT to Decimate Mis/Disinformation: Using ChatGPT for Fake News Detection,\" 2023 IEEE World AI IoT Congress (AIIoT), Seattle, WA, USA, 2023, pp. 0042-0046, doi: 10.1109/AIIoT58121.2023.10174450.\"\"\"\n",
    "#text = \"\"\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence? In: J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\"\"\"\n",
    "#text = \"\"\"A. Einstein, B. Podolsky, and N. Rosen, “Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?,” Physical Review, vol. 47, no. 10, pp. 777–780, May 1935, doi: 10.1103/PhysRev.47.777.\"\"\"\n",
    "#text = \"\"\"Badaro, G., Saeed, M., & Papotti, P. (2023). Transformers for tabular data representation: a survey of models and applications. Transactions of the Association for Computational Linguistics, 11, pp. 227–249. URL: https://aclanthology.org/2023.tacl-1.14, doi:10.1162/tacl_a_00544\"\"\"\n",
    "#text = \"\"\"Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics 11, (2023), 1–17. URL: https://aclanthology.org/2023.tacl-1.1, doi:10.1162/tacl_a_00530\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. (2017). Joint prediction of word alignment with alignment types. Transactions of the Association for Computational Linguistics, 5, pp. 501–514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. 2017. \"Joint Prediction of Word Alignment with Alignment Types.\" Transactions of the Association for Computational Linguistics, 5: 501–514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076.\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. 2017. ‘Joint Prediction of Word Alignment with Alignment Types.\" Transactions of the Association forComputational Linguistics, 5: 501-514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076.\"\"\"\n",
    "#text = \"\"\"Devika K, Hariprasath .s.b, Haripriya B, Vigneshwar E, Premjith B, and Bharathi Raja Chakravarthi. From dataset to detection: a comprehensive approach to combating Malayalam fake news. In Bharathi Raja Chakravarthi, Ruba Priyadharshini, Anand Kumar Madasamy, Sajeetha Thavareesan, Elizabeth Sherly, Rajeswari Nadarajan, and Manikandan Ravikiran, editors, Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 16–23, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL: https://aclanthology.org/2024.dravidianlangtech-1.3.\"\"\"\n",
    "#text = \"\"\"R, Jairam, G, Jyothish, and B, Premjith. \"A few-shot multi-accented speech classification for Indian languages using transformers and LLM's fine-tuning approaches.\" Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages. Eds. Chakravarthi, Bharathi Raja, Priyadharshini, Ruba, Madasamy, Anand Kumar, Thavareesan, Sajeetha, Sherly, Elizabeth, Nadarajan, Rajeswari, and Ravikiran, Manikandan. St. Julian's, Malta: Association for Computational Linguistics, 2024. 1–9. URL: https://aclanthology.org/2024.dravidianlangtech-1.1\"\"\"\n",
    "text = \"\"\"Rozovskaya, Alla, Roth, Dan, and Sammons, Mark. \"Adapting to learner errors with minimal supervision.\" Computational Linguistics 43.4 (2017): 723–760. URL: https://aclanthology.org/J17-4002, doi:10.1162/COLI_a_00299\"\"\"\n",
    "#text = \"\"\"Eds. Alonso, Jose M., and Catala, Alejandro. Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019). 2019. URL: https://aclanthology.org/W19-8400\"\"\"\n",
    "print(create_bibtex(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8b8a9e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Referenzstring</th>\n",
       "      <th>Style</th>\n",
       "      <th>Literaturtyp</th>\n",
       "      <th>BibTeX</th>\n",
       "      <th>Modellergebnis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yuan Zhang, Regina Barzilay, and Tommi Jaakkol...</td>\n",
       "      <td>acm</td>\n",
       "      <td>article</td>\n",
       "      <td>@article{zhang-etal-2017-aspect,\\n    title = ...</td>\n",
       "      <td>@article{Zhang_2017, \\nauthor={Yuan Zhang and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ryan J. Gallagher, Kyle Reing, David Kale, and...</td>\n",
       "      <td>acm</td>\n",
       "      <td>article</td>\n",
       "      <td>@article{gallagher-etal-2017-anchored,\\n    ti...</td>\n",
       "      <td>@article{Gallagher_2017, \\nauthor={Ryan J. Gal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shafiq Joty, Francisco Guzmán, Lluís Màrquez, ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>article</td>\n",
       "      <td>@article{joty-etal-2017-discourse,\\n    title ...</td>\n",
       "      <td>@article{Joty_2017, \\nauthor={Shafiq Joty and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alla Rozovskaya, Dan Roth, and Mark Sammons. 2...</td>\n",
       "      <td>acm</td>\n",
       "      <td>article</td>\n",
       "      <td>@article{rozovskaya-etal-2017-adapting,\\n    t...</td>\n",
       "      <td>@article{Rozovskaya_2017, \\nauthor={Alla Rozov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ákos Kádár, Grzegorz Chrupała, and Afra Alisha...</td>\n",
       "      <td>acm</td>\n",
       "      <td>article</td>\n",
       "      <td>@article{kadar-etal-2017-representation,\\n    ...</td>\n",
       "      <td>@article{Kádár_2017, \\nauthor={Ákos Kádár and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Magda Ševčíková, Zdeněk Žabokrtský, Eleonora L...</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-international-resources,\\...</td>\n",
       "      <td>@phdthesis{Ševčíková_2019, \\nauthor={Magda Šev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Jose M. Alonso and Alejandro Catala (Eds.). 20...</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-interactive,\\n    title =...</td>\n",
       "      <td>@phdthesis{Alonso_2019, \\nauthor={Jose M. Alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Yoshinobu Kano, Claus Aranha, Michimasa Inaba,...</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-international-ai,\\n    ti...</td>\n",
       "      <td>@phdthesis{Kano_2019, \\nauthor={Yoshinobu Kano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Anusha Balakrishnan, Vera Demberg, Chandra Kha...</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-discourse-structure,\\n   ...</td>\n",
       "      <td>@phdthesis{Balakrishnan_2019, \\nauthor={Anusha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Alexandre Rademaker and Francis Tyers (Eds.). ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-universal,\\n    title = \"...</td>\n",
       "      <td>@phdthesis{Rademaker_2019, \\nauthor={Alexandre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Xinying Chen and Ramon Ferrer-i-Cancho (Eds.)....</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-quantitative,\\n    title ...</td>\n",
       "      <td>@phdthesis{Chen_2019, \\nauthor={Xinying Chen a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>Talia Tseriotou, Ryan Chan, Adam Tsakalidis, I...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{tseriotou-etal-2024-sig,\\n    t...</td>\n",
       "      <td>@inproceedings{Tseriotou_2024, \\nauthor={Talia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>Jairam R, Jyothish G, and Premjith B. 2024. A ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{r-etal-2024-shot,\\n    title = ...</td>\n",
       "      <td>@inproceedings{R_2024, \\nauthor={Jairam R and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Christeena Varghese, Sergey Koshelev, and Ivan...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{varghese-etal-2024-neural,\\n   ...</td>\n",
       "      <td>@phdthesis{Varghese_2024, \\nauthor={Christeena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>Devika K, Hariprasath .s.b, Haripriya B, Vigne...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{k-etal-2024-dataset,\\n    title...</td>\n",
       "      <td>@phdthesis{K_2024, \\nauthor={Hariprasath Devik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>Girma Bade, Olga Kolesnikova, Grigori Sidorov,...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{bade-etal-2024-social,\\n    tit...</td>\n",
       "      <td>@phdthesis{Bade_2024, \\nauthor={Girma Bade and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>Vigneshwar Lakshminarayanan and Emily Prud'hom...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{lakshminarayanan-prudhommeaux-2...</td>\n",
       "      <td>@phdthesis{Lakshminarayanan_2024, \\nauthor={Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Bill Worzel and Rick L. Riolo. 2003. Genetic P...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{worzel:2002:GPTP,\\n  author =   ...</td>\n",
       "      <td>@incollection{Worzel_2003, \\nauthor={Bill Worz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>Bill Worzel and Duncan MacLean. 2005. Content ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{worzel:2005:GPTP,\\n  author =   ...</td>\n",
       "      <td>@incollection{Worzel_2005, \\nauthor={Bill Worz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>W. P. Worzel, A. Almal, and C. D. MacLean. 200...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{Worzel:2006:GPTP,\\n  author =   ...</td>\n",
       "      <td>@incollection{Worzel_2006, \\nauthor={W. P. Wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>Gary I. Wu. 1999. Evolution of Infanticide and...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{wu:1999:EIYNAO,\\n  author =     ...</td>\n",
       "      <td>@incollection{Wu_1999, \\nauthor={Gary I. Wu},\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Taro Yabuki and Hitoshi Iba. 2004. Genetic pro...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{yabuki04genetic,\\n  author =    ...</td>\n",
       "      <td>@incollection{Yabuki_2004, \\nauthor={Taro Yabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Mei-Wan Yang. 1999. Space Configuration using ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{yang:1999:SCGA,\\n  author =     ...</td>\n",
       "      <td>@incollection{Yang_1999, \\nauthor={Mei-Wan Yan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>Kareen Bock, Anemone Widmer, Therese Neff, and...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Bock.2005,\\n author = {Bock, Kareen and ...</td>\n",
       "      <td>@book{Bock_2005, \\nauthor={Kareen Bock and Ane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>Thilde Boecker, Aileen Leib, Marika Mihm, and ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Boecker.1990,\\n author = {Boecker, Thild...</td>\n",
       "      <td>@book{Boecker_1990, \\nauthor={Thilde Boecker a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>Vincenz Böhmer, Nadine Kempkes, and Siegulf Ha...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Bohmer.2007,\\n author = {B{\\\"o}hmer, Vin...</td>\n",
       "      <td>@book{Böhmer_2007, \\nauthor={Vincenz Böhmer an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>Trautchen Bohner, Nikolaus-Jonny Sehr, and Arm...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Bohner.2009,\\n author = {Bohner, Trautch...</td>\n",
       "      <td>@book{Bohner_2009, \\nauthor={Trautchen Bohner ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>Ingetrud Bohnet, Heidegunde Specht, Steve März...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Bohnet.2012,\\n author = {Bohnet, Ingetru...</td>\n",
       "      <td>@book{Bohnet_2012, \\nauthor={Ingetrud Bohnet a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>Bärbel Bolduan, Heinzhermann Reinwald, Gretlie...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Bolduan.2021,\\n author = {Bolduan, B{\\\"a...</td>\n",
       "      <td>@book{Bolduan_2021, \\nauthor={Bärbel Bolduan a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Maëlyss Berns. 2019. Vermeidung von Interferen...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Berns.2019,\\n author = {Berns, Ma{\\...</td>\n",
       "      <td>@phdthesis{Berns_2019, \\nauthor={Maëlyss Berns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Mélys Berthot. 2017. Efficient storage and ana...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Berthot.2017,\\n author = {Berthot, ...</td>\n",
       "      <td>@incollection{Berthot_2017, \\nauthor={Mélys Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>Cléa Bertot. 2012. Harvesting and summarizing ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Bertot.2012,\\n author = {Bertot, Cl...</td>\n",
       "      <td>@incollection{Bertot_2012, \\nauthor={Cléa Bert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>Anaëlle Bettam. 2015. Cardiac motion and funct...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Bettam.2015,\\n author = {Bettam, An...</td>\n",
       "      <td>@incollection{Bettam_2015, \\nauthor={Anaëlle B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Aloïs Bette. 2019. Similarity Search Algorithm...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Bette.2019,\\n author = {Bette, Alo{...</td>\n",
       "      <td>@phdthesis{Bette_2019, \\nauthor={Aloïs Bette},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>Mélissandre Bidder. 2018. Real-time event dete...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Bidder.2018,\\n author = {Bidder, M{...</td>\n",
       "      <td>@phdthesis{Bidder_2018, \\nauthor={Mélissandre ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Referenzstring Style   Literaturtyp  \\\n",
       "1     Yuan Zhang, Regina Barzilay, and Tommi Jaakkol...   acm        article   \n",
       "2     Ryan J. Gallagher, Kyle Reing, David Kale, and...   acm        article   \n",
       "3     Shafiq Joty, Francisco Guzmán, Lluís Màrquez, ...   acm        article   \n",
       "4     Alla Rozovskaya, Dan Roth, and Mark Sammons. 2...   acm        article   \n",
       "5     Ákos Kádár, Grzegorz Chrupała, and Afra Alisha...   acm        article   \n",
       "300   Magda Ševčíková, Zdeněk Žabokrtský, Eleonora L...   acm    proceedings   \n",
       "301   Jose M. Alonso and Alejandro Catala (Eds.). 20...   acm    proceedings   \n",
       "302   Yoshinobu Kano, Claus Aranha, Michimasa Inaba,...   acm    proceedings   \n",
       "303   Anusha Balakrishnan, Vera Demberg, Chandra Kha...   acm    proceedings   \n",
       "304   Alexandre Rademaker and Francis Tyers (Eds.). ...   acm    proceedings   \n",
       "305   Xinying Chen and Ramon Ferrer-i-Cancho (Eds.)....   acm    proceedings   \n",
       "600   Talia Tseriotou, Ryan Chan, Adam Tsakalidis, I...   acm  inproceedings   \n",
       "601   Jairam R, Jyothish G, and Premjith B. 2024. A ...   acm  inproceedings   \n",
       "602   Christeena Varghese, Sergey Koshelev, and Ivan...   acm  inproceedings   \n",
       "603   Devika K, Hariprasath .s.b, Haripriya B, Vigne...   acm  inproceedings   \n",
       "604   Girma Bade, Olga Kolesnikova, Grigori Sidorov,...   acm  inproceedings   \n",
       "605   Vigneshwar Lakshminarayanan and Emily Prud'hom...   acm  inproceedings   \n",
       "900   Bill Worzel and Rick L. Riolo. 2003. Genetic P...   acm   incollection   \n",
       "901   Bill Worzel and Duncan MacLean. 2005. Content ...   acm   incollection   \n",
       "902   W. P. Worzel, A. Almal, and C. D. MacLean. 200...   acm   incollection   \n",
       "903   Gary I. Wu. 1999. Evolution of Infanticide and...   acm   incollection   \n",
       "904   Taro Yabuki and Hitoshi Iba. 2004. Genetic pro...   acm   incollection   \n",
       "905   Mei-Wan Yang. 1999. Space Configuration using ...   acm   incollection   \n",
       "1200  Kareen Bock, Anemone Widmer, Therese Neff, and...   acm           book   \n",
       "1201  Thilde Boecker, Aileen Leib, Marika Mihm, and ...   acm           book   \n",
       "1202  Vincenz Böhmer, Nadine Kempkes, and Siegulf Ha...   acm           book   \n",
       "1203  Trautchen Bohner, Nikolaus-Jonny Sehr, and Arm...   acm           book   \n",
       "1204  Ingetrud Bohnet, Heidegunde Specht, Steve März...   acm           book   \n",
       "1205  Bärbel Bolduan, Heinzhermann Reinwald, Gretlie...   acm           book   \n",
       "1500  Maëlyss Berns. 2019. Vermeidung von Interferen...   acm      phdthesis   \n",
       "1501  Mélys Berthot. 2017. Efficient storage and ana...   acm      phdthesis   \n",
       "1502  Cléa Bertot. 2012. Harvesting and summarizing ...   acm      phdthesis   \n",
       "1503  Anaëlle Bettam. 2015. Cardiac motion and funct...   acm      phdthesis   \n",
       "1504  Aloïs Bette. 2019. Similarity Search Algorithm...   acm      phdthesis   \n",
       "1505  Mélissandre Bidder. 2018. Real-time event dete...   acm      phdthesis   \n",
       "\n",
       "                                                 BibTeX  \\\n",
       "1     @article{zhang-etal-2017-aspect,\\n    title = ...   \n",
       "2     @article{gallagher-etal-2017-anchored,\\n    ti...   \n",
       "3     @article{joty-etal-2017-discourse,\\n    title ...   \n",
       "4     @article{rozovskaya-etal-2017-adapting,\\n    t...   \n",
       "5     @article{kadar-etal-2017-representation,\\n    ...   \n",
       "300   @proceedings{ws-2019-international-resources,\\...   \n",
       "301   @proceedings{ws-2019-interactive,\\n    title =...   \n",
       "302   @proceedings{ws-2019-international-ai,\\n    ti...   \n",
       "303   @proceedings{ws-2019-discourse-structure,\\n   ...   \n",
       "304   @proceedings{ws-2019-universal,\\n    title = \"...   \n",
       "305   @proceedings{ws-2019-quantitative,\\n    title ...   \n",
       "600   @inproceedings{tseriotou-etal-2024-sig,\\n    t...   \n",
       "601   @inproceedings{r-etal-2024-shot,\\n    title = ...   \n",
       "602   @inproceedings{varghese-etal-2024-neural,\\n   ...   \n",
       "603   @inproceedings{k-etal-2024-dataset,\\n    title...   \n",
       "604   @inproceedings{bade-etal-2024-social,\\n    tit...   \n",
       "605   @inproceedings{lakshminarayanan-prudhommeaux-2...   \n",
       "900   @InCollection{worzel:2002:GPTP,\\n  author =   ...   \n",
       "901   @InCollection{worzel:2005:GPTP,\\n  author =   ...   \n",
       "902   @InCollection{Worzel:2006:GPTP,\\n  author =   ...   \n",
       "903   @InCollection{wu:1999:EIYNAO,\\n  author =     ...   \n",
       "904   @InCollection{yabuki04genetic,\\n  author =    ...   \n",
       "905   @InCollection{yang:1999:SCGA,\\n  author =     ...   \n",
       "1200  @book{Bock.2005,\\n author = {Bock, Kareen and ...   \n",
       "1201  @book{Boecker.1990,\\n author = {Boecker, Thild...   \n",
       "1202  @book{Bohmer.2007,\\n author = {B{\\\"o}hmer, Vin...   \n",
       "1203  @book{Bohner.2009,\\n author = {Bohner, Trautch...   \n",
       "1204  @book{Bohnet.2012,\\n author = {Bohnet, Ingetru...   \n",
       "1205  @book{Bolduan.2021,\\n author = {Bolduan, B{\\\"a...   \n",
       "1500  @phdthesis{Berns.2019,\\n author = {Berns, Ma{\\...   \n",
       "1501  @phdthesis{Berthot.2017,\\n author = {Berthot, ...   \n",
       "1502  @phdthesis{Bertot.2012,\\n author = {Bertot, Cl...   \n",
       "1503  @phdthesis{Bettam.2015,\\n author = {Bettam, An...   \n",
       "1504  @phdthesis{Bette.2019,\\n author = {Bette, Alo{...   \n",
       "1505  @phdthesis{Bidder.2018,\\n author = {Bidder, M{...   \n",
       "\n",
       "                                         Modellergebnis  \n",
       "1     @article{Zhang_2017, \\nauthor={Yuan Zhang and ...  \n",
       "2     @article{Gallagher_2017, \\nauthor={Ryan J. Gal...  \n",
       "3     @article{Joty_2017, \\nauthor={Shafiq Joty and ...  \n",
       "4     @article{Rozovskaya_2017, \\nauthor={Alla Rozov...  \n",
       "5     @article{Kádár_2017, \\nauthor={Ákos Kádár and ...  \n",
       "300   @phdthesis{Ševčíková_2019, \\nauthor={Magda Šev...  \n",
       "301   @phdthesis{Alonso_2019, \\nauthor={Jose M. Alon...  \n",
       "302   @phdthesis{Kano_2019, \\nauthor={Yoshinobu Kano...  \n",
       "303   @phdthesis{Balakrishnan_2019, \\nauthor={Anusha...  \n",
       "304   @phdthesis{Rademaker_2019, \\nauthor={Alexandre...  \n",
       "305   @phdthesis{Chen_2019, \\nauthor={Xinying Chen a...  \n",
       "600   @inproceedings{Tseriotou_2024, \\nauthor={Talia...  \n",
       "601   @inproceedings{R_2024, \\nauthor={Jairam R and ...  \n",
       "602   @phdthesis{Varghese_2024, \\nauthor={Christeena...  \n",
       "603   @phdthesis{K_2024, \\nauthor={Hariprasath Devik...  \n",
       "604   @phdthesis{Bade_2024, \\nauthor={Girma Bade and...  \n",
       "605   @phdthesis{Lakshminarayanan_2024, \\nauthor={Vi...  \n",
       "900   @incollection{Worzel_2003, \\nauthor={Bill Worz...  \n",
       "901   @incollection{Worzel_2005, \\nauthor={Bill Worz...  \n",
       "902   @incollection{Worzel_2006, \\nauthor={W. P. Wor...  \n",
       "903   @incollection{Wu_1999, \\nauthor={Gary I. Wu},\\...  \n",
       "904   @incollection{Yabuki_2004, \\nauthor={Taro Yabu...  \n",
       "905   @incollection{Yang_1999, \\nauthor={Mei-Wan Yan...  \n",
       "1200  @book{Bock_2005, \\nauthor={Kareen Bock and Ane...  \n",
       "1201  @book{Boecker_1990, \\nauthor={Thilde Boecker a...  \n",
       "1202  @book{Böhmer_2007, \\nauthor={Vincenz Böhmer an...  \n",
       "1203  @book{Bohner_2009, \\nauthor={Trautchen Bohner ...  \n",
       "1204  @book{Bohnet_2012, \\nauthor={Ingetrud Bohnet a...  \n",
       "1205  @book{Bolduan_2021, \\nauthor={Bärbel Bolduan a...  \n",
       "1500  @phdthesis{Berns_2019, \\nauthor={Maëlyss Berns...  \n",
       "1501  @incollection{Berthot_2017, \\nauthor={Mélys Be...  \n",
       "1502  @incollection{Bertot_2012, \\nauthor={Cléa Bert...  \n",
       "1503  @incollection{Bettam_2015, \\nauthor={Anaëlle B...  \n",
       "1504  @phdthesis{Bette_2019, \\nauthor={Aloïs Bette},...  \n",
       "1505  @phdthesis{Bidder_2018, \\nauthor={Mélissandre ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Ausführung der Codezelle hat 255.6526 Sekunden gedauert.\n"
     ]
    }
   ],
   "source": [
    "# Einlesen von Testdaten für eine vorläufige Evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Startzeit erfassen\n",
    "start_time = time.time()\n",
    "\n",
    "# Unterdrücken aller Warnungen\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Setze das Logging-Level für Transformers auf ERROR\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Pfad zur CSV-Datei\n",
    "csv_file_path = 'Trainingsdaten/Testdaten/test_acm.csv'\n",
    "\n",
    "# Einlesen der CSV-Datei in einen Pandas DataFrame mit dem Delimiter |\n",
    "df = pd.read_csv(csv_file_path, delimiter='|')\n",
    "df = df.iloc[list(range(1, 6)) + list(range(300, 306)) + list(range(600, 606)) + list(range(900, 906)) + list(range(1200, 1206)) + list(range(1500, 1506))]\n",
    "\n",
    "# Neue Spalte für Ergebnisse oder Fehler erstellen\n",
    "df['Modellergebnis'] = None\n",
    "\n",
    "# Fehlerbehandlung und Anwenden der Funktion\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        result = create_bibtex(row['Referenzstring'])\n",
    "        df.at[index, 'Modellergebnis'] = result\n",
    "    except Exception as e:\n",
    "        df.at[index, 'Modellergebnis'] = f\"Fehler: {e}\"\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.to_csv('Trainingsdaten/Testdaten/test_acm_result3.csv', sep='|', index=False)\n",
    "#print(create_bibtex(text))\n",
    "\n",
    "# Endzeit erfassen\n",
    "end_time = time.time()\n",
    "\n",
    "# Ausführungszeit berechnen\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Die Ausführung der Codezelle hat {execution_time:.4f} Sekunden gedauert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1cf33fa-012f-4140-982b-1b3b61591cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: acm, phdthesis\n",
      "  Style Literaturtyp        Zeit\n",
      "0   acm    phdthesis  636.630408\n",
      "Start: acm, article\n",
      "  Style Literaturtyp        Zeit\n",
      "0   acm      article  663.219442\n",
      "Start: acm, book\n",
      "  Style Literaturtyp        Zeit\n",
      "0   acm         book  617.174743\n",
      "Start: acm, inproceedings\n",
      "  Style   Literaturtyp        Zeit\n",
      "0   acm  inproceedings  837.186132\n",
      "Start: acm, proceedings\n",
      "  Style Literaturtyp        Zeit\n",
      "0   acm  proceedings  669.577584\n",
      "Start: acm, incollection\n",
      "  Style  Literaturtyp        Zeit\n",
      "0   acm  incollection  654.223004\n",
      "Start: apa, phdthesis\n",
      "  Style Literaturtyp        Zeit\n",
      "0   apa    phdthesis  666.490255\n",
      "Start: apa, article\n",
      "  Style Literaturtyp        Zeit\n",
      "0   apa      article  703.541977\n",
      "Start: apa, book\n",
      "  Style Literaturtyp        Zeit\n",
      "0   apa         book  618.058641\n",
      "Start: apa, inproceedings\n",
      "  Style   Literaturtyp        Zeit\n",
      "0   apa  inproceedings  905.227921\n",
      "Start: apa, proceedings\n",
      "  Style Literaturtyp        Zeit\n",
      "0   apa  proceedings  736.632788\n",
      "Start: apa, incollection\n",
      "  Style  Literaturtyp        Zeit\n",
      "0   apa  incollection  709.781408\n",
      "Start: harvard, phdthesis\n",
      "     Style Literaturtyp        Zeit\n",
      "0  harvard    phdthesis  688.676228\n",
      "Start: harvard, article\n",
      "     Style Literaturtyp       Zeit\n",
      "0  harvard      article  729.11797\n",
      "Start: harvard, book\n",
      "     Style Literaturtyp        Zeit\n",
      "0  harvard         book  634.762473\n",
      "Start: harvard, inproceedings\n",
      "     Style   Literaturtyp        Zeit\n",
      "0  harvard  inproceedings  917.323296\n",
      "Start: harvard, proceedings\n",
      "     Style Literaturtyp        Zeit\n",
      "0  harvard  proceedings  718.886927\n",
      "Start: harvard, incollection\n",
      "     Style  Literaturtyp        Zeit\n",
      "0  harvard  incollection  706.725809\n",
      "Start: ieee, phdthesis\n",
      "  Style Literaturtyp        Zeit\n",
      "0  ieee    phdthesis  693.493427\n",
      "Start: ieee, article\n",
      "  Style Literaturtyp        Zeit\n",
      "0  ieee      article  708.703761\n",
      "Start: ieee, book\n",
      "  Style Literaturtyp        Zeit\n",
      "0  ieee         book  634.549615\n",
      "Start: ieee, inproceedings\n",
      "  Style   Literaturtyp        Zeit\n",
      "0  ieee  inproceedings  897.507677\n",
      "Start: ieee, proceedings\n",
      "  Style Literaturtyp       Zeit\n",
      "0  ieee  proceedings  721.46517\n",
      "Start: ieee, incollection\n",
      "  Style  Literaturtyp        Zeit\n",
      "0  ieee  incollection  728.727907\n",
      "Start: mla, phdthesis\n",
      "  Style Literaturtyp        Zeit\n",
      "0   mla    phdthesis  682.779682\n",
      "Start: mla, article\n",
      "  Style Literaturtyp        Zeit\n",
      "0   mla      article  712.042816\n",
      "Start: mla, book\n",
      "  Style Literaturtyp        Zeit\n",
      "0   mla         book  630.527875\n",
      "Start: mla, inproceedings\n",
      "  Style   Literaturtyp        Zeit\n",
      "0   mla  inproceedings  902.098119\n",
      "Start: mla, proceedings\n",
      "  Style Literaturtyp        Zeit\n",
      "0   mla  proceedings  732.207707\n",
      "Start: mla, incollection\n",
      "  Style  Literaturtyp        Zeit\n",
      "0   mla  incollection  689.884012\n",
      "Start: plain, phdthesis\n",
      "   Style Literaturtyp        Zeit\n",
      "0  plain    phdthesis  669.640402\n",
      "Start: plain, article\n",
      "   Style Literaturtyp        Zeit\n",
      "0  plain      article  721.307612\n",
      "Start: plain, book\n",
      "   Style Literaturtyp        Zeit\n",
      "0  plain         book  669.316178\n",
      "Start: plain, inproceedings\n",
      "   Style   Literaturtyp        Zeit\n",
      "0  plain  inproceedings  926.346727\n",
      "Start: plain, proceedings\n",
      "   Style Literaturtyp        Zeit\n",
      "0  plain  proceedings  734.014448\n",
      "Start: plain, incollection\n",
      "   Style  Literaturtyp        Zeit\n",
      "0  plain  incollection  721.439469\n"
     ]
    }
   ],
   "source": [
    "# Einlesen aller Testdaten und Prozessierung von 36 x 100 Beispielen\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Unterdrücken aller Warnungen\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Setze das Logging-Level für Transformers auf ERROR\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Einlesen der CSV-Datei in einen Pandas DataFrame mit dem Delimiter |\n",
    "df_acm = pd.read_csv('Trainingsdaten/Testdaten/test_acm.csv', delimiter='|')\n",
    "df_apa = pd.read_csv('Trainingsdaten/Testdaten/test_apa.csv', delimiter='|')\n",
    "df_harv = pd.read_csv('Trainingsdaten/Testdaten/test_harv.csv', delimiter='|')\n",
    "df_ieee = pd.read_csv('Trainingsdaten/Testdaten/test_ieee.csv', delimiter='|')\n",
    "df_mla = pd.read_csv('Trainingsdaten/Testdaten/test_mla.csv', delimiter='|')\n",
    "df_plain = pd.read_csv('Trainingsdaten/Testdaten/test_plain.csv', delimiter='|')\n",
    "df = pd.concat([df_acm, df_apa, df_harv, df_ieee, df_mla, df_plain])\n",
    "\n",
    "styles = ['acm', 'apa', 'harvard', 'ieee', 'mla', 'plain']\n",
    "types = ['phdthesis', 'article', 'book', 'inproceedings', 'proceedings', 'incollection']\n",
    "df_result = pd.DataFrame()\n",
    "df_time = pd.DataFrame()\n",
    "\n",
    "# Durchlaufen der Stile und Literaturtypen und Filterung der ersten 100 Beispiele pro Attributkombination\n",
    "# Konvertierung von je 100 Beispielen plus Zeitmessung\n",
    "for style in styles:\n",
    "    for type in types:\n",
    "        print(\"Start: \" + style + \", \" + type)\n",
    "        df_temp = df[(df[\"Style\"]==style) & (df[\"Literaturtyp\"]==type)]\n",
    "        df_temp = df_temp.head(100)\n",
    "\n",
    "        # Neue Spalte für Ergebnisse oder Fehler erstellen\n",
    "        df_temp['Modellergebnis'] = None\n",
    "\n",
    "        # Startzeit erfassen\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Fehlerbehandlung und Anwenden des Modells\n",
    "        for index, row in df_temp.iterrows():\n",
    "            try:\n",
    "                result = create_bibtex(row['Referenzstring'])\n",
    "                df_temp.at[index, 'Modellergebnis'] = result\n",
    "            except Exception as e:\n",
    "                df_temp.at[index, 'Modellergebnis'] = f\"Fehler: {e}\"\n",
    "\n",
    "        # Endzeit erfassen\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Ausführungszeit berechnen (in Sekunden)\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "        # Temporäres DataFrame an das Haupt-DataFrame anhängen\n",
    "        df_result = pd.concat([df_result, df_temp])\n",
    "        #display(df_temp)\n",
    "\n",
    "        df_time_temp = pd.DataFrame({'Style': [style], 'Literaturtyp': [type], 'Zeit': [execution_time]})\n",
    "        print(df_time_temp)\n",
    "        df_time = pd.concat([df_time, df_time_temp])\n",
    "\n",
    "# Export der Ergebnisse\n",
    "df_result.to_csv('Trainingsdaten/Testdaten/test_result.csv', sep='|', index=False)\n",
    "df_time.to_csv('Trainingsdaten/Testdaten/test_time.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651ff6f-247d-45bc-b111-ce270d66be7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
