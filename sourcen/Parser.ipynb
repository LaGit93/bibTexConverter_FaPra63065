{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad55d2b1",
   "metadata": {},
   "source": [
    "@book{advanced_physics, author = {Stephen Hawking and Brian Greene and Maria Trump and Susan Soy}, title = {Advanced Concepts in Theoretical Physics}, publisher = {Cambridge University Press}, year = {2025}, edition = {3}, volume = {1}, series = {Advanced Studies in Physics}, address = {Cambridge, UK}, month = {May}, note = {A seminal work in the field of theoretical physics}, isbn = {978-0-521-76948-0}, doi = {10.1017/example.book.2025}, url = {https://www.cambridge.org/advanced_physics}, annote = {Widely cited in the physics community}, abstract = {This book explores cutting-edge theories and concepts in theoretical physics...}, keywords = {Theoretical Physics, Quantum Mechanics, String Theory}, language = {English}, price = {75.00}, size = {600 pages}, lccn = {2020934576}, mrnumber = {MR3070071} }\n",
    "\n",
    "@article{quantum_entanglement,\n",
    "  author        = {Albert Einstein and Boris Podolsky and Nathan Rosen},\n",
    "  title         = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},\n",
    "  journal       = {Physical Review},\n",
    "  year          = {1935},\n",
    "  volume        = {47},\n",
    "  number        = {10},\n",
    "  pages         = {777-780},\n",
    "  month         = {May},\n",
    "  note          = {EPR Paradox paper, fundamental for quantum mechanics},\n",
    "  doi           = {10.1103/PhysRev.47.777},\n",
    "  url           = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},\n",
    "  abstract      = {In this paper, the authors discuss the EPR paradox and challenge the completeness of quantum mechanics...},\n",
    "  keywords      = {Quantum Mechanics, EPR Paradox, Physical Reality},\n",
    "  language      = {English},\n",
    "  publisher     = {American Physical Society}\n",
    "}\n",
    "\n",
    "@inproceedings{deep_learning,\n",
    "  author        = {Geoffrey Hinton and Yoshua Bengio and Yann LeCun},\n",
    "  title         = {Deep Learning for Artificial Intelligence},\n",
    "  booktitle     = {Proceedings of the IEEE International Conference on Neural Networks},\n",
    "  year          = {2021},\n",
    "  editor        = {Jane Smith and John Doe},\n",
    "  volume        = {1},\n",
    "  number = {5},\n",
    "  series        = {Advances in Neural Information Processing},\n",
    "  pages         = {100-120},\n",
    "  address       = {Montreal, Canada},\n",
    "  month         = {December},\n",
    "  organization  = {IEEE},\n",
    "  publisher     = {IEEE Press},\n",
    "  note          = {Keynote paper on recent advancements in deep learning},\n",
    "  isbn          = {978-1-5386-4637-1},\n",
    "  doi           = {10.1109/ICNN.2021.9483948},\n",
    "  url           = {https://ieeexplore.ieee.org/document/9483948},\n",
    "  annote        = {A seminal work on how deep learning transforms AI},\n",
    "  abstract      = {This paper explores cutting-edge deep learning techniques and their impact on the development of artificial intelligence...},\n",
    "  keywords      = {Deep Learning, Artificial Intelligence, Neural Networks},\n",
    "  language      = {English}\n",
    "}\n",
    "\n",
    "\n",
    "@incollection{quantum_computation,\n",
    "  author        = {Michael A. Nielsen and Isaac L. Chuang},\n",
    "  title         = {Quantum Computation and Quantum Information},\n",
    "  booktitle     = {Handbook of Quantum Information Science},\n",
    "  publisher     = {Springer},\n",
    "  year          = {2026},\n",
    "  editor        = {Charles H. Bennett and David P. DiVincenzo},\n",
    "  volume        = {4},\n",
    "  series        = {Quantum Science and Technology},\n",
    "  chapter       = {10},\n",
    "  pages         = {250-300},\n",
    "  address       = {Berlin, Germany},\n",
    "  month         = {October},\n",
    "  note          = {A comprehensive overview of the fundamentals of quantum computation},\n",
    "  isbn          = {978-3-540-88702-7},\n",
    "  doi           = {10.1007/springerreference_303198},\n",
    "  url           = {https://www.springer.com/gp/book/9783540887027},\n",
    "  annote        = {Essential reading for researchers entering the field of quantum information},\n",
    "  abstract      = {This chapter delves into the principles of quantum computing, offering an accessible yet thorough introduction...},\n",
    "  keywords      = {Quantum Computing, Quantum Information, Computational Models},\n",
    "  language      = {English},\n",
    "  price         = {45.00},\n",
    "  size          = {50 pages}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8acde04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "torch.cuda.is_available()\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "\n",
    "def getIndexOfSubstring(text, regEx = [], reverse = False):\n",
    "    min_index = float('inf')\n",
    "    end_index = 0\n",
    "    matches = []\n",
    "    substring = \"\"\n",
    "    for regExElement in regEx:\n",
    "        matches = list(re.finditer(regExElement, text))\n",
    "        if matches:\n",
    "            if reverse:\n",
    "                match = matches[-1]\n",
    "            else:\n",
    "                match = matches[0]\n",
    "            if match.start() < min_index:\n",
    "                min_index = match.start()\n",
    "                end_index = match.end()\n",
    "                substring = text[match.start():match.end()]\n",
    "    if matches:\n",
    "        return match.start(), match.end(), substring   \n",
    "    return -1, -1, substring\n",
    "\n",
    "def is_SurenameFirst(names):\n",
    "    splitedNames = names.split(\" \")\n",
    "    #regex wie w+ erkennt bspw. KEIN è \n",
    "    if splitedNames[0].endswith(\".\") or not is_punctuation(splitedNames[0][-1], []):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def is_NameShortened(df_PER):\n",
    "    for index in df_PER.index.values.tolist():\n",
    "        if \".\" == text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_punctuation(text, replaceCharacter = []):\n",
    "    allowed_chars = string.punctuation + ' '\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return all(char in allowed_chars for char in text)\n",
    "\n",
    "def is_Editor(editorRegEx, textBetweenNames, index, text):\n",
    "    if re.search(editorRegEx, textBetweenNames):\n",
    "        x = re.search(editorRegEx, text)\n",
    "        #print(f'x: {x.start()}')\n",
    "        if is_punctuation(text[index:x.start()], [\"&\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def processNames(authors):\n",
    "    finalAuthors = \"\"\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "    andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "    surenameFirst = is_SurenameFirst(authors.strip())\n",
    "    print(f'function processNames, surenameFirst: {surenameFirst}')\n",
    "    if surenameFirst:\n",
    "        print(\"Fall surenameFirst\".format(authors))\n",
    "        #hier völlig egal, ob er einzelne Initialen in ein eigenes Word gesteckt hat, obwohl es noch Nachnamen gib\n",
    "        authors = authors.replace(andInAuthors, \" and \")\n",
    "        #print(f'authors: {authors}')\n",
    "        finalAuthors = authors.replace(\", \", \" and \")\n",
    "    elif andInAuthors != \"\":\n",
    "        if \"., \" in authors:\n",
    "            print(\"Fall ., {0}\".format(authors))\n",
    "            search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "            andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "            authors = authors.replace(andInAuthors, \"., \")\n",
    "            authors = authors.split(\"., \")\n",
    "            authors = [name + \".\" for name in authors]\n",
    "            authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "            for author in authors[:-1]:\n",
    "                buffer = author.split(\", \")\n",
    "                finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "            buffer = authors[-1].split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "        elif \", \" in authors:\n",
    "            print(\"Fall , {0}\".format(authors))\n",
    "            search_terms = [\", and \", \", & \", \" and \", \" & \"]\n",
    "            andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "            authors = authors.replace(andInAuthors, \", \")\n",
    "            authors = authors.split(\", \")\n",
    "            for i in range(0, len(authors) - 3, 2):\n",
    "                finalAuthors = finalAuthors + authors[i+1] + \" \" + authors[i] + \" and \"\n",
    "            finalAuthors = finalAuthors + authors[len(authors) - 1] + \" \" + authors[len(authors) - 2]\n",
    "    else:\n",
    "        print(\"Fall else {0}\".format(authors))\n",
    "        authors = authors.split(\", \")\n",
    "        finalAuthros = authors[1] + authors[0]\n",
    "    return finalAuthors\n",
    "\n",
    "def getAuthors(df_PER, text):\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    authorsDetected = False\n",
    "    setChainStart = True\n",
    "    startIndexAuthors = -1\n",
    "    endIndexAuthors = -1\n",
    "    chainStartIndex = -1\n",
    "\n",
    "\n",
    "    for index in index_df_PER_List:\n",
    "        #beachte: Hiermit lese ich immer schon vor!\n",
    "        if index < len(index_df_PER_List) - 1:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "        else:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "        onlyPunctuation = is_punctuation(textBetweenNames, [\"&\"])\n",
    "        firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "        onlyAnd = textBetweenNames == andTyp\n",
    "        if setChainStart: \n",
    "            chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "            setChainStart = False\n",
    "        if not onlyPunctuation and not onlyAnd:\n",
    "            setChainStart = True\n",
    "            startIndexAuthors = chainStartIndex\n",
    "            endIndexAuthors = df_PER[\"end\"].iloc[index]\n",
    "            break\n",
    "    return startIndexAuthors,endIndexAuthors\n",
    "\n",
    "def getEditors(df_PER, text):\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    editorRegEx = \" (\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\"\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    setChainStart = True\n",
    "    isEditor = False\n",
    "    startIndexEditors = -1\n",
    "    endIndexEditors = -1\n",
    "    chainStartIndex = -1\n",
    "\n",
    "\n",
    "    for index in index_df_PER_List:\n",
    "        if index < len(index_df_PER_List) - 1:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "        else:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "        onlyPunctuation = is_punctuation(textBetweenNames, [\"&\"])\n",
    "        firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "        onlyAnd = textBetweenNames == andTyp\n",
    "        #wenn true, dann beginnt eine neue Autorenkette\n",
    "        if setChainStart: \n",
    "            chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "            #Solange das auf False, sollen der Substring erweitert werden, also start bleibt konstant\n",
    "            setChainStart = False\n",
    "        #Dann ist die Autorenkette zu Ende\n",
    "        if not onlyPunctuation and not onlyAnd:\n",
    "            setChainStart = True\n",
    "            #Es können auch nur Editoren und keine Autoren vorkommen\n",
    "            isEditor = is_Editor(editorRegEx, textBetweenNames, df_PER[\"end\"].iloc[index], text)\n",
    "            if isEditor:\n",
    "                startIndexEditors = chainStartIndex\n",
    "                endIndexEditors = df_PER[\"end\"].iloc[index]\n",
    "                break\n",
    "    #print(f'getAuthorsAndEditors: return: {[startIndexAuthors,endIndexAuthors],[startIndexEditors, endIndexEditors]}')\n",
    "    return startIndexEditors, endIndexEditors\n",
    "\n",
    "def replaceSubstring (startIndex, endIndex, text, substituteString, replacedPunctuation = [\"&\", \"(\", \")\"]):\n",
    "    if endIndex > 0:\n",
    "        startIndexReplace = -1\n",
    "        endIndexReplace = -1\n",
    "        changedText = text\n",
    "        if startIndex > 0:\n",
    "            for i in range(startIndex - 1, 0, -1):\n",
    "                if is_punctuation(text[i], replacedPunctuation):\n",
    "                    startIndexReplace = i + 1\n",
    "                    break\n",
    "        else:\n",
    "            startIndexReplace = 0\n",
    "        print(f'startIndexReplace : {startIndexReplace}')\n",
    "        if endIndex < len(text):\n",
    "            for i in range(endIndex, len(text), 1):\n",
    "                if is_punctuation(text[i], replacedPunctuation):\n",
    "                    endIndexReplace = i\n",
    "                    break\n",
    "        else:\n",
    "            endIndexReplace = len(text)\n",
    "        print(f'endIndexReplace : {endIndexReplace}')\n",
    "        changedText = text[0:startIndexReplace] + substituteString + text[endIndexReplace:len(text)]\n",
    "        print(f'text after replaceSubstring : {text[startIndexReplace:endIndexReplace]}')\n",
    "        return changedText, text[startIndexReplace:endIndexReplace]\n",
    "    return text, \"\"\n",
    "\n",
    "\n",
    "def getPersonTags(text):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    return df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "\n",
    "def getORGTag(text, score):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"ORG\") & (df_outputs[\"score\"] >= score)].reset_index(drop=True).tail(1)\n",
    "    return df_outputs\n",
    "\n",
    "#search_terms = [\", et al.\", \" et al.\"]\n",
    "#firstStartIndex, firstEndIndex, etAl = find_First_Term(text, search_terms)\n",
    "#if firstStartIndex > -1:\n",
    "    #text = replaceSubstring(firstStartIndex, firstEndIndex, text, \", \")\n",
    "\n",
    "def create_bibtex(text):\n",
    "    df_PER = getPersonTags(text)\n",
    "\n",
    "    doiUrlRegEx = \"https:\\/\\/doi\\.org(\\/[^\\s]*)?$\"\n",
    "    doiUrlRegEx2 = \"(DOI|doi):(https:\\/\\/doi\\.org)?([^\\s]*)+$\"\n",
    "    editorRegEx = \"(\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\"\n",
    "    year1 = \"(\\(\\d{4}\\)|\\. \\d{4}\\.)\"\n",
    "    year2 = \"(\\.|,) \\d{4}(\\.|,)\"\n",
    "\n",
    "\n",
    "    finalAuthors = \"\"\n",
    "    finalEditors = \"\"\n",
    "    startIndexAuthors,endIndexAuthors = getAuthors(df_PER, text)\n",
    "    if startIndexAuthors > -1:\n",
    "        text, authors = replaceSubstring(startIndexAuthors, endIndexAuthors, text, \"#AUTHOR#\")\n",
    "        print(f'text after replace authors : {text}')\n",
    "        finalAuthors = processNames(authors)\n",
    "    else:\n",
    "        startIndexAuthors, endIndexAuthors = 0, 0\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    startIndexEditors, endIndexEditors = getEditors(df_PER, text)\n",
    "    if startIndexEditors > -1:\n",
    "        text, editors = replaceSubstring(startIndexEditors, endIndexEditors, text, \"#EDITOR#\")\n",
    "        print(f'text after replace editors : {text}')\n",
    "        #es soll erst ab Editors gesucht werden, daher text[endIndexEditors:]. Sonst Verwechslungsgefahr\n",
    "        print(f'text[endIndexEditors:] : {text[endIndexEditors:]}')\n",
    "        endIndexEditors = getIndexOfSubstring(text, [\"#EDITOR#\"])[1]\n",
    "        startIndexEditorMarker, endIndexEditorMarker, finalEditors = getIndexOfSubstring(text[endIndexEditors:], [editorRegEx])\n",
    "        print(f'startIndexEditorMarker : {startIndexEditorMarker}')\n",
    "        print(f'endIndexEditorMarker : {endIndexEditorMarker}')\n",
    "        startIndexEditorMarker = startIndexEditorMarker + endIndexEditors\n",
    "        endIndexEditorMarker = endIndexEditorMarker + endIndexEditors\n",
    "        print(f'startIndexEditorMarker : {startIndexEditorMarker}')\n",
    "        print(f'endIndexEditorMarker : {endIndexEditorMarker} \\r\\n')\n",
    "        print(f'editors: {editors}')\n",
    "        finalEditors = processNames(editors)\n",
    "        print(f'finalEditors : {finalEditors} \\r\\n')\n",
    "        text, replacedEditorMarker = replaceSubstring(startIndexEditorMarker, endIndexEditorMarker, text, \"\")\n",
    "        print(f'text after replace EditorMarker : {text}')\n",
    "\n",
    "    else:\n",
    "        startIndexEditors, endIndexEditors = 0, 0\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    startIndex, endIndex, finalDoi = getIndexOfSubstring(text, [doiUrlRegEx, doiUrlRegEx2], True)\n",
    "    text, finalDoi = replaceSubstring(startIndex, endIndex, text, \"#DOI#\")\n",
    "    #print(f'text after replace DOI: {text}')\n",
    "\n",
    "    urlRegEx = \"https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?::\\d+)?(?:/[^\\s]*)?\"\n",
    "    startIndex, endIndex, finalURL = getIndexOfSubstring(text, [urlRegEx], True)\n",
    "    text, finalURL = replaceSubstring(startIndex, endIndex, text, \"#URL#\")\n",
    "    #print(f'text after replace DOI: {text}')\n",
    "\n",
    "    startIndex, endIndex, finalYear = getIndexOfSubstring(text, [year1])\n",
    "    if startIndex < 0:\n",
    "        startIndex, endIndex, finalYear = getIndexOfSubstring(text, [year2], True)\n",
    "    text, finalYear = replaceSubstring(startIndex, endIndex, text, \"#YEAR#\")\n",
    "    finalYear = re.search(r'\\d+', finalYear).group(0) if re.search(r'\\d+', finalYear) else \"\"\n",
    "\n",
    "    pageFinder = \"(?:pp\\.? )?\\d+(-|–)\\d+\"\n",
    "\n",
    "    startIndex, endIndex, finalPage = getIndexOfSubstring(text, [pageFinder], True)\n",
    "    text, finalPage = replaceSubstring(startIndex, endIndex, text, \"#PAGE#\")\n",
    "    finalPage = re.search(r'\\d+(-|–)\\d+', finalPage).group(0) if re.search(r'\\d+', finalPage) else \"\"\n",
    "\n",
    "    #print(f'text after replace Page: {text}')\n",
    "\n",
    "    number1 = \" no\\. \\d+\"\n",
    "    number2 = \" Issue \\d+\"\n",
    "    number3 = \"\\d+\"\n",
    "\n",
    "    #Volume, Seite, Number stehen IMMEr nach dem Titel. Also diese von Hinten suchen\n",
    "    startIndex, endIndex, finalNumber = getIndexOfSubstring(text, [number1, number2, number3], True)\n",
    "    text, finalNumber = replaceSubstring(startIndex, endIndex, text, \"#NUM#\")\n",
    "    finalNumber = re.search(r'\\d+', finalNumber).group(0) if re.search(r'\\d+', finalNumber) else \"\"\n",
    "\n",
    "    #print(f'text after replace Number: {text}')\n",
    "\n",
    "    volume1 = \"Vol\\. \\d+\"\n",
    "    volume2 = \"vol\\. \\d+\" \n",
    "    volume3 = \"\\d+\"\n",
    "    edition1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "    edition2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "\n",
    "    startIndex, endIndex, finalVolume = getIndexOfSubstring(text, [volume1, volume2, volume3], True)\n",
    "    text, finalVolume = replaceSubstring(startIndex, endIndex, text, \"#VOL#\")\n",
    "    finalVolume = re.search(r'\\d+', finalVolume).group(0) if re.search(r'\\d+', finalVolume) else \"\"\n",
    "\n",
    "    #print(f'text after replace Volume: {text}')\n",
    "\n",
    "    startIndex, endIndex, finalEdition = getIndexOfSubstring(text, [edition1, edition2], True)\n",
    "    text, finalEdition = replaceSubstring(startIndex, endIndex, text, \"#ED#\")\n",
    "    finalEdition = re.search(r'\\d+', finalEdition).group(0) if re.search(r'\\d+', finalEdition) else \"\"\n",
    "    \n",
    "    #Stadt und Land kommen immer nach dem Publisher\n",
    "    finalTown = \"\"\n",
    "    FinalCountry = \"\"\n",
    "\n",
    "    #Nun die Delimiter der felder ermitteln. Ist ein Bereich dann nur Org, dann muss es publisher sein\n",
    "    #print(f'text after replace Edition: {text}')\n",
    "    text = text.replace('(', '.')\n",
    "    text = text.replace(')', '.')\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    print(f'text after replace : {text}')\n",
    "    textList = [element.strip() for element in text.split('#')]\n",
    "    textList = [element for element in textList if element.strip()]\n",
    "    print(textList)\n",
    "    finalPublisher = \"\"\n",
    "    possiblePublisher = textList[-1].strip()\n",
    "    print(f'possiblePublisher: {possiblePublisher}')\n",
    "    df_Org = getORGTag(possiblePublisher, 0.8)\n",
    "    punctuation = string.punctuation.replace(' ', '')  # Entferne das Leerzeichen, falls es enthalten ist\n",
    "    pattern = f\"[{re.escape(punctuation)}]\"\n",
    "    possiblePublisher = re.sub(pattern, '', possiblePublisher)\n",
    "    publisherDetected = False\n",
    "    print(f'possiblePublisher: {possiblePublisher}')\n",
    "    if not df_Org.empty:\n",
    "        startIndex, endIndex = df_PER[\"start\"].iloc[0], df_PER[\"end\"].iloc[0]\n",
    "        print(endIndex - startIndex == len(possiblePublisher))\n",
    "        if endIndex - startIndex == len(possiblePublisher):\n",
    "            finalPublisher = possiblePublisher[startIndex:endIndex]\n",
    "            #double Check\n",
    "            #nlp = spacy.load(\"en_core_web_sm\")\n",
    "            #doc = nlp(text)\n",
    "            #for ent in doc.ents:\n",
    "                #if ent.label_ == \"ORG\" or ent.label_ == \"MISC\":\n",
    "                    #print(ent.end_char - ent.start_char == len(possiblePublisher))\n",
    "                    #if ent.end_char - ent.start_char != len(possiblePublisher):\n",
    "                        #startIndex = ent.start_char\n",
    "                        #endIndex = ent.end_char                         \n",
    "    if finalPublisher != \"\":\n",
    "        textList = textList[:-1]\n",
    "\n",
    "    print(f'text after replace Publisher : {text}')\n",
    "\n",
    "    return f'authors: {finalAuthors}' + \", \\r\\n\" + f'editors: {finalEditors}' \\\n",
    "+ \", \\r\\n\" + f'doi: {finalDoi}' +  \", \\r\\n\"  +  f'year: {finalYear}' +  \", \\r\\n\"  + f'number : {finalNumber}' \\\n",
    "+  \", \\r\\n\" + f'volume : {finalVolume}' +  \", \\r\\n\"  + f'edition: {finalEdition}' +  \", \\r\\n\"  + f'page: {finalPage}' \\\n",
    "+  \", \\r\\n\"  + f'url: {finalURL}' +  \", \\r\\n\"  + f'publisher: {finalPublisher}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a1358ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startIndexReplace : 0\n",
      "endIndexReplace : 34\n",
      "text after replaceSubstring : Hinton, G., Bengio, Y., & LeCun, Y\n",
      "text after replace authors : #AUTHOR#. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\n",
      "function processNames, surenameFirst: False\n",
      "Fall ., Hinton, G., Bengio, Y., & LeCun, Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startIndexReplace : 64\n",
      "endIndexReplace : 81\n",
      "text after replaceSubstring : J. Smith & J. Doe\n",
      "text after replace editors : #AUTHOR#. (2021). Deep Learning for Artificial Intelligence. In #EDITOR# (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\n",
      "text[endIndexEditors:] : Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\n",
      "startIndexEditorMarker : 1\n",
      "endIndexEditorMarker : 7\n",
      "startIndexEditorMarker : 73\n",
      "endIndexEditorMarker : 79 \r\n",
      "\n",
      "editors: J. Smith & J. Doe\n",
      "function processNames, surenameFirst: True\n",
      "Fall surenameFirst\n",
      "finalEditors : J. Smith and J. Doe \r\n",
      "\n",
      "startIndexReplace : 73\n",
      "endIndexReplace : 79\n",
      "text after replaceSubstring : (Eds.)\n",
      "text after replace EditorMarker : #AUTHOR#. (2021). Deep Learning for Artificial Intelligence. In #EDITOR# , Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\n",
      "\n",
      "startIndexReplace : 192\n",
      "endIndexReplace : 233\n",
      "text after replaceSubstring : https://doi.org/10.1109/ICNN.2021.9483948\n",
      "startIndexReplace : 10\n",
      "endIndexReplace : 16\n",
      "text after replaceSubstring : (2021)\n",
      "startIndexReplace : 161\n",
      "endIndexReplace : 173\n",
      "text after replaceSubstring : pp. 100–120)\n",
      "startIndexReplace : 158\n",
      "endIndexReplace : 159\n",
      "text after replaceSubstring : 5\n",
      "startIndexReplace : 149\n",
      "endIndexReplace : 150\n",
      "text after replaceSubstring : 1\n",
      "text after replace : #AUTHOR#. #YEAR#. Deep Learning for Artificial Intelligence. In #EDITOR# , Proceedings of the IEEE International Conference on Neural Networks .Vol. #VOL#, Issue #NUM#, #PAGE#. Physical Review. #URL#\n",
      "['AUTHOR', '.', 'YEAR', '. Deep Learning for Artificial Intelligence. In', 'EDITOR', ', Proceedings of the IEEE International Conference on Neural Networks .Vol.', 'VOL', ', Issue', 'NUM', ',', 'PAGE', '. Physical Review.', 'URL']\n",
      "possiblePublisher: URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possiblePublisher: URL\n",
      "text after replace Publisher : #AUTHOR#. #YEAR#. Deep Learning for Artificial Intelligence. In #EDITOR# , Proceedings of the IEEE International Conference on Neural Networks .Vol. #VOL#, Issue #NUM#, #PAGE#. Physical Review. #URL#\n",
      "authors: G. Hinton and Y. Bengio and Y. LeCun, \r\n",
      "editors: J. Smith and J. Doe, \r\n",
      "doi: , \r\n",
      "year: 2021, \r\n",
      "number : 5, \r\n",
      "volume : 1, \r\n",
      "edition: , \r\n",
      "page: 100–120, \r\n",
      "url: https://doi.org/10.1109/ICNN.2021.9483948, \r\n",
      "publisher: \n"
     ]
    }
   ],
   "source": [
    "text=\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "\n",
    "print(create_bibtex(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11af99c",
   "metadata": {},
   "source": [
    "Idee: Nun zunächst Jahr, Volume, Seiten, Edition und URL/DOI extrahieren. Den Rest (also Titel, Publisher, Series), dann nochmal den SpacyParer drüber laufen lassen, weil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc72533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 22, 'edited by')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score          word  start  end\n",
      "0         MISC  0.871492          Book      5    9\n",
      "1          PER  0.999499  Martin Trump     24   36\n"
     ]
    }
   ],
   "source": [
    "s = \"This Book was edited by Martin Trump\" \n",
    "print(getIndexOfSubstring(s, [\"edited by\"]))\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text = s\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "print(df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ad04c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geoffrey Geoffrey PROPN NNP compound Xxxxx True False\n",
      "Hinton Hinton PROPN NNP ROOT Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "Yoshua Yoshua PROPN NNP compound Xxxxx True False\n",
      "Bengio Bengio PROPN NNP appos Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "and and CCONJ CC cc xxx True True\n",
      "Yann Yann PROPN NNP compound Xxxx True False\n",
      "LeCun LeCun PROPN NNP conj XxXxx True False\n",
      ". . PUNCT . punct . False False\n",
      "2021 2021 NUM CD ROOT dddd False False\n",
      ". . PUNCT . punct . False False\n",
      "Deep Deep PROPN NNP compound Xxxx True False\n",
      "Learning Learning PROPN NNP ROOT Xxxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "Artificial Artificial PROPN NNP compound Xxxxx True False\n",
      "Intelligence Intelligence PROPN NNP pobj Xxxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "In in ADP IN ROOT Xx True True\n",
      "Proceedings Proceedings PROPN NNP pobj Xxxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "IEEE IEEE PROPN NNP compound XXXX True False\n",
      "International International PROPN NNP compound Xxxxx True False\n",
      "Conference Conference PROPN NNP pobj Xxxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "Neural Neural PROPN NNP compound Xxxxx True False\n",
      "Networks Networks PROPN NNPS pobj Xxxxx True False\n",
      "( ( PUNCT -LRB- punct ( False False\n",
      "Advances Advances PROPN NNPS appos Xxxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "Neural Neural PROPN NNP compound Xxxxx True False\n",
      "Information Information PROPN NNP compound Xxxxx True False\n",
      "Processing Processing PROPN NNP pobj Xxxxx True False\n",
      ") ) PUNCT -RRB- punct ) False False\n",
      ", , PUNCT , punct , False False\n",
      "IEEE IEEE PROPN NNP compound XXXX True False\n",
      "University University PROPN NNP compound Xxxxx True False\n",
      "Press Press PROPN NNP meta Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "Montreal Montreal PROPN NNP conj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "Canada Canada PROPN NNP conj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "100–120 100–120 NUM CD appos ddd–ddd False False\n",
      ". . PUNCT . punct . False False\n",
      "DOI DOI PROPN NNP ROOT XXX True False\n",
      ": : PUNCT : punct : False False\n",
      "https://doi.org/10.1109 https://doi.org/10.1109 PROPN NNP nmod xxxx://xxx.xxx/dd.dddd False False\n",
      "/ / SYM SYM punct / False False\n",
      "ICNN.2021.9483948 ICNN.2021.9483948 PROPN NNP appos XXXX.dddd.dddd False False\n",
      "Geoffrey Hinton 0 15 PERSON\n",
      "Yoshua Bengio 17 30 PERSON\n",
      "Yann LeCun 36 46 PERSON\n",
      "2021 48 52 DATE\n",
      "Deep Learning for Artificial Intelligence 54 95 ORG\n",
      "the IEEE International Conference on Neural Networks 115 167 ORG\n",
      "IEEE University Press 213 234 ORG\n",
      "Montreal 236 244 GPE\n",
      "Canada 246 252 GPE\n",
      "100–120 254 261 CARDINAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   entity_group     score                                              word  \\\n",
      "0           PER  0.999674                                   Geoffrey Hinton   \n",
      "1           PER  0.999051                                     Yoshua Bengio   \n",
      "2           PER  0.994860                                        Yann LeCun   \n",
      "3          MISC  0.763999                                          Learning   \n",
      "4          MISC  0.932916                           Artificial Intelligence   \n",
      "5          MISC  0.961130  IEEE International Conference on Neural Networks   \n",
      "6          MISC  0.945457         Advances in Neural Information Processing   \n",
      "7           ORG  0.979995                             IEEE University Press   \n",
      "8           LOC  0.991279                                          Montreal   \n",
      "9           LOC  0.998504                                            Canada   \n",
      "10          ORG  0.763287                                              ICNN   \n",
      "\n",
      "    start  end  \n",
      "0       0   15  \n",
      "1      17   30  \n",
      "2      36   46  \n",
      "3      59   67  \n",
      "4      72   95  \n",
      "5     119  167  \n",
      "6     169  210  \n",
      "7     213  234  \n",
      "8     236  244  \n",
      "9     246  252  \n",
      "10    291  295  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-PER', 'score': 0.9997296, 'index': 1, 'word': 'Geoffrey', 'start': 0, 'end': 8}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.9996716, 'index': 2, 'word': 'Hi', 'start': 9, 'end': 11}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.9996216, 'index': 3, 'word': '##nton', 'start': 11, 'end': 15}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.9996699, 'index': 5, 'word': 'Yo', 'start': 17, 'end': 19}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.9979983, 'index': 6, 'word': '##shu', 'start': 19, 'end': 22}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.9995653, 'index': 7, 'word': '##a', 'start': 22, 'end': 23}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.99898916, 'index': 8, 'word': 'Ben', 'start': 24, 'end': 27}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.99903166, 'index': 9, 'word': '##gio', 'start': 27, 'end': 30}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.99966466, 'index': 12, 'word': 'Yan', 'start': 36, 'end': 39}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.9996431, 'index': 13, 'word': '##n', 'start': 39, 'end': 40}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.999579, 'index': 14, 'word': 'Le', 'start': 41, 'end': 43}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.99356526, 'index': 15, 'word': '##C', 'start': 43, 'end': 44}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-PER', 'score': 0.98184687, 'index': 16, 'word': '##un', 'start': 44, 'end': 46}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.76399887, 'index': 22, 'word': 'Learning', 'start': 59, 'end': 67}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9160853, 'index': 24, 'word': 'Art', 'start': 72, 'end': 75}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.87057996, 'index': 25, 'word': '##ific', 'start': 75, 'end': 79}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9549257, 'index': 26, 'word': '##ial', 'start': 79, 'end': 82}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9900739, 'index': 27, 'word': 'Intelligence', 'start': 83, 'end': 95}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.8279813, 'index': 33, 'word': 'IEEE', 'start': 119, 'end': 123}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.99123913, 'index': 34, 'word': 'International', 'start': 124, 'end': 137}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9902305, 'index': 35, 'word': 'Conference', 'start': 138, 'end': 148}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9719587, 'index': 36, 'word': 'on', 'start': 149, 'end': 151}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.99477345, 'index': 37, 'word': 'N', 'start': 152, 'end': 153}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9512147, 'index': 38, 'word': '##eur', 'start': 153, 'end': 156}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.97369057, 'index': 39, 'word': '##al', 'start': 156, 'end': 158}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.98795575, 'index': 40, 'word': 'Networks', 'start': 159, 'end': 167}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9800095, 'index': 42, 'word': 'Advances', 'start': 169, 'end': 177}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.88461185, 'index': 43, 'word': 'in', 'start': 178, 'end': 180}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.99360424, 'index': 44, 'word': 'N', 'start': 181, 'end': 182}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9263024, 'index': 45, 'word': '##eur', 'start': 182, 'end': 185}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9189576, 'index': 46, 'word': '##al', 'start': 185, 'end': 187}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.97249526, 'index': 47, 'word': 'Information', 'start': 188, 'end': 199}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.95279294, 'index': 48, 'word': 'Process', 'start': 200, 'end': 207}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.93488413, 'index': 49, 'word': '##ing', 'start': 207, 'end': 210}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-ORG', 'score': 0.9949018, 'index': 52, 'word': 'IEEE', 'start': 213, 'end': 217}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-ORG', 'score': 0.9643159, 'index': 53, 'word': 'University', 'start': 218, 'end': 228}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-ORG', 'score': 0.9807669, 'index': 54, 'word': 'Press', 'start': 229, 'end': 234}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-LOC', 'score': 0.9912788, 'index': 56, 'word': 'Montreal', 'start': 236, 'end': 244}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-LOC', 'score': 0.99850416, 'index': 58, 'word': 'Canada', 'start': 246, 'end': 252}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-ORG', 'score': 0.91807085, 'index': 81, 'word': 'I', 'start': 291, 'end': 292}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-ORG', 'score': 0.8852397, 'index': 82, 'word': '##C', 'start': 292, 'end': 293}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-ORG', 'score': 0.7398024, 'index': 83, 'word': '##N', 'start': 293, 'end': 294}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-ORG', 'score': 0.510037, 'index': 84, 'word': '##N', 'start': 294, 'end': 295}\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Test, um Datum Zuverlässig erkannt wird\n",
    "\n",
    "text = \"Geoffrey Hinton, Yoshua Bengio, and Yann LeCun. 2021. Deep Learning for Artificial Intelligence. In Proceedings of the IEEE International Conference on Neural Networks (Advances in Neural Information Processing), IEEE University Press, Montreal, Canada, 100–120. DOI:https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "print(df_outputs)\n",
    "pos_pipeline = pipeline(\"token-classification\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "results = pos_pipeline(text)\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "    #print(f\"Word: {result['word']}, POS Tag: {result['entity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9aaa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "doiUrl2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
