{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wenn man den typen bestimmte hat, sollte man zunächst alle Pflicht-Felder des Bibtex-Eintrages raussuchen und bestimmen.\n",
    "#Dazu die Pflichtattribute in einer Schleife durchgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bde3f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75bbdc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text=\"G. H. Hinton, , Bengio, Y. and LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "df_PER = df_outputs[df_outputs[\"entity_group\"] == \"PER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b9bb15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.999297</td>\n",
       "      <td>G</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.920350</td>\n",
       "      <td>H. Hinton</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.994751</td>\n",
       "      <td>Bengio</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998586</td>\n",
       "      <td>Y</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.961921</td>\n",
       "      <td>LeCun</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998160</td>\n",
       "      <td>Y</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>J. Smith</td>\n",
       "      <td>95</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.994571</td>\n",
       "      <td>J. Doe</td>\n",
       "      <td>106</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score       word  start  end\n",
       "0          PER  0.999297          G      0    1\n",
       "1          PER  0.920350  H. Hinton      3   12\n",
       "2          PER  0.994751     Bengio     16   22\n",
       "3          PER  0.998586          Y     24   25\n",
       "4          PER  0.961921      LeCun     31   36\n",
       "5          PER  0.998160          Y     38   39\n",
       "8          PER  0.971820   J. Smith     95  103\n",
       "9          PER  0.994571     J. Doe    106  112"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e9f3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "startIndexPER = df_PER[\"start\"].iloc[0]\n",
    "endIndexPER = df_PER[\"end\"].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2df9df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1\n",
      "B    4\n",
      "C    7\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispiel DataFrame erstellen\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "})\n",
    "\n",
    "# Zugriff auf die erste Zeile\n",
    "erste_zeile = df.iloc[0]\n",
    "print(erste_zeile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daa1be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinton 0 6 GPE\n",
      "G. 8 10 GPE\n",
      "Bengio 12 18 PERSON\n",
      "Y. 20 22 PERSON\n",
      "Y. 34 36 PERSON\n",
      "2021 38 42 DATE\n",
      "Deep Learning for Artificial Intelligence 0 41 ORG\n",
      "J. Smith & J. Doe (Eds 3 25 ORG\n",
      "Proceedings of the IEEE International Conference on Neural Networks 29 96 ORG\n",
      "1 103 104 CARDINAL\n",
      "5 112 113 DATE\n",
      "100–120 119 126 CARDINAL\n",
      "IEEE Press 0 10 ORG\n",
      "https://doi.org/10.1109/ICNN.2021.9483948 0 41 GPE\n"
     ]
    }
   ],
   "source": [
    "#Test, um Datum Zuverlässig erkannt wird\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hinton, G., Bengio, Y. and LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\")\n",
    "for ent in doc.ents:\n",
    "  print(ent, ent.start_char-ent.sent.start_char, ent.end_char-ent.sent.start_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55d2b1",
   "metadata": {},
   "source": [
    "@book{advanced_physics, author = {Stephen Hawking and Brian Greene and Maria Trump and Susan Soy}, title = {Advanced Concepts in Theoretical Physics}, publisher = {Cambridge University Press}, year = {2025}, edition = {3}, volume = {1}, series = {Advanced Studies in Physics}, address = {Cambridge, UK}, month = {May}, note = {A seminal work in the field of theoretical physics}, isbn = {978-0-521-76948-0}, doi = {10.1017/example.book.2025}, url = {https://www.cambridge.org/advanced_physics}, annote = {Widely cited in the physics community}, abstract = {This book explores cutting-edge theories and concepts in theoretical physics...}, keywords = {Theoretical Physics, Quantum Mechanics, String Theory}, language = {English}, price = {75.00}, size = {600 pages}, lccn = {2020934576}, mrnumber = {MR3070071} }\n",
    "\n",
    "@article{quantum_entanglement,\n",
    "  author        = {Albert Einstein and Boris Podolsky and Nathan Rosen},\n",
    "  title         = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},\n",
    "  journal       = {Physical Review},\n",
    "  year          = {1935},\n",
    "  volume        = {47},\n",
    "  number        = {10},\n",
    "  pages         = {777-780},\n",
    "  month         = {May},\n",
    "  note          = {EPR Paradox paper, fundamental for quantum mechanics},\n",
    "  doi           = {10.1103/PhysRev.47.777},\n",
    "  url           = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},\n",
    "  abstract      = {In this paper, the authors discuss the EPR paradox and challenge the completeness of quantum mechanics...},\n",
    "  keywords      = {Quantum Mechanics, EPR Paradox, Physical Reality},\n",
    "  language      = {English},\n",
    "  publisher     = {American Physical Society}\n",
    "}\n",
    "\n",
    "@inproceedings{deep_learning,\n",
    "  author        = {Geoffrey Hinton and Yoshua Bengio and Yann LeCun},\n",
    "  title         = {Deep Learning for Artificial Intelligence},\n",
    "  booktitle     = {Proceedings of the IEEE International Conference on Neural Networks},\n",
    "  year          = {2021},\n",
    "  editor        = {Jane Smith and John Doe},\n",
    "  volume        = {1},\n",
    "  number = {5},\n",
    "  series        = {Advances in Neural Information Processing},\n",
    "  pages         = {100-120},\n",
    "  address       = {Montreal, Canada},\n",
    "  month         = {December},\n",
    "  organization  = {IEEE},\n",
    "  publisher     = {IEEE Press},\n",
    "  note          = {Keynote paper on recent advancements in deep learning},\n",
    "  isbn          = {978-1-5386-4637-1},\n",
    "  doi           = {10.1109/ICNN.2021.9483948},\n",
    "  url           = {https://ieeexplore.ieee.org/document/9483948},\n",
    "  annote        = {A seminal work on how deep learning transforms AI},\n",
    "  abstract      = {This paper explores cutting-edge deep learning techniques and their impact on the development of artificial intelligence...},\n",
    "  keywords      = {Deep Learning, Artificial Intelligence, Neural Networks},\n",
    "  language      = {English}\n",
    "}\n",
    "\n",
    "\n",
    "@incollection{quantum_computation,\n",
    "  author        = {Michael A. Nielsen and Isaac L. Chuang},\n",
    "  title         = {Quantum Computation and Quantum Information},\n",
    "  booktitle     = {Handbook of Quantum Information Science},\n",
    "  publisher     = {Springer},\n",
    "  year          = {2026},\n",
    "  editor        = {Charles H. Bennett and David P. DiVincenzo},\n",
    "  volume        = {4},\n",
    "  series        = {Quantum Science and Technology},\n",
    "  chapter       = {10},\n",
    "  pages         = {250-300},\n",
    "  address       = {Berlin, Germany},\n",
    "  month         = {October},\n",
    "  note          = {A comprehensive overview of the fundamentals of quantum computation},\n",
    "  isbn          = {978-3-540-88702-7},\n",
    "  doi           = {10.1007/springerreference_303198},\n",
    "  url           = {https://www.springer.com/gp/book/9783540887027},\n",
    "  annote        = {Essential reading for researchers entering the field of quantum information},\n",
    "  abstract      = {This chapter delves into the principles of quantum computing, offering an accessible yet thorough introduction...},\n",
    "  keywords      = {Quantum Computing, Quantum Information, Computational Models},\n",
    "  language      = {English},\n",
    "  price         = {45.00},\n",
    "  size          = {50 pages}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93a7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RegEx zum FInden der Seiten\n",
    "pageFinder = \", (?:pp\\.? )?\\d+-\\d+,\"\n",
    "doi1 = \"https:\\/\\/doi\\.org\"\n",
    "doi2 = \"DOI:\"\n",
    "\n",
    "#if havard und book:\n",
    "volume = \", \\d+,\"\n",
    "number = \", \\d+,\"\n",
    "edition1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "edition2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "\n",
    "#if APA or Havard then\n",
    "volume = \"\\(Vol\\. \\d+\" #für Volume und number noch als Bedingung, dass es nach Titel stehen muss, also Index > Titel-Index\n",
    "number = \"Issue \\d+\"\n",
    "yearApaHavard = \"\\(\\d{4}\\)\"\n",
    "editorApaHavard = \" (\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors|edited by )(\\))?\"\n",
    "\n",
    "#editor bei anderen\n",
    "\n",
    "\n",
    "#Beachte: Editor und Edition werden oft mit gleichen Abkürzungen versehen, auch im gleichen Stil! Nur Edition wird oft\n",
    "#klein geschrieben. Daher zustzlich prüfen, ob NER das als Person erkennt.\n",
    "\n",
    "#Beachte: ieee und mla trennen mit , und nicht mit .\n",
    "#if ieee and mla\n",
    "volumeIeeeMla = \", vol\\. \\d+\" \n",
    "noIeeeMla = \", no\\. \\d+\"\n",
    "\n",
    "etAl = \"et al.\"\n",
    "\n",
    "yearACM = \"\\. \\d{4}\\.\"\n",
    "\n",
    "#Man sollte zunächst die Felder extrahieren, die zu 100% sicher erkennen kann wie Volume oder Number, doi, Autoren...-> \n",
    "#Titel, booktitle, series und journal danach\n",
    "\n",
    "#man sollte eine Überdeckungsprüfung machen: Also Bspw. sagt Space im Bereich 25-50 ist eine Orga, Regex sagt, \n",
    "#dort ist ein (Eds.)\n",
    "#zu finden und Huggingface sagt, von 25-45 ist was. Dann sollte der minimalteste Wert und maximalste Wert \n",
    "#genommen werden und so lange nach links und analog nach rechts gehen, bis wieder ein Punkt kommt.\n",
    "# Da erkannte Felder aus dem String entfernt werden, muss der String am Ende leer sein. \n",
    "#Daher sollten zunächste die Dinge ausgeschnitten werden wo der Algo sich am sichersten ist\n",
    "#TODO: Grundsätzliche Reihenfolge der BibTex-Einträge ermitteln.\n",
    "#TODO: Für jede Extrahierung eines Feldes eine eigene Unterroutine schreiben? -> In der Oberschleife sollte Style sein\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80320cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first white-space character is located in position: 25\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = \"Tpp. 250–300). Springer. https://doi.org/10.1007/springerreference_303198\"\n",
    "x = re.search(doi1, txt)\n",
    "startIndex = x.start()\n",
    "endIndex = x.end()\n",
    "\n",
    "print(\"The first white-space character is located in position:\", x.start()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e11e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "listRegEx = [pageFinder, doi1]\n",
    "for regex in listRegEx:\n",
    "    x = re.search(regex, txt)\n",
    "    if x:\n",
    "        startIndex = x.start()\n",
    "        endIndex = x.end()\n",
    "        print(startIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd2124",
   "metadata": {},
   "source": [
    "Test Parser APA + Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8acde04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   entity_group     score        word  start  end\n",
      "0           PER  0.999392       Singh      0    5\n",
      "1           PER  0.997331           S      7    8\n",
      "2           PER  0.998019           K     10   11\n",
      "3           PER  0.999534       Kumar     14   19\n",
      "4           PER  0.998208           S     21   22\n",
      "5           PER  0.984761       Mehra     27   32\n",
      "6           PER  0.999057           P     34   35\n",
      "7           PER  0.998489           S     37   38\n",
      "8           PER  0.837229           E     66   67\n",
      "9           PER  0.862536  F. Johnson     69   79\n",
      "10          PER  0.992032           R     82   83\n",
      "11          PER  0.965553      H. Lee     85   91\n",
      "listIndicesAnd [[22, 27, ', & '], [79, 82, ' & ']]\n",
      "Fall ., Singh, S. K., Kumar, S., & Mehra, P. S.\n",
      "S. K. Singh and S. Kumar and P. S. Mehra\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text=\"Hawking, S., Greene, B., Trump, M. and Soy, S. (2025). Advanced Concepts and John Miller in Theoretical Physics (3rd ed., Vol. 1). Cambridge University Press. https://doi.org/10.1017/example.book.2025\"\n",
    "text = \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chat GPT & Google Bard AI: A Review. 2023 International Conference on IoT, Communication and Automation Technology (ICICAT), 1–6. https://doi.org/10.1109/ICICAT57735.2023.10263706\"\n",
    "text = \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chapter Title. In E. F. Johnson & R. H. Lee (Eds.), Book Title. Publisher Name.\"\n",
    "#text = \"Singh, S. K. and Peterman, P. (2023). Chapter and Title. Trump, M. and Soy, S. Book Title. Publisher Name.\"\n",
    "#text = \"S. K. Singh, S. Kumar & P. S. Mehra, (2023). Chapter Title. In E. F. Johnson & R. H. Lee (Eds.), Book Title. Publisher Name.\"\n",
    "\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "#index neu setzen, da diese nicht automatich geupdates werden\n",
    "df_PER = df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "df_PER_len = len(df_PER.index)\n",
    "\n",
    "\n",
    "print(df_PER)\n",
    "\n",
    "fullNameInOneWord = False\n",
    "allFullNamesInOneWord = True\n",
    "nameAbgekürzt = False\n",
    "surenameFirst = False\n",
    "\n",
    "for index in df_PER.index.values.tolist():\n",
    "    if \".\" == text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "        nameAbgekürzt = True\n",
    "for element in text:\n",
    "    if element == \",\":\n",
    "        break\n",
    "    if element in [\".\", \" \"]:\n",
    "        surenameFirst = True\n",
    "\n",
    "\n",
    "editorRegEx = \" (\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\"\n",
    "startIndexEditorMarker = -1\n",
    "if re.search(editorRegEx, txt):\n",
    "    startIndexEditorMarker = x.start()\n",
    "\n",
    "\n",
    "startIndexPER = df_PER[\"start\"].iloc[0]\n",
    "endIndexPER = df_PER[\"end\"].iloc[-1] + 1\n",
    "#Beachte, ein ., auch zwischen Autoren und Titel vorkommen kann\n",
    "authors = text[startIndexPER:endIndexPER]\n",
    "editors = \"\"\n",
    "andInAuthors = False\n",
    "dotKommaInAuthors = False\n",
    "firstStartIndexAnd = -1\n",
    "firstEndIndexAnd = -1\n",
    "search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "\n",
    "if \"et al.\" not in text:\n",
    "    firstStartIndexAnd, firstEndIndexAnd, andTyp = find_First_And_index(authors, search_terms)\n",
    "    if firstStartIndexAnd > -1:\n",
    "        andInAuthors = True\n",
    "else:\n",
    "    print(\"et. al\")\n",
    "\n",
    "if not surenameFirst:\n",
    "    if authors.find(\".,\") > -1:\n",
    "        dotKommaInAuthors = True\n",
    "\n",
    "\n",
    "#prüfe, wo der letzte Autor in der Autorenkette ist, um Namen auszuschließen, die im Titel sein können\n",
    "#vorher prüfen, wo et al. steht, weil so erkennt man ende sicher\n",
    "#zusäzich kann man noch prüfen, wo das erst \"and\" oder \"&\" kommt\n",
    "\n",
    "\n",
    "index_df_PER_List = df_PER.index.values.tolist()\n",
    "\n",
    "distanceAuthorNames = 0\n",
    "maxDistanceAuthorNames = 0\n",
    "startDistanceAuthorNames = 0\n",
    "endDistanceAuthorNames = 0\n",
    "authorSeparatedByAnd = False\n",
    "possibleEditors = False\n",
    "andTyp = \"\"\n",
    "\n",
    "firstStartIndexAnd = -1\n",
    "firstEndIndexAnd = -1\n",
    "listIndicesAnd = []\n",
    "for index in index_df_PER_List[:-1]:\n",
    "    textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "    firstStartIndexAnd, firstEndIndexAnd, andTyp = find_First_And_index(textBetweenNames, search_terms)\n",
    "    distanceAuthorNames = df_PER[\"start\"].iloc[index + 1] - df_PER[\"end\"].iloc[index]\n",
    "    if firstStartIndexAnd > -1:\n",
    "        listIndicesAnd.append([df_PER[\"end\"].iloc[index], df_PER[\"start\"].iloc[index + 1], andTyp])\n",
    "    if distanceAuthorNames > maxDistanceAuthorNames:\n",
    "        maxDistanceAuthorNames = distanceAuthorNames\n",
    "        #die endDistanceAuthorNames braucht man, um dann den STart der Editors zu finden\n",
    "        endIndexMaxDistanceAuthorNames = df_PER[\"start\"].iloc[index + 1]\n",
    "        startIndexMaxDistanceAuthorNames = df_PER[\"end\"].iloc[index]\n",
    "\n",
    "print(\"listIndicesAnd {0}\".format(listIndicesAnd))\n",
    "if len(listIndicesAnd) > 0:\n",
    "    authorSeparatedByAnd = True\n",
    "    for element in listIndicesAnd:\n",
    "        if maxDistanceAuthorNames > (element[1] - element[0]):\n",
    "            #Dann gibt es zwischen den Namen von Autoren eine größere Lücke als ein And. Irgendein And muss aber vorhanden sein.\n",
    "            #endDistanceAuthorNames und startDistanceAuthorNames zeigen dann Range der Lücke an\n",
    "            possibleEditors = True\n",
    "else:\n",
    "    # Dann kann der maximalste Abstand \"., \" sein\n",
    "    if maxDistanceAuthorNames > 3:\n",
    "        possibleEditors = True\n",
    "\n",
    "if possibleEditors:\n",
    "    authors = text[startIndexPER:startIndexMaxDistanceAuthorNames + 1]\n",
    "    editors = text[endIndexMaxDistanceAuthorNames:endIndexPER]\n",
    "else:\n",
    "    authors = text[startIndexPER:endIndexPER]\n",
    "\n",
    "andInAuthors = find_First_And_index(authors, search_terms)[2]\n",
    "andInEditors = find_First_And_index(editors, search_terms)[2]\n",
    "finalAuthors = \"\"\n",
    "finalEditors = \"\"\n",
    "\n",
    "if surenameFirst:\n",
    "    #hier völlig egal, ob er einzelne Initialen in ein eigenes Word gesteckt hat, obwohl es noch Nachnamen gib\n",
    "    andInAuhtors = find_First_And_index(authors, search_terms)[2]\n",
    "    authors = authors.replace(andInAuthors, \" and \")\n",
    "    finalAuthors = authors.replace(\", \", \" and \")\n",
    "elif andInAuthors != \"\":\n",
    "    if \"., \" in authors:\n",
    "        print(\"Fall ., {0}\".format(authors))\n",
    "        search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "        andInAuthors = find_First_And_index(authors, search_terms)[2]\n",
    "        authors = authors.replace(andInAuthors, \"., \")\n",
    "        authors = authors.split(\"., \")\n",
    "        authors = [name + \".\" for name in authors]\n",
    "        authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "        for author in authors[:-1]:\n",
    "            buffer = author.split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "        buffer = authors[-1].split(\", \")\n",
    "        finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "    elif \", \" in authors:\n",
    "        search_terms = [\", and \", \", & \", \" and \", \" & \"]\n",
    "        andInAuthors = find_First_And_index(authors, search_terms)[2]\n",
    "        authors = authors.replace(andInAuthors, \", \")\n",
    "        authors = authors.split(\", \")\n",
    "        for i in range(0, len(authors) - 3, 2):\n",
    "            finalAuthors = finalAuthors + authors[i+1] + \" \" + authors[i] + \" and \"\n",
    "        finalAuthors = finalAuthors + authors[len(authors) - 1] + \" \" + authors[len(authors) - 2]\n",
    "else:\n",
    "    print(\"Fall else {0}\".format(authors))\n",
    "    authors = authors.split(\", \")\n",
    "    finalAuthros = authors[1] + authors[0]\n",
    "        \n",
    "\n",
    "print(finalAuthors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1358ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(\"Hawking, S., Greene, B., Trump, M.,  Soy., S.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b180b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_First_And_index(text, search_terms):\n",
    "    # Initialisiere mit einem hohen Wert\n",
    "    min_index = float('inf')\n",
    "    end_index = 0\n",
    "    andTyp = \"\"\n",
    "    \n",
    "    # Suche jeden Suchbegriff in dem Text und behalte den kleinsten Index\n",
    "    for term in search_terms:\n",
    "        index = text.find(term)\n",
    "        if index != -1 and index < min_index:\n",
    "            min_index = index\n",
    "            end_index = min_index + len(term) - 1\n",
    "            andTyp = term\n",
    "    \n",
    "    # Wenn min_index unverändert ist, wurde keiner der Begriffe gefunden\n",
    "    return (min_index, end_index, andTyp) if min_index != float('inf') else (-1, -1, \"\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae188dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"\" == \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc72533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
