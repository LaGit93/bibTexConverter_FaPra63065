{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad55d2b1",
   "metadata": {},
   "source": [
    "@book{advanced_physics, author = {Stephen Hawking and Brian Greene and Maria Trump and Susan Soy}, title = {Advanced Concepts in Theoretical Physics}, publisher = {Cambridge University Press}, year = {2025}, edition = {3}, volume = {1}, series = {Advanced Studies in Physics}, address = {Cambridge, UK}, month = {May}, note = {A seminal work in the field of theoretical physics}, isbn = {978-0-521-76948-0}, doi = {10.1017/example.book.2025}, url = {https://www.cambridge.org/advanced_physics}, annote = {Widely cited in the physics community}, abstract = {This book explores cutting-edge theories and concepts in theoretical physics...}, keywords = {Theoretical Physics, Quantum Mechanics, String Theory}, language = {English}, price = {75.00}, size = {600 pages}, lccn = {2020934576}, mrnumber = {MR3070071} }\n",
    "\n",
    "@article{quantum_entanglement,\n",
    "  author        = {Albert Einstein and Boris Podolsky and Nathan Rosen},\n",
    "  title         = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},\n",
    "  journal       = {Physical Review},\n",
    "  year          = {1935},\n",
    "  volume        = {47},\n",
    "  number        = {10},\n",
    "  pages         = {777-780},\n",
    "  month         = {May},\n",
    "  note          = {EPR Paradox paper, fundamental for quantum mechanics},\n",
    "  doi           = {10.1103/PhysRev.47.777},\n",
    "  url           = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},\n",
    "  abstract      = {In this paper, the authors discuss the EPR paradox and challenge the completeness of quantum mechanics...},\n",
    "  keywords      = {Quantum Mechanics, EPR Paradox, Physical Reality},\n",
    "  language      = {English},\n",
    "  publisher     = {American Physical Society}\n",
    "}\n",
    "\n",
    "@inproceedings{deep_learning,\n",
    "  author        = {Geoffrey Hinton and Yoshua Bengio and Yann LeCun},\n",
    "  title         = {Deep Learning for Artificial Intelligence},\n",
    "  booktitle     = {Proceedings of the IEEE International Conference on Neural Networks},\n",
    "  year          = {2021},\n",
    "  editor        = {Jane Smith and John Doe},\n",
    "  volume        = {1},\n",
    "  number = {5},\n",
    "  series        = {Advances in Neural Information Processing},\n",
    "  pages         = {100-120},\n",
    "  address       = {Montreal, Canada},\n",
    "  month         = {December},\n",
    "  organization  = {IEEE},\n",
    "  publisher     = {IEEE Press},\n",
    "  note          = {Keynote paper on recent advancements in deep learning},\n",
    "  isbn          = {978-1-5386-4637-1},\n",
    "  doi           = {10.1109/ICNN.2021.9483948},\n",
    "  url           = {https://ieeexplore.ieee.org/document/9483948},\n",
    "  annote        = {A seminal work on how deep learning transforms AI},\n",
    "  abstract      = {This paper explores cutting-edge deep learning techniques and their impact on the development of artificial intelligence...},\n",
    "  keywords      = {Deep Learning, Artificial Intelligence, Neural Networks},\n",
    "  language      = {English}\n",
    "}\n",
    "\n",
    "\n",
    "@incollection{quantum_computation,\n",
    "  author        = {Michael A. Nielsen and Isaac L. Chuang},\n",
    "  title         = {Quantum Computation and Quantum Information},\n",
    "  booktitle     = {Handbook of Quantum Information Science},\n",
    "  publisher     = {Springer},\n",
    "  year          = {2026},\n",
    "  editor        = {Charles H. Bennett and David P. DiVincenzo},\n",
    "  volume        = {4},\n",
    "  series        = {Quantum Science and Technology},\n",
    "  chapter       = {10},\n",
    "  pages         = {250-300},\n",
    "  address       = {Berlin, Germany},\n",
    "  month         = {October},\n",
    "  note          = {A comprehensive overview of the fundamentals of quantum computation},\n",
    "  isbn          = {978-3-540-88702-7},\n",
    "  doi           = {10.1007/springerreference-303198},\n",
    "  url           = {https://www.springer.com/gp/book/9783540887027},\n",
    "  annote        = {Essential reading for researchers entering the field of quantum information},\n",
    "  abstract      = {This chapter delves into the principles of quantum computing, offering an accessible yet thorough introduction...},\n",
    "  keywords      = {Quantum Computing, Quantum Information, Computational Models},\n",
    "  language      = {English},\n",
    "  price         = {45.00},\n",
    "  size          = {50 pages}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8acde04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "torch.cuda.is_available()\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "def custom_strip(text):\n",
    "    strip_chars = string.whitespace + string.punctuation + '“”'\n",
    "    return text.strip(strip_chars)\n",
    "\n",
    "\n",
    "def getIndexOfSubstring(text, regEx = [], reverse = False):\n",
    "    #if reverse = False then it finds the first occurance of a given regEx.\n",
    "    #if reverse = True, then it finds the last occurance of a given regEx.\n",
    "    #beceause the occurance with the minimal startIndex is taken, it always chooses the most specific regex.\n",
    "    minStartIndex = float('inf')\n",
    "    endIndex = -1\n",
    "    matches = []\n",
    "    substring = \"\"\n",
    "    for regExElement in regEx:\n",
    "        matches = list(re.finditer(regExElement, text))\n",
    "        if matches:\n",
    "            if reverse:\n",
    "                match = matches[-1]\n",
    "            else:\n",
    "                match = matches[0]\n",
    "            if match.start() < minStartIndex:\n",
    "                minStartIndex = match.start()\n",
    "                endIndex = match.end()\n",
    "                substring = text[match.start():match.end()]\n",
    "    if substring != \"\":\n",
    "        return minStartIndex, endIndex, substring   \n",
    "    return -1, -1, substring\n",
    "\n",
    "def is_SurenameFirst(names):\n",
    "    splitedNames = names.split(\" \")\n",
    "    #regex wie w+ erkennt bspw. KEIN è \n",
    "    if splitedNames[0].endswith(\".\") or not isSpeceficPunctuation(splitedNames[0][-1], []):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def is_NameShortened(df_PER):\n",
    "    for index in df_PER.index.values.tolist():\n",
    "        if \".\" == text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def isSpeceficPunctuation(text, replaceCharacter = []):\n",
    "    allowed_chars = string.punctuation + string.whitespace\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return all(char in allowed_chars for char in text)\n",
    "\n",
    "def is_Editor(editorRegEx, textBetweenNames, index, text):\n",
    "    if re.search(editorRegEx, textBetweenNames):\n",
    "        x = re.search(editorRegEx, text)\n",
    "        #print(f'x: {x.start()}')\n",
    "        if isSpeceficPunctuation(text[index:x.start()], [\"&\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def processNames(authors):\n",
    "    finalAuthors = \"\"\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "    #andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "    surenameFirst = is_SurenameFirst(authors.strip())\n",
    "    print(f'function processNames, surenameFirst: {surenameFirst}')\n",
    "    if surenameFirst:\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        print(\"Fall surenameFirst\".format(authors))\n",
    "        #hier völlig egal, ob er einzelne Initialen in ein eigenes Word gesteckt hat, obwohl es noch Nachnamen gib\n",
    "        authors = authors.replace(andInAuthors, \" and \")\n",
    "        #print(f'authors: {authors}')\n",
    "        finalAuthors = authors.replace(\", \", \" and \")\n",
    "    elif \"., \" in authors:\n",
    "        print(\"Fall ., {0}\".format(authors))\n",
    "        search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        if andInAuthors != \"\":\n",
    "            authors = authors.replace(andInAuthors, \"., \")\n",
    "        authors = authors.split(\"., \")\n",
    "        authors = [name + \".\" for name in authors]\n",
    "        authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "        for author in authors[:-1]:\n",
    "            buffer = author.split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "        buffer = authors[-1].split(\", \")\n",
    "        finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "    elif \".; \" in authors:\n",
    "        print(\"Fall .; {0}\".format(authors))\n",
    "        search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        if andInAuthors != \"\":\n",
    "            authors = authors.replace(andInAuthors, \".; \")\n",
    "        authors = authors.split(\".; \")\n",
    "        authors = [name + \".\" for name in authors]\n",
    "        authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "        for author in authors[:-1]:\n",
    "            buffer = author.split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "        buffer = authors[-1].split(\", \")\n",
    "        finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "    elif \", \" in authors:\n",
    "        print(\"Fall , {0}\".format(authors))\n",
    "        search_terms = [\", and \", \", & \", \" and \", \" & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        if andInAuthors != \"\":  \n",
    "            authors = authors.replace(andInAuthors, \", \")\n",
    "        authors = authors.split(\", \")\n",
    "        for i in range(0, len(authors) - 3, 2):\n",
    "            finalAuthors = finalAuthors + authors[i+1] + \" \" + authors[i] + \" and \"\n",
    "        finalAuthors = finalAuthors + authors[len(authors) - 1] + \" \" + authors[len(authors) - 2]\n",
    "    return finalAuthors\n",
    "\n",
    "def getAuthors(text):\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    \n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    authorsDetected = False\n",
    "    setChainStart = True\n",
    "    startIndexAuthors = -1\n",
    "    endIndexAuthors = -1\n",
    "    chainStartIndex = -1\n",
    "    changedText = \"\"\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    \n",
    "    for index in index_df_PER_List:\n",
    "        #beachte: Hiermit lese ich immer schon vor!\n",
    "        if index < len(index_df_PER_List) - 1:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "        else:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "        onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "        firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "        onlyAnd = textBetweenNames == andTyp\n",
    "        if setChainStart: \n",
    "            chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "            setChainStart = False\n",
    "        if not onlyPunctuation and not onlyAnd:\n",
    "            setChainStart = True\n",
    "            startIndexAuthors = chainStartIndex\n",
    "            endIndexAuthors = df_PER[\"end\"].iloc[index]\n",
    "            break\n",
    "    if startIndexAuthors > -1:\n",
    "        changedText, author = replaceSubstring(startIndexAuthors, endIndexAuthors, text, \"\")\n",
    "        author = processNames(author)\n",
    "        return changedText, author\n",
    "    return text, \"\"\n",
    "\n",
    "def getEditors(text):\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    editorRegEx = \" (\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\"\n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    setChainStart = True\n",
    "    isEditor = False\n",
    "    startIndexEditors = -1\n",
    "    endIndexEditors = -1\n",
    "    chainStartIndex = -1\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "\n",
    "    for index in index_df_PER_List:\n",
    "        if index < len(index_df_PER_List) - 1:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "        else:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "        onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "        firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "        onlyAnd = textBetweenNames == andTyp\n",
    "        #wenn true, dann beginnt eine neue Autorenkette\n",
    "        if setChainStart: \n",
    "            chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "            #Solange das auf False, sollen der Substring erweitert werden, also start bleibt konstant\n",
    "            setChainStart = False\n",
    "        #Dann ist die Autorenkette zu Ende\n",
    "        if not onlyPunctuation and not onlyAnd:\n",
    "            setChainStart = True\n",
    "            #Es können auch nur Editoren und keine Autoren vorkommen\n",
    "            isEditor = is_Editor(editorRegEx, textBetweenNames, df_PER[\"end\"].iloc[index], text)\n",
    "            if isEditor:\n",
    "                startIndexEditors = chainStartIndex\n",
    "                endIndexEditors = df_PER[\"end\"].iloc[index]\n",
    "                break\n",
    "    #print(f'getAuthorsAndEditors: return: {[startIndexAuthors,endIndexAuthors],[startIndexEditors, endIndexEditors]}')\n",
    "    if startIndexEditors > -1:\n",
    "        changedText, editor = replaceSubstring(startIndexEditors, endIndexEditors, text, \"#EDITOR#\")\n",
    "        print(f'text after replace editors : {text}')\n",
    "        #es soll erst ab Editors gesucht werden, daher text[endIndexEditors:]. Sonst Verwechslungsgefahr\n",
    "        endIndexEditors = getIndexOfSubstring(text, [\"#EDITOR#\"])[1]\n",
    "        startIndexEditorMarker, endIndexEditorMarker, buffer = getIndexOfSubstring(changedText[endIndexEditors:], [editorRegEx])\n",
    "        startIndexEditorMarker = startIndexEditorMarker + endIndexEditors\n",
    "        endIndexEditorMarker = endIndexEditorMarker + endIndexEditors\n",
    "        editor = processNames(editor)\n",
    "        changedText, replacedEditorMarker = replaceSubstring(startIndexEditorMarker, endIndexEditorMarker, changedText, \".\")\n",
    "        startIndexEditor, endIndexEditor, buffer = getIndexOfSubstring(text, [\"#EDITOR#\"])\n",
    "        changedText, replacedEditorMarker = replaceSubstring(startIndexEditor, endIndexEditor, text, \".\")\n",
    "        return changedText, editor\n",
    "    return text, \"\"\n",
    "\n",
    "def getPublisher(text, doi):\n",
    "    publisher = \"\"\n",
    "    if doi != \"\":\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            publisher = data['message'].get('publisher', 'Publisher not found')\n",
    "    if publisher != \"\":\n",
    "        startIndex, endIndex, publisher = getIndexOfSubstring(text, [publisher], True)\n",
    "        #double check\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, publisher\n",
    "        changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        if publisher != \"\":\n",
    "            return changedText, custom_strip(publisher)\n",
    "    df_ORG = getORGTag(text, 0.8)\n",
    "    if not df_ORG.empty:\n",
    "        startIndex = df_ORG[\"start\"].iloc[0]\n",
    "        endIndex = df_ORG[\"end\"].iloc[0]\n",
    "        publisher = text[startIndex:endIndex]\n",
    "        #If the range determined by the tagger corresponds to a string \n",
    "        #that is only delimited by punctuation before and after, then it is most likely a publisher.\n",
    "        #startIndex - 2 because of a space inbetween\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "    return text, \"\"\n",
    "            \n",
    "\n",
    "\n",
    "def replaceSubstring (startIndex, endIndex, text, substituteString, ignorePunctuation = [\"&\", \"(\", \")\"]):\n",
    "    #The regex also checks for punctuation so that it is particularly precise. \n",
    "    #The cut text however should be without delimiters of the bibTex fields in the bibiography, \n",
    "    #so that future regex are not affected.\n",
    "    if endIndex > 0:\n",
    "        startIndexReplace = -1\n",
    "        endIndexReplace = -1\n",
    "        changedText = text\n",
    "        if startIndex > 0:\n",
    "            for i in range(startIndex - 1, 0, -1):\n",
    "                if not isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    startIndexReplace = i + 1\n",
    "                    for n in range(startIndexReplace, len(text), 1):\n",
    "                        if not isSpeceficPunctuation(text[n], ignorePunctuation):\n",
    "                            startIndexReplace = n\n",
    "                            break\n",
    "                    break\n",
    "        else:\n",
    "            startIndexReplace = 0\n",
    "            \n",
    "        #print(f' replaceSubstring, startIndexReplace={{{startIndexReplace}}}')\n",
    "        if endIndex < len(text):\n",
    "            for i in range(endIndex, len(text), 1):\n",
    "                if not isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    endIndexReplace = i\n",
    "                    for n in range(endIndexReplace-1, 0, -1):\n",
    "                        if not isSpeceficPunctuation(text[n], ignorePunctuation):\n",
    "                            endIndexReplace = n + 1\n",
    "                            break\n",
    "                    break\n",
    "        else:\n",
    "            endIndexReplace = len(text)\n",
    "        #print(f' replaceSubstring, endIndexReplace={{{endIndexReplace}}}')\n",
    "        changedText = text[0:startIndexReplace] + substituteString + text[endIndexReplace:len(text)]\n",
    "        return changedText, text[startIndexReplace:endIndexReplace]\n",
    "    return text, \"\"\n",
    "\n",
    "def getAddress(text):\n",
    "    overflow = 0\n",
    "    df_LOC = getLOCTag(text)\n",
    "    addressFound = False\n",
    "    if not df_LOC.empty:\n",
    "        startIndex = df_LOC[\"start\"].iloc[-1]\n",
    "        endIndex = df_LOC[\"end\"].iloc[-1]\n",
    "        address = text[startIndex:endIndex]\n",
    "        #If the range determined by the tagger corresponds to a string \n",
    "        #that is only delimited by punctuation before and after, then it is most likely a publisher.\n",
    "        #startIndex - 2 because of a space inbetween\n",
    "        print(f' getAddress, len(text)={{{len(text)}}}')\n",
    "        print(f' getAddress, endIndex={{{endIndex}}}')\n",
    "        print(f' getAddress, text={{{text}}}')\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                addressFound = True\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                addressFound = True\n",
    "        if len(df_LOC) > 1 and addressFound:\n",
    "            startIndex2 = df_LOC[\"start\"].iloc[0]\n",
    "            endIndex2 = df_LOC[\"end\"].iloc[0]\n",
    "            address2 = text[startIndex2:endIndex2]\n",
    "            if isSpeceficPunctuation(text[startIndex2 - 2]) and isSpeceficPunctuation(text[endIndex2:startIndex]):\n",
    "                changedText, address = replaceSubstring(startIndex2, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(address)\n",
    "        if addressFound:\n",
    "            changedText, address = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "            return changedText, custom_strip(address)\n",
    "    return text, \"\"\n",
    "\n",
    "def getDate(text):\n",
    "    monthYearRegex = \"\\b(January|Jan\\.?|February|Feb\\.?|March|Mar\\.?|April|Apr\\.?\" \\\n",
    "    \"|May|May\\.?|June|Jun\\.?|July|Jul\\.?|August|Aug\\.?|September|Sep\\.?|Sept\\.?|October|\" \\\n",
    "    \"Oct\\.?|November|Nov\\.?|December|Dec\\.?)\\s\\d{4}\\b\"\n",
    "    changedText, monthYear  = getSubstringByRegEx(text, [monthYearRegex])\n",
    "    if monthYear == \"\":\n",
    "        yearRegEx1 = \"(\\(\\d{4}\\)|\\. \\d{4}\\.)\"\n",
    "        yearRegEx2 = \"(\\.|,) \\d{4}(\\.|,|;)\"\n",
    "        changedText, year  = getSubstringByRegEx(text, [yearRegEx1, yearRegEx2])\n",
    "        return changedText, \"\", f'year={{{year}}}'\n",
    "    monthYear = monthYear.split(' ')\n",
    "    return changedText, f'month={{{monthYear[0]}}}', f'year={{{monthYear[1]}}}'\n",
    "    \n",
    "def getTitel(text):\n",
    "    text = custom_strip(text)\n",
    "    limit = len(text) - 1\n",
    "    i = 0\n",
    "    #remove pairs of punctuation marks \n",
    "    while i < limit:\n",
    "        if isSpeceficPunctuation(text[i]) and isSpeceficPunctuation(text[i+1]):\n",
    "            text = text[:i] + \".\" + text[i+2:]\n",
    "            i = i - 1\n",
    "            limit = limit - 1\n",
    "        i = i +1\n",
    "    if text.count(\".\") == 1:\n",
    "        text = text.split(\".\")\n",
    "    elif text.count(\",\") == 1: \n",
    "        text = text.split(\",\")\n",
    "    elif text.count(\".\") == 2:\n",
    "        text = text.split(\".\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), custom_strip(text[2])\n",
    "    elif text.count(\",\") > 1:\n",
    "        text = text.rsplit(',', 1)\n",
    "    return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "\n",
    "def getPersonTags(text):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty: \n",
    "        return df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getORGTag(text, score):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"ORG\") & (df_outputs[\"score\"] >= score)].reset_index(drop=True).tail(1)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getLOCTag(text):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"LOC\")].reset_index(drop=True).tail(2)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getDoi(text):\n",
    "    doiUrlRegEx1 = \"https:\\/\\/doi\\.org(\\/[^\\s]*)?$\"\n",
    "    doiUrlRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org)?([^\\s]*)+$\"\n",
    "    text, doi  = getSubstringByRegEx(text, [doiUrlRegEx1, doiUrlRegEx2])\n",
    "    httpsDomainRegEx1 = \"https:\\/\\/doi\\.org\\/\"\n",
    "    httpsDomainRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org\\/)?\"\n",
    "    doi, httpsDomain = getSubstringByRegEx(doi, [httpsDomainRegEx1, httpsDomainRegEx2])\n",
    "    return text, custom_strip(doi)\n",
    "\n",
    "def getSubstringByRegEx(text, regex = []):\n",
    "    startIndex, endIndex, substring = getIndexOfSubstring(text, regex, True)\n",
    "    changedText, substring = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "    return changedText, custom_strip(substring)\n",
    "\n",
    "#search_terms = [\", et al.\", \" et al.\"]\n",
    "#firstStartIndex, firstEndIndex, etAl = find_First_Term(text, search_terms)\n",
    "#if firstStartIndex > -1:\n",
    "    #text = replaceSubstring(firstStartIndex, firstEndIndex, text, \", \")\n",
    "\n",
    "def create_bibtex(text):\n",
    "    author = \"\"\n",
    "    editor = \"\"\n",
    "    title = \"\"\n",
    "    booktitle = \"\"\n",
    "    journal = \"\"\n",
    "    series = \"\"\n",
    "    year = \"\"\n",
    "    volume = \"\"\n",
    "    number = \"\"\n",
    "    edition = \"\"\n",
    "    pages = \"\"\n",
    "    month = \"\"\n",
    "    publisher = \"\"\n",
    "    school = \"\"\n",
    "    address = \"\"\n",
    "    note = \"\"\n",
    "    annote = \"\"\n",
    "    doi = \"\"\n",
    "    url = \"\"\n",
    "    book = False\n",
    "    article = False\n",
    "    proceedings = False\n",
    "    inproceedings = False\n",
    "    incollection = False\n",
    "    phdThesis = False\n",
    "    \n",
    "    urlRegEx = \"https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?::\\d+)?(?:/[^\\s]*)?\"\n",
    "    pageRegEx = \"(?:pp\\.? )?\\d+(-|–)\\d+\"\n",
    "    volumeRegEx = \"(V|v)ol\\. \\d+\"\n",
    "    number1RegEx = \"no\\. \\d+\"\n",
    "    number2RegEx = \"Issue \\d+\"\n",
    "    number3RegEx = \"\\d+\"\n",
    "    edition1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "    edition2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "    \n",
    "    text, author = getAuthors(text)\n",
    "    text, editor = getEditors(text)\n",
    "    text, doi = getDoi(text)\n",
    "    text, url = getSubstringByRegEx(text, [urlRegEx])\n",
    "    text, month, year = getDate(text)\n",
    "    text, page = getSubstringByRegEx(text, [pageRegEx])\n",
    "    text, volume = getSubstringByRegEx(text, [volumeRegEx])\n",
    "    text, number = getSubstringByRegEx(text, [number1RegEx, number2RegEx, number3RegEx])\n",
    "    text, edition = getSubstringByRegEx(text, [edition1, edition2])\n",
    "    #BUGFIX: Wenn nur num vorkommt, dann schneidet volume die Zahl von num aus!!!!!!\n",
    "    #volume3 darf also erst geprüft werden, wenn num1 und num2 geprüft wurden.\n",
    "    #VOlumer erscheint aber immre vor number\n",
    "    text, address = getAddress(text)\n",
    "    text, publisher = getPublisher(text, doi)\n",
    "    titel, booktitel, series = getTitel(text)\n",
    "\n",
    "    return f'authors: {author}' + \", \\r\\n\" + f'editors: {editor}' \\\n",
    "+ \", \\r\\n\" + f'doi: {doi}' +  \", \\r\\n\"  +  f'{year}' +  \", \\r\\n\"  + f'number : {number}' \\\n",
    "+  \", \\r\\n\" + f'volume : {volume}' +  \", \\r\\n\"  + f'edition: {edition}' +  \", \\r\\n\"  + f'page: {page}' \\\n",
    "+  \", \\r\\n\"  + f'url: {url}' +  \", \\r\\n\"  + f'publisher: {publisher}' +  \", \\r\\n\"  + f'address: {address}' \\\n",
    "+  \", \\r\\n\" + f'titel: {titel}' +  \", \\r\\n\"  + f'booktitel: {booktitel}' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a1358ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function processNames, surenameFirst: True\n",
      "Fall surenameFirst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text after replace editors : , “Quantum Computation and Quantum Information,” in Handbook of Quantum Information Science, vol. 4, C. H. Bennett and D. P. DiVincenzo, Eds. Berlin, Germany: Springer, 2026, pp. 250–300. doi: 10.1007/springerreference-303198.\n",
      "function processNames, surenameFirst: True\n",
      "Fall surenameFirst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " getAddress, len(text)={166}\n",
      " getAddress, endIndex={151}\n",
      " getAddress, text={, “Quantum Computation and Quantum Information,” in Handbook of Quantum Information Science, , C. H. Bennett and D. P. DiVincenzo, Eds. Berlin, Germany: Springer, ,  }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors: M. A. Nielsen and I. L. Chuang, \r\n",
      "editors: C. H. Bennett and D. P. DiVincenzo, \r\n",
      "doi: 10.1007/springerreference-303198, \r\n",
      "year={2026}, \r\n",
      "number : , \r\n",
      "volume : vol. 4, \r\n",
      "edition: , \r\n",
      "page: pp. 250–300, \r\n",
      "url: , \r\n",
      "publisher: Springer, \r\n",
      "address: Berlin, Germany, \r\n",
      "titel: Quantum Computation and Quantum Information, \r\n",
      "booktitel: in Handbook of Quantum Information Science.C.H.Bennett and D.P.DiVincenzo.Eds\n"
     ]
    }
   ],
   "source": [
    "text=\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "text = \"M. A. Nielsen and I. L. Chuang, “Quantum Computation and Quantum Information,” in Handbook of Quantum Information Science, vol. 4, C. H. Bennett and D. P. DiVincenzo, Eds. Berlin, Germany: Springer, 2026, pp. 250–300. doi: 10.1007/springerreference-303198.\"\n",
    "#text = \"Nielsen, M. A.; Chuang, I. L. Quantum Computation and Quantum Information. In Handbook of Quantum Information Science; Bennett, C. H., DiVincenzo, D. P., Eds.; Quantum Science and Technology; Springer: Berlin, Germany, 2026; Vol. 4, pp 250–300. https://doi.org/10.1007/springerreference-303198.\"\n",
    "print(create_bibtex(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11af99c",
   "metadata": {},
   "source": [
    "Idee: Nun zunächst Jahr, Volume, Seiten, Edition und URL/DOI extrahieren. Den Rest (also Titel, Publisher, Series), dann nochmal den SpacyParer drüber laufen lassen, weil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc72533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 22, 'edited by')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score          word  start  end\n",
      "0         MISC  0.871492          Book      5    9\n",
      "1          PER  0.999499  Martin Trump     24   36\n"
     ]
    }
   ],
   "source": [
    "s = \"This Book was edited by Martin Trump\" \n",
    "print(getIndexOfSubstring(s, [\"edited by\"]))\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text = s\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "print(df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ad04c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsfsdf dsfsdf NOUN NN compound xxxx True False\n",
      "London London PROPN NNP ROOT Xxxxx True False\n",
      "London 7 13 GPE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score    word  start  end\n",
      "0          LOC  0.926873  London      7   13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-LOC', 'score': 0.92687315, 'index': 6, 'word': 'London', 'start': 7, 'end': 13}\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Test, um Datum Zuverlässig erkannt wird\n",
    "\n",
    "text = \"Geoffrey Hinton, Yoshua Bengio, and Yann LeCun. 2021. Deep Learning for Artificial Intelligence. In Proceedings of the IEEE International Conference on Neural Networks (Advances in Neural Information Processing), IEEE University Press, Montreal, Canada, 100–120. DOI:https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "text = \"dsfsdf London\"\n",
    "print(len(text))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "print(df_outputs)\n",
    "pos_pipeline = pipeline(\"token-classification\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "results = pos_pipeline(text)\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "    #print(f\"Word: {result['word']}, POS Tag: {result['entity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9aaa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function processNames, { hallo } \n"
     ]
    }
   ],
   "source": [
    "x = \"hallo\"\n",
    "print(f'function processNames, {{ {x} }} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09971af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The publisher for DOI 10.1109/SeFeT55524.2022.9908774 is: IEEE\n"
     ]
    }
   ],
   "source": [
    "doi = \"10.1109/SeFeT55524.2022.9908774\"\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_publisher_from_doi(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        publisher = data['message'].get('publisher', 'Publisher not found')\n",
    "        return publisher\n",
    "    else:\n",
    "        return 'DOI not found or invalid'\n",
    "\n",
    "# Beispiel-DOI\n",
    "publisher = get_publisher_from_doi(doi)\n",
    "print(f\"The publisher for DOI {doi} is: {publisher}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fda54e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    }
   ],
   "source": [
    "print(\"ldsf ffdsf df, kkkk, sdfsdf, in: nbaldd, #NUM#, London\"[53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6c5f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo wie gehts dir Hoffentlich gut\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def strip_punctuation(text):\n",
    "    # Erstelle eine Übersetzungstabelle, die alle Interpunktionszeichen entfernt\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Wende die Übersetzungstabelle auf den Text an\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Beispielverwendung\n",
    "text = \"Hallo, wie geht's dir? Hoffentlich gut!\"\n",
    "stripped_text = strip_punctuation(text)\n",
    "print(stripped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "690d3437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"Hallo\"[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92f327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
