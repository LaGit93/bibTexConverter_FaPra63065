{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad55d2b1",
   "metadata": {},
   "source": [
    "@book{advanced_physics, author = {Stephen Hawking and Brian Greene and Maria Trump and Susan Soy}, title = {Advanced Concepts in Theoretical Physics}, publisher = {Cambridge University Press}, year = {2025}, edition = {3}, volume = {1}, series = {Advanced Studies in Physics}, address = {Cambridge, UK}, month = {May}, note = {A seminal work in the field of theoretical physics}, isbn = {978-0-521-76948-0}, doi = {10.1017/example.book.2025}, url = {https://www.cambridge.org/advanced_physics}, annote = {Widely cited in the physics community}, abstract = {This book explores cutting-edge theories and concepts in theoretical physics...}, keywords = {Theoretical Physics, Quantum Mechanics, String Theory}, language = {English}, price = {75.00}, size = {600 pages}, lccn = {2020934576}, mrnumber = {MR3070071} }\n",
    "\n",
    "@article{quantum_entanglement,\n",
    "  author        = {Albert Einstein and Boris Podolsky and Nathan Rosen},\n",
    "  title         = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},\n",
    "  journal       = {Physical Review},\n",
    "  year          = {1935},\n",
    "  volume        = {47},\n",
    "  number        = {10},\n",
    "  pages         = {777-780},\n",
    "  month         = {May},\n",
    "  note          = {EPR Paradox paper, fundamental for quantum mechanics},\n",
    "  doi           = {10.1103/PhysRev.47.777},\n",
    "  url           = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},\n",
    "  abstract      = {In this paper, the authors discuss the EPR paradox and challenge the completeness of quantum mechanics...},\n",
    "  keywords      = {Quantum Mechanics, EPR Paradox, Physical Reality},\n",
    "  language      = {English},\n",
    "  publisher     = {American Physical Society}\n",
    "}\n",
    "\n",
    "@inproceedings{deep_learning,\n",
    "  author        = {Geoffrey Hinton and Yoshua Bengio and Yann LeCun},\n",
    "  title         = {Deep Learning for Artificial Intelligence},\n",
    "  booktitle     = {Proceedings of the IEEE International Conference on Neural Networks},\n",
    "  year          = {2021},\n",
    "  editor        = {Jane Smith and John Doe},\n",
    "  volume        = {1},\n",
    "  number = {5},\n",
    "  series        = {Advances in Neural Information Processing},\n",
    "  pages         = {100-120},\n",
    "  address       = {Montreal, Canada},\n",
    "  month         = {December},\n",
    "  organization  = {IEEE},\n",
    "  publisher     = {IEEE Press},\n",
    "  note          = {Keynote paper on recent advancements in deep learning},\n",
    "  isbn          = {978-1-5386-4637-1},\n",
    "  doi           = {10.1109/ICNN.2021.9483948},\n",
    "  url           = {https://ieeexplore.ieee.org/document/9483948},\n",
    "  annote        = {A seminal work on how deep learning transforms AI},\n",
    "  abstract      = {This paper explores cutting-edge deep learning techniques and their impact on the development of artificial intelligence...},\n",
    "  keywords      = {Deep Learning, Artificial Intelligence, Neural Networks},\n",
    "  language      = {English}\n",
    "}\n",
    "\n",
    "\n",
    "@incollection{quantum_computation,\n",
    "  author        = {Michael A. Nielsen and Isaac L. Chuang},\n",
    "  title         = {Quantum Computation and Quantum Information},\n",
    "  booktitle     = {Handbook of Quantum Information Science},\n",
    "  publisher     = {Springer},\n",
    "  year          = {2026},\n",
    "  editor        = {Charles H. Bennett and David P. DiVincenzo},\n",
    "  volume        = {4},\n",
    "  series        = {Quantum Science and Technology},\n",
    "  chapter       = {10},\n",
    "  pages         = {250-300},\n",
    "  address       = {Berlin, Germany},\n",
    "  month         = {October},\n",
    "  note          = {A comprehensive overview of the fundamentals of quantum computation},\n",
    "  isbn          = {978-3-540-88702-7},\n",
    "  doi           = {10.1007/springerreference-303198},\n",
    "  url           = {https://www.springer.com/gp/book/9783540887027},\n",
    "  annote        = {Essential reading for researchers entering the field of quantum information},\n",
    "  abstract      = {This chapter delves into the principles of quantum computing, offering an accessible yet thorough introduction...},\n",
    "  keywords      = {Quantum Computing, Quantum Information, Computational Models},\n",
    "  language      = {English},\n",
    "  price         = {45.00},\n",
    "  size          = {50 pages}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8acde04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "torch.cuda.is_available()\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "def custom_strip(text, replaceCharacter = []):\n",
    "    \n",
    "    '''\n",
    "    Strip-Funktion, die standardmäßig neben Whitespace auch zeichen aus string.punctuation und “ sowie ” entfernt werden.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Text, der gestripped werden soll\n",
    "    replaceCharacter = []: Liste von Zeichen, die aus der standardmäßigen Prüfung entfernt werden sollen.\n",
    "    \n",
    "    return: gestrippter Text\n",
    "    '''\n",
    "    \n",
    "    allowed_chars = string.punctuation + string.whitespace + \"“\" + \"”\"\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return text.strip(allowed_chars)\n",
    "\n",
    "\n",
    "def getIndexOfSubstring(text, regEx = [], reverse = False):\n",
    "    \n",
    "    '''\n",
    "    Prüft für eine Liste von RegEx, ob sie in text vorkommen. Der RegEx, der den Substring mit der grötßen\n",
    "    Länge ermittelt, kommt zum Zuge.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Text, wo das Auftreten des RegEx geprüft wird\n",
    "    regEx = []: Liste mit RegEx. \n",
    "    reverse: Ob das erste oder letzte Auftreten eines Matches geprüft maßgeblich ist. Wenn reverse = False, dann wird das\n",
    "    erste Auftreten geprüft.\n",
    "    \n",
    "    return: Substring und den zugehörigen Start- und Endindex.\n",
    "    '''\n",
    "    \n",
    "    length = 0\n",
    "    matches = []\n",
    "    substring = \"\"\n",
    "    #print(f'regEx: {regEx}')\n",
    "    #print(f'text: {text}')\n",
    "    for regExElement in regEx:\n",
    "        matches = list(re.finditer(regExElement, text))\n",
    "        #print(f'matches: {matches}')\n",
    "        if matches:\n",
    "            if reverse:\n",
    "                match = matches[-1]\n",
    "            else:\n",
    "                match = matches[0]\n",
    "            buffer = match.end() - match.start()\n",
    "            if buffer > length:\n",
    "                length = buffer\n",
    "                startIndex = match.start()\n",
    "                endIndex = match.end()\n",
    "                substring = text[match.start():match.end()]\n",
    "    if substring != \"\":\n",
    "        return startIndex, endIndex, substring   \n",
    "    return -1, -1, substring\n",
    "\n",
    "def replaceSubstring (startIndex, endIndex, text, substituteString, ignorePunctuation = [\"&\", \"(\", \")\"]):\n",
    "    \n",
    "    '''\n",
    "    Ersetzt in einem String einen Substring durch einen anderen Substring (substituteString). Wenn der zu ersetztende \n",
    "    Substring mit bestimmten Zeichen beginnt oder endet und diese bestimmten Zeichen nicht durch ignorePunctuation \n",
    "    ignoriert werden sollen, dann wird der StartIndex runtergezählt und der EndIndex hochgezählt. \n",
    "    \n",
    "    Parameter:\n",
    "    startIndex: Index, wo der zu ersetztende Substring in Text eingefügt werden soll\n",
    "    endIndex: Index, wo der zu ersetztende Substring in Text enden soll\n",
    "    text: Text, wo das Auftreten des Substrings geprüft wird\n",
    "    substituteString: Der einzufügende Substring\n",
    "    ignorePunctuation: Zeichen, die für die Indexverschiebung nicht berücksichtigt werden sollen\n",
    "    reverse: Ob das erste oder letzte Auftreten eines Matches geprüft werden soll. Wenn reverse = False, dann wird das\n",
    "    erste Auftreten geprüft.\n",
    "    \n",
    "    return: Substring und den zugehörigen Start- und Endindex.\n",
    "    '''\n",
    "    \n",
    "    if endIndex > 0:\n",
    "        startIndexReplace = 0\n",
    "        endIndexReplace = 0\n",
    "        if startIndex > 0:\n",
    "            for i in range(startIndex, -1, -1):\n",
    "                if isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    startIndexReplace = i + 1\n",
    "                    break\n",
    "        else:\n",
    "            startIndexReplace = 0            \n",
    "        #print(f' replaceSubstring, startIndexReplace={{{startIndexReplace}}}')\n",
    "        if endIndex < len(text):\n",
    "            for i in range(endIndex-1, len(text), 1):\n",
    "                if isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    endIndexReplace = i + 1\n",
    "                    break\n",
    "                elif i == len(text)-1:\n",
    "                    endIndexReplace = len(text)\n",
    "        else:\n",
    "            endIndexReplace = len(text)\n",
    "        #print(f' replaceSubstring, endIndexReplace={{{endIndexReplace}}}')\n",
    "        if endIndexReplace > 0:\n",
    "            changedText = text[0:startIndexReplace] + substituteString + text[endIndexReplace:len(text)]\n",
    "            return changedText, text[startIndexReplace:endIndexReplace]\n",
    "    return text, \"\"\n",
    "\n",
    "def is_SurenameFirst(names):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob die Autoren- oder Editornamen mit dem Vornamen beginnen. \n",
    "    \n",
    "    Parameter:\n",
    "    names: Substring vom Literaturstring, der die Autoren- oder Editornamen enthält\n",
    "    '''\n",
    "    \n",
    "    splitedNames = names.split(\" \")\n",
    "    #regex wie w+ erkennt bspw. KEIN è \n",
    "    if splitedNames[0].endswith(\".\"):\n",
    "        return True\n",
    "    splitedNames = names.split(\",\")\n",
    "    if all(\" \" in item.strip() for item in splitedNames):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def is_NameShortened(df_PER):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob die Namen mit einem Punkt abgekürzt sind\n",
    "    \n",
    "    Parameter:\n",
    "    df_Per: Dataframe mit den Start- und Endindizes von Personennamen\n",
    "    '''\n",
    "    \n",
    "    for index in df_PER.index.values.tolist():\n",
    "        if \".\" == text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def isSpeceficPunctuation(text, replaceCharacter = []):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob ein Sring nur aus bestimmten Satzzeichen besteht. Standardmäßig wird string.punctuation + string.whitespace\n",
    "    geprüft.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Text, der geprüft werden soll\n",
    "    replaceCharacter = []: Liste von Zeichen, die aus der standardmäßigen Prüfung entfernt werden sollen.\n",
    "    '''\n",
    "        \n",
    "    allowed_chars = string.punctuation + string.whitespace\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return all(char in allowed_chars for char in text)\n",
    "\n",
    "def is_Editor(editorRegEx, textBetweenNames, startIndexTextBetweenNames, markerBehind = True):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob es sich bei einem String um ein Signalwort für Editoren handelt.\n",
    "    \n",
    "    Parameter:\n",
    "    editorRegEx: RegEx, die Signalwörter für das Auftreten von Editoren erkennen sollen\n",
    "    textBetweenNames: Substring vom Literaturstring, der geprüft weden soll, ob das Signalwort enthalten ist. Dieser \n",
    "    Substring steht stehts zwischen potententiellen Namen.\n",
    "    startIndexTextBetweenNames: Startindex, wo Substring im original Literaturstring steht\n",
    "    markerBehind: Ob das Signalwort für Editoren vor oder hinter den Editorennamen im original Literaturstring steht\n",
    "    \n",
    "    return: Boolean, ob gefunden, und Start- und Endindizies, wo es im original Literaturstring vorkommt\n",
    "    '''\n",
    "    \n",
    "    startSubstring, endSubstring, substring = getIndexOfSubstring(textBetweenNames, [editorRegEx])\n",
    "    if startIndexTextBetweenNames > -1 and markerBehind:\n",
    "        if isSpeceficPunctuation(textBetweenNames[startIndexTextBetweenNames:startSubstring], [\"&\"]):\n",
    "            return True, startSubstring + startIndexTextBetweenNames, endSubstring + startIndexTextBetweenNames\n",
    "    elif startIndexTextBetweenNames > -1:\n",
    "        if isSpeceficPunctuation(textBetweenNames[endSubstring:startIndexTextBetweenNames], [\"&\"]):\n",
    "            return True, startSubstring + startIndexTextBetweenNames, endSubstring + startIndexTextBetweenNames\n",
    "    return False, -1, -1\n",
    "\n",
    "def processNames(authors):\n",
    "    \n",
    "    '''\n",
    "    Bereitet die Autorennamen in ein Standardformat auf\n",
    "    \n",
    "    Parameter:\n",
    "    authors: Substring aus dem original Literaturstring, der die Autoren enthält\n",
    "    \n",
    "    return: Standardformat für Autoren\n",
    "    '''\n",
    "    \n",
    "    finalAuthors = \"\"\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "    surenameFirst = is_SurenameFirst(authors.strip())\n",
    "    authors = custom_strip(authors)\n",
    "    if surenameFirst:\n",
    "        startIndex, endIndex, andInAuthors = getIndexOfSubstring(authors, search_terms)\n",
    "        if startIndex >= 0:\n",
    "            authors = authors.replace(andInAuthors, \" and \")\n",
    "            finalAuthors = authors.replace(\", \", \" and \")\n",
    "        else:\n",
    "            finalAuthors = authors\n",
    "    elif \"., \" in authors:\n",
    "        search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        authors = authors.replace(\"., \", \"#., \")\n",
    "        if andInAuthors != \"\":\n",
    "            authors = authors.replace(andInAuthors, \"#., \")\n",
    "        authors = authors.split(\"., \")\n",
    "        authors = [name.replace(\"#\",\".\") for name in authors]\n",
    "        authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "        for author in authors[:-1]:\n",
    "            buffer = author.split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "        buffer = authors[-1].split(\", \")\n",
    "        finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "    elif \", \" in authors:\n",
    "        search_terms = [\", and \", \", & \", \" and \", \" & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        if andInAuthors != \"\":  \n",
    "            authors = authors.replace(andInAuthors, \", \")\n",
    "        authors = authors.split(\", \")\n",
    "        for i in range(0, len(authors) - 3, 2):\n",
    "            finalAuthors = finalAuthors + authors[i+1] + \" \" + authors[i] + \" and \"\n",
    "        finalAuthors = finalAuthors + authors[len(authors) - 1] + \" \" + authors[len(authors) - 2]\n",
    "    return custom_strip(finalAuthors)\n",
    "\n",
    "def getAuthors(text):\n",
    "    \n",
    "    '''\n",
    "    Schneidet die Autoren aus dem original Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Literaturstring ohne Autoren und die Autoren\n",
    "    '''\n",
    "    \n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    \n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    authorsDetected = False\n",
    "    setChainStart = True\n",
    "    startIndexAuthors = -1\n",
    "    endIndexAuthors = -1\n",
    "    chainStartIndex = -1\n",
    "    changedText = \"\"\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    \n",
    "    if not df_PER.empty and df_PER[\"start\"].iloc[0] == 0:\n",
    "        for index in index_df_PER_List:\n",
    "            if index < len(index_df_PER_List) - 1:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "            else:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "            firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "            onlyAnd = textBetweenNames == andTyp\n",
    "            if setChainStart: \n",
    "                chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            if not onlyPunctuation and not onlyAnd:\n",
    "                setChainStart = True\n",
    "                startIndexAuthors = chainStartIndex\n",
    "                endIndexAuthors = df_PER[\"end\"].iloc[index]\n",
    "                break\n",
    "        if startIndexAuthors > -1:\n",
    "            changedText, author = replaceSubstring(startIndexAuthors, endIndexAuthors, text, \".\")\n",
    "            author = processNames(author)\n",
    "            return changedText, author\n",
    "    return text, \"\"\n",
    "\n",
    "def getEditors(text):\n",
    "    \n",
    "    '''\n",
    "    Schneidet die Editoren aus dem Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Literaturstring ohne Editoren und die ausgeschnittenen Editoren\n",
    "    '''\n",
    "    \n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    editorRegEx = \"\\s*(\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\\s*\"\n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    setChainStart = True\n",
    "    isEditor = False\n",
    "    startIndexEditors = -1\n",
    "    endIndexEditors = -1\n",
    "    chainStartIndex = -1\n",
    "    startIndexEditorMarker = -1\n",
    "    endIndexEditorMarker = -1\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    if not df_PER.empty:\n",
    "        for index in index_df_PER_List:\n",
    "            if index < len(index_df_PER_List) - 1:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "            else:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "            firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "            onlyAnd = textBetweenNames == andTyp\n",
    "            #if true, that a new chain of Authors begins. An Author Chain is for example \"Name1, Name2 and Name3\"\n",
    "            if setChainStart: \n",
    "                chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            #If the following if-STatement is true, than the chain has reached an end\n",
    "            if not onlyPunctuation and not onlyAnd:\n",
    "                setChainStart = True\n",
    "                #editors can be the first Part of an literature reference\n",
    "                textFromStartUntilFirstName = text[0:df_PER[\"start\"].iloc[0]]\n",
    "                isEditor, startIndexEditorMarker, endIndexEditorMarker = is_Editor(editorRegEx, textFromStartUntilFirstName, 0, False)\n",
    "                if startIndexEditorMarker == -1:\n",
    "                    isEditor, startIndexEditorMarker, endIndexEditorMarker = is_Editor(editorRegEx, textBetweenNames, df_PER[\"end\"].iloc[index])                         \n",
    "                if isEditor:\n",
    "                    startIndexEditors = chainStartIndex\n",
    "                    endIndexEditors = df_PER[\"end\"].iloc[index]\n",
    "                    break\n",
    "    if startIndexEditors > -1:\n",
    "        changedText, editor = replaceSubstring(startIndexEditors, endIndexEditors, text, \".\")\n",
    "        editor = processNames(editor)\n",
    "        if startIndexEditorMarker > -1:\n",
    "            if startIndexEditorMarker > startIndexEditors:\n",
    "                startIndexEditorMarker = startIndexEditorMarker - (len(text) - len(changedText))\n",
    "                endIndexEditorMarker = endIndexEditorMarker - (len(text) - len(changedText))\n",
    "            changedText, buffer = replaceSubstring(startIndexEditorMarker, endIndexEditorMarker, changedText, \".\")\n",
    "        startIndexIn = 0 \n",
    "        if startIndexEditorMarker < startIndexEditors:\n",
    "            startIndexEditors = startIndexEditors - (endIndexEditorMarker - startIndexEditorMarker)\n",
    "            endIndexEditors = endIndexEditors - (endIndexEditorMarker - startIndexEditorMarker)\n",
    "        for i in range(startIndexEditors-1, -1, -1):\n",
    "            if isSpeceficPunctuation(changedText[i], [\":\", \" \"]):\n",
    "                startIndexIn = i + 1\n",
    "                break\n",
    "        changedText, replacedEditorMarker = replaceSubstring(startIndexIn, startIndexEditors, changedText, \".\")\n",
    "        return changedText, editor\n",
    "    return text, \"\"\n",
    "    \n",
    "def getPersonTags(text):\n",
    "    \n",
    "    '''\n",
    "    Gibt die durch Named Entity Recognition erkannten dictionaries vom Typ Person in einem Dataframe zurück\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Person\n",
    "    '''\n",
    "    \n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty: \n",
    "        return df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getORGTag(text, score):\n",
    "    \n",
    "    '''\n",
    "    Gibt die durch Named Entity Recognition erkannten dictionaries vom Typ Organization in einem Dataframe zurück.\n",
    "    Um in das Dataframe aufgenommen zu werden, muss ein bestimmter Score erreicht sein.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    score: Schwellenwert zwischen 0 und 1.\n",
    "    \n",
    "    return: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Organization\n",
    "    '''\n",
    "    \n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"ORG\") & (df_outputs[\"score\"] >= score)].reset_index(drop=True).tail(1)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getLOCTag(text):\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    Gibt die durch Named Entity Recognition erkannten dictionaries vom Typ Location in einem Dataframe zurück\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Location\n",
    "    '''\n",
    "    \n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"LOC\")].reset_index(drop=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getDoi(text):\n",
    "    \n",
    "    '''\n",
    "    Schneidet die DOI aus dem Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Literaturstring ohne DOI und die ausgeschnittene DOI\n",
    "    '''\n",
    "    \n",
    "    doiUrlRegEx1 = \"https:\\/\\/doi\\.org(\\/[^\\s]*)?$\"\n",
    "    doiUrlRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org)?([^\\s]*)+$\"\n",
    "    changedText, doi  = getSubstringByRegEx(text, [doiUrlRegEx1, doiUrlRegEx2])\n",
    "    httpsDomainRegEx1 = \"https:\\/\\/doi\\.org\\/\"\n",
    "    httpsDomainRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org\\/)?\"\n",
    "    doi, httpsDomain = getSubstringByRegEx(doi, [httpsDomainRegEx1, httpsDomainRegEx2])\n",
    "    return changedText, custom_strip(doi)\n",
    "\n",
    "def getURL(text):\n",
    "        \n",
    "    '''\n",
    "    Schneidet die URL aus dem Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Literaturstring ohne URL und die ausgeschnittene URL\n",
    "    '''\n",
    "    \n",
    "    urlRegEx = \"(URL:|url:)?\\s*https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?::\\d+)?(?:/[^\\s]*)?\"\n",
    "    changedText, url = getSubstringByRegEx(text, [urlRegEx])\n",
    "    urlPrefixRegEx = r\"(url:\\s*|URL:\\s*)\"\n",
    "    url = re.sub(urlPrefixRegEx, '', url).strip()\n",
    "    return changedText, custom_strip(url)\n",
    "\n",
    "def getDate(text):\n",
    "            \n",
    "    '''\n",
    "    Schneidet Date aus dem Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Literaturstring ohne Date und ausgeschnittene Date\n",
    "    '''\n",
    "    \n",
    "    monthYearRegex = \"(January|Jan\\.?|February|Feb\\.?|March|Mar\\.?|April|Apr\\.?\" \\\n",
    "    \"|May|May\\.?|June|Jun\\.?|July|Jul\\.?|August|Aug\\.?|September|Sep\\.?|Sept\\.?|October|\" \\\n",
    "    \"Oct\\.?|November|Nov\\.?|December|Dec\\.?)\\s\\d{4}\"\n",
    "    changedText, monthYear  = getSubstringByRegEx(text, [monthYearRegex])\n",
    "    if monthYear == \"\":\n",
    "        yearRegEx1 = \"(\\.|,)? \\(\\d{4}\\)(\\.|,|:)\"\n",
    "        yearRegEx2 = \"(\\.|,) \\d{4}(\\.|,|;)\"\n",
    "        changedText, year  = getSubstringByRegEx(text, [yearRegEx1, yearRegEx2])\n",
    "        return changedText, \"\", f'{year}'\n",
    "    monthYear = monthYear.split(' ')\n",
    "    return changedText, f'{monthYear[0]}', f'{monthYear[1]}'\n",
    "\n",
    "def getPage(text):\n",
    "                \n",
    "    '''\n",
    "    Schneidet Page aus dem Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Literaturstring ohne Page und ausgeschnittene Page\n",
    "    '''\n",
    "    \n",
    "    pageRegEx = \"(?:pp\\.? )?\\d+(-|--|–)\\d+\"\n",
    "    changedText, pages = getSubstringByRegEx(text, [pageRegEx])\n",
    "    if pages != \"\":\n",
    "        pages = re.search(r'\\d+(-|--|–)\\d+', pages).group()\n",
    "    return changedText, custom_strip(pages)\n",
    "\n",
    "def getSubstringByRegEx(text, regex = []):\n",
    "    \n",
    "    '''\n",
    "    Prüft für eine Liste von RegEx, ob sie in text vorkommen. Der RegEx, der den Substring mit der grötßen\n",
    "    Länge ermittelt, kommt zum Zuge. Dieser Substring wird dann aus dem text ausgeschnitten. Ein RegEx wird\n",
    "    dabei von hinten beginnend in text geprüft und der erste Match zählt.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    regEx = []: Liste mit RegEx. \n",
    "    \n",
    "    return: Literaturstring ohne Substring und ausgeschnittenen Substring\n",
    "    '''\n",
    "    \n",
    "    startIndex, endIndex, substring = getIndexOfSubstring(text, regex, True)\n",
    "    changedText, substring = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "    return changedText, custom_strip(substring)\n",
    "\n",
    "def getVolumeNumber(text):\n",
    "                    \n",
    "    '''\n",
    "    Schneidet Volume und Number aus dem Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Literaturstring ohne Volume und Number und ausgeschnittene Volume und Number\n",
    "    '''\n",
    "    \n",
    "    volumeAndNumberRegex = \"(\\d+\\(\\d+\\)|\\d+\\.\\d+)\"\n",
    "    startIndex, endIndex, substring = getIndexOfSubstring(text, [volumeAndNumberRegex], True)\n",
    "    if startIndex > -1:\n",
    "        changedText, volumeNumber = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        startIndex, endIndex, volume = getIndexOfSubstring(volumeNumber, [\"\\d+\"])\n",
    "        startIndex, endIndex, number = getIndexOfSubstring(volumeNumber, [\"\\d+\"], True)\n",
    "        return changedText, volume, number\n",
    "    else:\n",
    "        volumeRegEx = \"(V|v)ol\\. \\d+\"\n",
    "        volumeRegEx2 = \", \\d+,\"\n",
    "        volumeRegEx3 = \",? \\d+(:|\\.)\"\n",
    "        number1RegEx = \"no\\. \\d+\"\n",
    "        number2RegEx = \"Issue \\d+\"\n",
    "        number3RegEx = \", \\d+,\"\n",
    "        number4RegEx = \"\\.\\d+\"\n",
    "        startIndex, endIndex, volume = getIndexOfSubstring(text, [volumeRegEx, volumeRegEx2, volumeRegEx3], True)\n",
    "        changedText, substring = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        startIndex, endIndex, number = getIndexOfSubstring(changedText, [number1RegEx, number2RegEx, number3RegEx, number4RegEx], True)\n",
    "        changedText, substring = replaceSubstring(startIndex, endIndex, changedText, \"\")\n",
    "        if volume != \"\":\n",
    "            volume = re.search(r'\\d+', volume).group(0)\n",
    "        if number != \"\":\n",
    "            number = re.search(r'\\d+', number).group(0)\n",
    "        return changedText, volume, number\n",
    "\n",
    "def getEdition(text):\n",
    "                        \n",
    "    '''\n",
    "    Schneidet Edition aus dem Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Literaturstring ohne Edition und ausgeschnittene Edition\n",
    "    '''\n",
    "    \n",
    "    editionRegEx1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "    editionRegEx2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "    changedText, edition = getSubstringByRegEx(text, [editionRegEx1, editionRegEx2])\n",
    "    if edition != \"\":\n",
    "        edition = re.search(r'\\d+', edition).group()\n",
    "    return changedText, custom_strip(edition)\n",
    "\n",
    "def getAddress(text):\n",
    "                            \n",
    "    '''\n",
    "    Schneidet Address aus dem Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Literaturstring ohne Address und ausgeschnittene Address\n",
    "    '''\n",
    "    \n",
    "    df_LOC = getLOCTag(text)\n",
    "    addressFound = False\n",
    "    index_df_Loc_List = df_LOC.index.values.tolist()\n",
    "    textBetweenAddress = \"\"\n",
    "    setChainStart = True\n",
    "    startIndex = 0\n",
    "    endIndex = 0\n",
    "    if not df_LOC.empty:\n",
    "        for index in reversed(index_df_Loc_List):\n",
    "            if index < len(index_df_Loc_List) and index > 0:\n",
    "                textBetweenAddress = text[df_LOC[\"end\"].iloc[index-1]:df_LOC[\"start\"].iloc[index]]\n",
    "            else:\n",
    "                textBetweenAddress = text[:df_LOC[\"start\"].iloc[index]]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenAddress, [])\n",
    "            if setChainStart: \n",
    "                chainEnIndex = df_LOC[\"end\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            if not onlyPunctuation:\n",
    "                startIndex = df_LOC[\"start\"].iloc[index]\n",
    "                endIndex = chainEnIndex\n",
    "                break\n",
    "        address = text[startIndex:endIndex]\n",
    "        if startIndex > 2 and endIndex < len(text) - 1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                addressFound = True\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                addressFound = True\n",
    "        if addressFound:\n",
    "            changedText, address = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "            return changedText, custom_strip(address)\n",
    "    return text, \"\"  \n",
    "\n",
    "def getPublisher(text, doi):\n",
    "                                \n",
    "    '''\n",
    "    Schneidet Publisher aus dem Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    doi: DOI, um Publisher in externen Datenbanken zu suchen\n",
    "    \n",
    "    return: Literaturstring ohne Publisher und ausgeschnittenen Publisher\n",
    "    '''\n",
    "    \n",
    "    publisher = \"\"\n",
    "    if doi != \"\":\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            publisher = data['message'].get('publisher', 'Publisher not found')\n",
    "    if publisher != \"\":\n",
    "        startIndex, endIndex, publisher = getIndexOfSubstring(text, [publisher], True)\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, publisher\n",
    "        changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        if publisher != \"\":\n",
    "            return changedText, custom_strip(publisher)\n",
    "    df_ORG = getORGTag(text, 0.8)\n",
    "    if not df_ORG.empty:\n",
    "        startIndex = df_ORG[\"start\"].iloc[0]\n",
    "        endIndex = df_ORG[\"end\"].iloc[0]\n",
    "        publisher = text[startIndex:endIndex]\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "    return text, \"\"\n",
    "    \n",
    "def getTitel(text):\n",
    "                                    \n",
    "    '''\n",
    "    Schneidet Titel aus dem Literaturstring aus\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring\n",
    "    \n",
    "    return: Literaturstring ohne Titel und ausgeschnittenen Titel\n",
    "    '''\n",
    "    \n",
    "    ignoreCharacters = [\"?\", \"!\", \"(\", \")\", \"“\", \"”\", \"\\\"\"]\n",
    "    text = custom_strip(text, ignoreCharacters)\n",
    "    ignoreCharacters = [\"?\", \":\", \"-\", \"(\", \")\", \"“\", \"”\", \"\\\"\"]\n",
    "    limit = len(text) - 1\n",
    "    i = 0\n",
    "    maxIndex = 0\n",
    "    while i < limit:\n",
    "        if (i + 2 < limit) and not (text[i] == \",\" and text[i+1] == \" \" and not isSpeceficPunctuation(text[i+2])):\n",
    "            if isSpeceficPunctuation(text[i], ignoreCharacters) and isSpeceficPunctuation(text[i+1], ignoreCharacters):\n",
    "                text = text[:i] + \".\" + text[i+2:]\n",
    "                i = i - 1\n",
    "                limit = limit - 1\n",
    "        i = i +1\n",
    "    \n",
    "    ignoreCharacters = [\"?\", \"!\", \"(\", \")\"]\n",
    "    if text[0] == \"“\":\n",
    "        text = text.rsplit('”', 1)\n",
    "        return custom_strip(text[0], ignoreCharacters), custom_strip(text[1], ignoreCharacters), \"\"\n",
    "    elif text[0] == \"\\\"\":\n",
    "        text = text.rsplit(\"\\\"\", 1)\n",
    "        return custom_strip(text[0], ignoreCharacters), custom_strip(text[1], ignoreCharacters), \"\"\n",
    "    elif text.count(\".\") == 1:\n",
    "        text = text.split(\".\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    elif text.count(\".\") == 2:\n",
    "        text = text.split(\".\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), custom_strip(text[2])\n",
    "    else:\n",
    "        for index, element in enumerate(text):\n",
    "            if isSpeceficPunctuation(element, [\".\", \",\", \" \", \"(\", \")\", \":\"]):\n",
    "                if maxIndex < index:\n",
    "                    maxIndex = index\n",
    "        if maxIndex > 0:\n",
    "            text = text.split(text[maxIndex])\n",
    "            return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    if maxIndex == 0 and text.count(\",\") == 1: \n",
    "        text = text.split(\",\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    elif maxIndex == 0 and text.count(\",\") > 1:\n",
    "        text = text.rsplit(',', 1)\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    return custom_strip(text), \"\", \"\"\n",
    "    \n",
    "    \n",
    "def getKey(author, year):\n",
    "                                        \n",
    "    '''\n",
    "    Erzeugt einen Key aus dem Nachnamen des ersten Autors und dem Jahr\n",
    "    \n",
    "    Parameter:\n",
    "    author: Autoren\n",
    "    year: Jahr\n",
    "    \n",
    "    return: Erzeugter Schlüssel\n",
    "    '''\n",
    "    \n",
    "    lastNameFirstAuthor = author.split(\" and \")[0].strip().split(\" \")[-1]\n",
    "    return f'{lastNameFirstAuthor}_{year}'\n",
    "\n",
    "\n",
    "def create_bibtex(text):\n",
    "    address = \"\"\n",
    "    author = \"\"\n",
    "    booktitle = \"\"\n",
    "    chapter = \"\"\n",
    "    doi = \"\"\n",
    "    edition = \"\"\n",
    "    editor = \"\"\n",
    "    howpublished = \"\"\n",
    "    isbn = \"\"\n",
    "    journal = \"\"\n",
    "    key = \"\"\n",
    "    month = \"\"\n",
    "    note = \"\"\n",
    "    number = \"\"\n",
    "    organization = \"\"\n",
    "    pages = \"\"\n",
    "    publisher = \"\"\n",
    "    school = \"\"\n",
    "    series = \"\"\n",
    "    title = \"\"\n",
    "    url = \"\"\n",
    "    volume = \"\"\n",
    "    year = \"\"\n",
    "    key = \"\"\n",
    "    isBook = False\n",
    "    isProceedings = False\n",
    "    isInProceedings = False\n",
    "    isIncollection = False\n",
    "    isArticle = False\n",
    "    \n",
    "    text, author = getAuthors(text)\n",
    "    text, editor = getEditors(text)\n",
    "    text, doi = getDoi(text)\n",
    "    text, url = getURL(text)\n",
    "    text, month, year = getDate(text)\n",
    "    text, page = getPage(text)\n",
    "    text, volume, number = getVolumeNumber(text)\n",
    "    text, edition = getEdition(text)\n",
    "    text, address = getAddress(text)\n",
    "    text, publisher = getPublisher(text, doi)\n",
    "    school = publisher\n",
    "    title, booktitle, series = getTitel(text)\n",
    "    journal = booktitle\n",
    "    if author != \"\":\n",
    "        key = getKey(author, year)\n",
    "    else:\n",
    "        key = getKey(editor, year)\n",
    "    \n",
    "    bookFields = [author, title, publisher, year, volume, number, \\\n",
    "                  series, address, edition, month, note, key, editor, \\\n",
    "                  howpublished, organization, chapter, pages, isbn, url]\n",
    "    inproceedingsFields = [author, title, booktitle, year, editor, volume, \\\n",
    "                            number, series, pages, address, month, organization, \\\n",
    "                            publisher, note, key, doi, url]\n",
    "    proceedingsFields = [title, year, editor, volume, number, series, \\\n",
    "                          address, month, organization, publisher, note, key, doi, url]\n",
    "    incollectionFields = [author, title, booktitle, publisher, year, editor, \\\n",
    "                           volume, number, series, chapter, pages, address, \\\n",
    "                           edition, month, note, key, doi, url]\n",
    "    articleFields = [author, title, journal, year, volume, number, \\\n",
    "                      pages, month, note, key, doi, url]\n",
    "    phdthesisFields = [author, title, publisher, year, address, month, \\\n",
    "                        note, key, doi, url]\n",
    "    \n",
    "    bookFieldsString = [\"author\", \"title\", \"publisher\", \"year\", \"volume\", \"number\", \\\n",
    "                  \"series\", \"address\", \"edition\", \"month\", \"note\", \"key\", \"editor\", \\\n",
    "                  \"howpublished\", \"organization\", \"chapter\", \"pages\", \"isbn\", \"url\"]\n",
    "    inproceedingsFieldsString  = [\"author\", \"title\", \"booktitle\", \"year\", \"editor\", \"volume\", \\\n",
    "                            \"number\", \"series\", \"pages\", \"address\", \"month\", \"organization\", \\\n",
    "                            \"publisher\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    proceedingsFieldsString  = [\"title\", \"year\", \"editor\", \"volume\", \"number\", \"series\", \\\n",
    "                          \"address\", \"month\", \"organization\", \"publisher\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    incollectionFieldsString  = [\"author\", \"title\", \"booktitle\", \"publisher\", \"year\", \"editor\", \\\n",
    "                           \"volume\", \"number\", \"series\", \"chapter\", \"pages\", \"address\", \\\n",
    "                           \"edition\", \"month\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    articleFieldsString  = [\"author\", \"title\", \"journal\", \"year\", \"volume\", \"number\", \\\n",
    "                      \"pages\", \"month\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    phdthesisFieldsString  = [\"author\", \"title\", \"school\", \"year\", \"address\", \"month\", \\\n",
    "                        \"note\", \"key\", \"doi\", \"url\"]\n",
    "\n",
    "    model = \"LaLaf93/LiteratureTyp_recognizer\"\n",
    "    classifier = pipeline(\"text-classification\", model=model)\n",
    "    literatureType = classifier(title + \".\" + booktitle)[0]['label']\n",
    "    \n",
    "    bibTex = \"@\"\n",
    "    if literatureType == \"book\":\n",
    "        zippedFieldsValues = zip(bookFieldsString, bookFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"book{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"proceedings\":\n",
    "        zippedFieldsValues = zip(proceedingsFieldsString, proceedingsFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"proceedings{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"inproceedings\":\n",
    "        zippedFieldsValues = zip(inproceedingsFieldsString, inproceedingsFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"inproceedings{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"incollection\":\n",
    "        zippedFieldsValues = zip(incollectionFieldsString, incollectionFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"incollection{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"article\":\n",
    "        zippedFieldsValues = zip(articleFieldsString, articleFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"article{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    else:\n",
    "        zippedFieldsValues = zip(phdthesisFieldsString, phdthesisFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"phdthesis{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    \n",
    "    bibTex += '}'\n",
    "\n",
    "    return bibTex \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a1358ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@inproceedings{Falk_2024, \n",
      "author={Neele Falk and Sara Papi and Mike Zhang},\n",
      "title={The Impact of Integration Step on Integrated Gradients},\n",
      "booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop},\n",
      "year={2024},\n",
      "editor={},\n",
      "volume={2024},\n",
      "number={},\n",
      "series={},\n",
      "pages={},\n",
      "address={St. Julian's, Malta},\n",
      "month={March},\n",
      "organization={},\n",
      "publisher={Association for Computational Linguistics},\n",
      "note={},\n",
      "key={Falk_2024},\n",
      "doi={},\n",
      "url={https://aclanthology.org/2024.eacl-srw.0},\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#text=\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "#text = \"M. A. Nielsen and I. L. Chuang, “Quantum Computation and Quantum Information,” in Handbook of Quantum Information Science, vol. 4, C. H. Bennett and D. P. DiVincenzo, Eds. Berlin, Germany: Springer, 2026, pp. 250–300. doi: 10.1007/springerreference-303198.\"\n",
    "#text = \"Nielsen, M. A.; Chuang, I. L. Quantum Computation and Quantum Information. In Handbook of Quantum Information Science; Bennett, C. H., DiVincenzo, D. P., Eds.; Quantum Science and Technology; Springer: Berlin, Germany, 2026; Vol. 4, pp 250–300. https://doi.org/10.1007/springerreference-303198.\"\n",
    "\n",
    "#BUG: startIndexReplace={-1} ist hier bei getYear! Deswegen doppelter String drin\n",
    "#text = \"\"\"Alahmed, Y., Abadla, R., Badri, A. A., & Ameen, N. (2023). “How Does ChatGPT Work” Examining Functionality, To The Creative AI CHATGPT on X’s (Twitter) Platform. 2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS), 1–7. https://doi.org/10.1109/SNAMS60348.2023.10375450\"\"\"\n",
    "#text = \"David Mertz, Regular Expression Puzzles and AI Coding Assistants: 24 puzzles solved by the author, with and without assistance from Copilot, ChatGPT and more , Manning, 2023.\"\n",
    "#text = \"\"\"Mohammed Baziyad, Ibrahim Kamel, and Tamer Rabie. 2023. On the Linguistic Limitations of ChatGPT: An Experimental Case Study. In 2023 International Symposium on Networks, Computers and Communications (ISNCC), 1–6. DOI:https://doi.org/10.1109/ISNCC58260.2023.10323661\"\"\"\n",
    "#text = \"\"\"K. M. Caramancion, \"Harnessing the Power of ChatGPT to Decimate Mis/Disinformation: Using ChatGPT for Fake News Detection,\" 2023 IEEE World AI IoT Congress (AIIoT), Seattle, WA, USA, 2023, pp. 0042-0046, doi: 10.1109/AIIoT58121.2023.10174450.\"\"\"\n",
    "#text = \"\"\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence? In: J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\"\"\"\n",
    "#text = \"\"\"A. Einstein, B. Podolsky, and N. Rosen, “Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?,” Physical Review, vol. 47, no. 10, pp. 777–780, May 1935, doi: 10.1103/PhysRev.47.777.\"\"\"\n",
    "#text = \"\"\"Badaro, G., Saeed, M., & Papotti, P. (2023). Transformers for tabular data representation: a survey of models and applications. Transactions of the Association for Computational Linguistics, 11, pp. 227–249. URL: https://aclanthology.org/2023.tacl-1.14, doi:10.1162/tacl_a_00544\"\"\"\n",
    "#text = \"\"\"Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics 11, (2023), 1–17. URL: https://aclanthology.org/2023.tacl-1.1, doi:10.1162/tacl_a_00530\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. (2017). Joint prediction of word alignment with alignment types. Transactions of the Association for Computational Linguistics, 5, pp. 501–514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. 2017. \"Joint Prediction of Word Alignment with Alignment Types.\" Transactions of the Association for Computational Linguistics, 5: 501–514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076.\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. 2017. ‘Joint Prediction of Word Alignment with Alignment Types.\" Transactions of the Association forComputational Linguistics, 5: 501-514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076.\"\"\"\n",
    "#text = \"\"\"Devika K, Hariprasath .s.b, Haripriya B, Vigneshwar E, Premjith B, and Bharathi Raja Chakravarthi. From dataset to detection: a comprehensive approach to combating Malayalam fake news. In Bharathi Raja Chakravarthi, Ruba Priyadharshini, Anand Kumar Madasamy, Sajeetha Thavareesan, Elizabeth Sherly, Rajeswari Nadarajan, and Manikandan Ravikiran, editors, Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 16–23, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL: https://aclanthology.org/2024.dravidianlangtech-1.3.\"\"\"\n",
    "#text = \"\"\"R, Jairam, G, Jyothish, and B, Premjith. \"A few-shot multi-accented speech classification for Indian languages using transformers and LLM's fine-tuning approaches.\" Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages. Eds. Chakravarthi, Bharathi Raja, Priyadharshini, Ruba, Madasamy, Anand Kumar, Thavareesan, Sajeetha, Sherly, Elizabeth, Nadarajan, Rajeswari, and Ravikiran, Manikandan. St. Julian's, Malta: Association for Computational Linguistics, 2024. 1–9. URL: https://aclanthology.org/2024.dravidianlangtech-1.1\"\"\"\n",
    "#text = \"\"\"Rozovskaya, Alla, Roth, Dan, and Sammons, Mark. \"Adapting to learner errors with minimal supervision.\" Computational Linguistics 43.4 (2017): 723–760. https://aclanthology.org/J17-4002, doi:10.1162/COLI_a_00299\"\"\"\n",
    "#text = \"\"\"Eds. Alonso, Jose M., and Catala, Alejandro. Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019) (Vol. 1, Issue 5, pp. 100–120). 2019. URL: https://aclanthology.org/W19-8400\"\"\"\n",
    "#text = \"\"\"Hawking, S., Greene, B., Trump, M., & Soy, S. (2025). Advanced Concepts in Theoretical Physics (3rd ed., Vol. 1). Cambridge University Press. https://doi.org/10.1017/example.book.2025\"\"\"\n",
    "#text = \"\"\"LeCun, Yann, et al. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE, vol. 86, no. 11, IEEE, 1998, pp. 2278–324, doi:10.1109/5.726791.\"\"\"\n",
    "#text = \"\"\"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, IEEE, New York, USA, 2278–2324. DOI:https://doi.org/10.1109/5.726791\"\"\"\n",
    "#text = \"\"\"Stuart Russell and Peter Norvig. 2016. Artificial Intelligence: A Modern Approach (3rd ed.). Pearson, Upper Saddle River, NJ, USA.\"\"\"\n",
    "text = \"\"\"Neele Falk, Sara Papi, and Mike Zhang. 2024. The Impact of Integration Step on Integrated Gradients. Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop. Association for Computational Linguistics, St. Julian's, Malta, (March 2024). URL: https://aclanthology.org/2024.eacl-srw.0\"\"\"\n",
    "print(create_bibtex(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11af99c",
   "metadata": {},
   "source": [
    "Idee: Nun zunächst Jahr, Volume, Seiten, Edition und URL/DOI extrahieren. Den Rest (also Titel, Publisher, Series), dann nochmal den SpacyParer drüber laufen lassen, weil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ad04c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f057d7b27f53478d93323010b68a074f-0\" class=\"displacy\" width=\"3900\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Harnessing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Power</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">ChatGPT</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Decimate</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Mis/</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">Disinformation:</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">Using</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">ChatGPT</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">Fake</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">News</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">Detection,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">2023</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">IEEE</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">World</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">AI</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">IoT</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">Congress (</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">AIIoT)</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-0\" stroke-width=\"2px\" d=\"M245,439.5 C245,352.0 380.0,352.0 380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,441.5 L237,429.5 253,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-1\" stroke-width=\"2px\" d=\"M70,439.5 C70,264.5 385.0,264.5 385.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M385.0,441.5 L393.0,429.5 377.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-2\" stroke-width=\"2px\" d=\"M420,439.5 C420,352.0 555.0,352.0 555.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M555.0,441.5 L563.0,429.5 547.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-3\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M730.0,441.5 L738.0,429.5 722.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-4\" stroke-width=\"2px\" d=\"M70,439.5 C70,177.0 915.0,177.0 915.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,441.5 L923.0,429.5 907.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-5\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,264.5 1435.0,264.5 1435.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-6\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,352.0 1430.0,352.0 1430.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,441.5 L1287,429.5 1303,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-7\" stroke-width=\"2px\" d=\"M945,439.5 C945,177.0 1440.0,177.0 1440.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,441.5 L1448.0,429.5 1432.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-8\" stroke-width=\"2px\" d=\"M70,439.5 C70,89.5 1620.0,89.5 1620.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1620.0,441.5 L1628.0,429.5 1612.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-9\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,352.0 1780.0,352.0 1780.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1780.0,441.5 L1788.0,429.5 1772.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-10\" stroke-width=\"2px\" d=\"M1820,439.5 C1820,352.0 1955.0,352.0 1955.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1955.0,441.5 L1963.0,429.5 1947.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-11\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,264.5 2485.0,264.5 2485.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,441.5 L2162,429.5 2178,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-12\" stroke-width=\"2px\" d=\"M2345,439.5 C2345,352.0 2480.0,352.0 2480.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2345,441.5 L2337,429.5 2353,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-13\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,177.0 2490.0,177.0 2490.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2490.0,441.5 L2498.0,429.5 2482.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-14\" stroke-width=\"2px\" d=\"M2695,439.5 C2695,89.5 3545.0,89.5 3545.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2695,441.5 L2687,429.5 2703,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-15\" stroke-width=\"2px\" d=\"M2870,439.5 C2870,177.0 3540.0,177.0 3540.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,441.5 L2862,429.5 2878,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-16\" stroke-width=\"2px\" d=\"M3045,439.5 C3045,352.0 3180.0,352.0 3180.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3045,441.5 L3037,429.5 3053,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-17\" stroke-width=\"2px\" d=\"M3220,439.5 C3220,264.5 3535.0,264.5 3535.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3220,441.5 L3212,429.5 3228,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-18\" stroke-width=\"2px\" d=\"M3395,439.5 C3395,352.0 3530.0,352.0 3530.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3395,441.5 L3387,429.5 3403,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-19\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,2.0 3550.0,2.0 3550.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3550.0,441.5 L3558.0,429.5 3542.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f057d7b27f53478d93323010b68a074f-0-20\" stroke-width=\"2px\" d=\"M3570,439.5 C3570,352.0 3705.0,352.0 3705.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f057d7b27f53478d93323010b68a074f-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3705.0,441.5 L3713.0,429.5 3697.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Harnessing, POS: VERB, Head: Harnessing, Dep: ROOT\n",
      "Text: the, POS: DET, Head: Power, Dep: det\n",
      "Text: Power, POS: PROPN, Head: Harnessing, Dep: dobj\n",
      "Text: of, POS: ADP, Head: Power, Dep: prep\n",
      "Text: ChatGPT, POS: PROPN, Head: of, Dep: pobj\n",
      "Text: to, POS: ADP, Head: Harnessing, Dep: prep\n",
      "Text: Decimate, POS: PROPN, Head: Disinformation, Dep: compound\n",
      "Text: Mis, POS: PROPN, Head: Disinformation, Dep: nmod\n",
      "Text: /, POS: SYM, Head: Disinformation, Dep: punct\n",
      "Text: Disinformation, POS: PROPN, Head: to, Dep: pobj\n",
      "Text: :, POS: PUNCT, Head: Harnessing, Dep: punct\n",
      "Text: Using, POS: VERB, Head: Harnessing, Dep: xcomp\n",
      "Text: ChatGPT, POS: NOUN, Head: Using, Dep: dobj\n",
      "Text: for, POS: ADP, Head: ChatGPT, Dep: prep\n",
      "Text: Fake, POS: PROPN, Head: Detection, Dep: compound\n",
      "Text: News, POS: PROPN, Head: Detection, Dep: compound\n",
      "Text: Detection, POS: PROPN, Head: for, Dep: pobj\n",
      "Text: ,, POS: PUNCT, Head: Using, Dep: punct\n",
      "Text: 2023, POS: NUM, Head: Congress, Dep: nummod\n",
      "Text: IEEE, POS: ADJ, Head: Congress, Dep: compound\n",
      "Text: World, POS: PROPN, Head: AI, Dep: compound\n",
      "Text: AI, POS: PROPN, Head: Congress, Dep: compound\n",
      "Text: IoT, POS: PROPN, Head: Congress, Dep: compound\n",
      "Text: Congress, POS: PROPN, Head: Using, Dep: npadvmod\n",
      "Text: (, POS: PUNCT, Head: Congress, Dep: punct\n",
      "Text: AIIoT, POS: PROPN, Head: Congress, Dep: appos\n",
      "Text: ), POS: PUNCT, Head: Congress, Dep: punct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score                   word  start  end\n",
      "0          ORG  0.879166                   IEEE    108  112\n",
      "1         MISC  0.753767  World AI IoT Congress    113  134\n",
      "2          ORG  0.621831                     AI    136  138\n",
      "3         MISC  0.473681                   ##Io    138  140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-ORG', 'score': 0.87916553, 'index': 36, 'word': 'IEEE', 'start': 108, 'end': 112}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9137714, 'index': 37, 'word': 'World', 'start': 113, 'end': 118}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.52765363, 'index': 38, 'word': 'AI', 'start': 119, 'end': 121}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.7182073, 'index': 39, 'word': 'I', 'start': 122, 'end': 123}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.6421149, 'index': 40, 'word': '##o', 'start': 123, 'end': 124}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.91405857, 'index': 41, 'word': '##T', 'start': 124, 'end': 125}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.80679643, 'index': 42, 'word': 'Congress', 'start': 126, 'end': 134}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-ORG', 'score': 0.621831, 'index': 44, 'word': 'AI', 'start': 136, 'end': 138}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.48083907, 'index': 45, 'word': '##I', 'start': 138, 'end': 139}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.46652368, 'index': 46, 'word': '##o', 'start': 139, 'end': 140}\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Test, um Datum Zuverlässig erkannt wird\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "\n",
    "text = \"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence, In Proceedings of the IEEE International Conference on Neural Networks, Physical Review.\"\n",
    "text = \"\"\"\"What Drives IT Students Toward ChatGPT? Analyzing the Factors Influencing Students' Intention to Use ChatGPT for Educational Purposes,\" 2024 21st International Multi-Conference on Systems, Signals & Devices (SSD)\"\"\"\n",
    "text = \"\"\"K. M. Caramancion, \"Harnessing the Power of ChatGPT to Decimate Mis/Disinformation: Using ChatGPT for Fake News Detection,\" 2023 IEEE World AI IoT Congress (AIIoT), Seattle, WA, USA, 2023, pp. 0042-0046, doi: 10.1109/AIIoT58121.2023.10174450.\"\"\"\n",
    "text = \"\"\". \"Harnessing the Power of ChatGPT to Decimate Mis/Disinformation: Using ChatGPT for Fake News Detection,\" 2023 IEEE World AI IoT Congress (AIIoT), Seattle, WA, USA,  \"\"\"\n",
    "text = \"\"\"Harnessing the Power of ChatGPT to Decimate Mis/Disinformation: Using ChatGPT for Fake News Detection, 2023 IEEE World AI IoT Congress (AIIoT)\"\"\"\n",
    "#text = \"dsfsdf London\"\n",
    "print(len(text))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "# Visualisierung der Abhängigkeiten\n",
    "displacy.render(doc, style='dep', jupyter=True)\n",
    "#for token in doc:\n",
    "#    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)\n",
    "#for ent in doc.ents:\n",
    "#    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Text: {token.text}, POS: {token.pos_}, Head: {token.head.text}, Dep: {token.dep_}')\n",
    "\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "print(df_outputs)\n",
    "pos_pipeline = pipeline(\"token-classification\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "results = pos_pipeline(text)\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "    #print(f\"Word: {result['word']}, POS Tag: {result['entity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "690d3437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: T.R. Fernandez Perez Tomei\n",
      "Affiliation: Researcher IV, Universidade Estadual Paulista\n",
      "\n",
      "Title: Observation of a new boson at a mass of 125 GeV with the CMS experiment at the LHC\n",
      "Authors: Serguei Chatrchyan and Vardan Khachatryan and Albert M Sirunyan and Armen Tumasyan and Wolfgang Adam and Ernest Aguilo and Thomas Bergauer and Marko Dragicevic and Janos Erö and Christian Fabjan and M Friedl and Rudolf Frühwirth and VM Ghete and J Hammer and M Hoch and N Hörmann and J Hrubec and M Jeitler and W Kiesenhofer and V Knünz and M Krammer and I Krätschmer and D Liko and W Majerotto and I Mikulec and M Pernicka and B Rahbaran and C Rohringer and H Rohringer and R Schöfbeck and J Strauss and F Szoncsó and A Taurok and W Waltenberger and G Walzel and E Widl and C-E Wulz and I Emeliantchik and V Makarenko and N Shumeiko and A Solin and R Stefanovitch and J Suarez Gonzalez and A Fedorov and M Korzhik and O Missevitch and R Zuyeuski and M Bansal and S Bansal and W Beaumont and Tom Cornelis and EA De Wolf and D Druzhkin and X Janssen and S Luyckx and L Mucibello and S Ochesanu and B Roland and R Rougny and M Selvaggi and Z Staykova and H Van Haevermaet and P Van Mechelen and A Van Spilbeeck and F Blekman and S Blyweert and J DʼHondt and O Devroede and R Gonzalez Suarez and R Goorens and A Kalogeropoulos and M Maes and A Olbrechts and S Tavernier and W Van Doninck and L Van Lancker and P Van Mulders and GP Van Onsem and I Villella and B Clerbaux and G De Lentdecker and V Dero and JP Dewulf and APR Gay and T Hreus and A Léonard and PE Marage and A Mohammadi and T Reis and S Rugovac and L Thomas and C Vander Velde and P Vanlaer and J Wang and J Wickens and Volker Adler and Kelly Beernaert and Anna Cimmino and Silvia Costantini and Guillaume Garcia and M Grunewald and Benjamin Klein and Jeremie Lellouch and Andrey Marinov and Joseph McCartin and AA Ocampo Rios and Dirk Ryckbosch and Nadja Strobbe and Filip Thyssen and Michael Tytgat and S Walsh and Efe Yazgan and Nikolaos Zaganidis and S Basegmez and G Bruno and R Castello and L Ceard and J De Favereau De Jeneret and C Delaere and P Demin and T du Pree and D Favart and L Forthomme and A Giammanco and G Grégoire and J Hollar and V Lemaitre and J Liao and O Militaru and C Nuttens and D Pagano and A Pin and K Piotrzkowski and N Schul and JM Vizan Garcia and N Beliy and T Caebergs and E Daubie and GH Hammad and GA Alves and L Brito and M Correa Martin Junior and T Martins and ME Pol and MHG Souza and WL Aldá Júnior and W Carvalho and A Custódio and EM Da Costa and D De Jesus Damiao\n",
      "Year: 2012\n",
      "\n",
      "Title: Global, regional, and national incidence, prevalence, and years lived with disability for 328 diseases and injuries for 195 countries, 1990–2016: a systematic analysis for the Global Burden of Disease Study 2016\n",
      "Authors: Theo Vos and Amanuel Alemu Abajobir and Kalkidan Hassen Abate and Cristiana Abbafati and Kaja M Abbas and Foad Abd-Allah and Rizwan Suliankatchi Abdulkader and Abdishakur M Abdulle and Teshome Abuka Abebo and Semaw Ferede Abera and Victor Aboyans and Laith J Abu-Raddad and Ilana N Ackerman and Abdu Abdullahi Adamu and Olatunji Adetokunboh and Mohsen Afarideh and Ashkan Afshin and Sanjay Kumar Agarwal and Rakesh Aggarwal and Anurag Agrawal and Sutapa Agrawal and Hamid Ahmadieh and Muktar Beshir Ahmed and Miloud Taki Eddine Aichour and Amani Nidhal Aichour and Ibtihel Aichour and Sneha Aiyar and Rufus Olusola Akinyemi and Nadia Akseer and Faris Hasan Al Lami and Fares Alahdab and Ziyad Al-Aly and Khurshid Alam and Noore Alam and Tahiya Alam and Deena Alasfoor and Kefyalew Addis Alene and Raghib Ali and Reza Alizadeh-Navaei and Ala'a Alkerwi and François Alla and Peter Allebeck and Christine Allen and Fatma Al-Maskari and Rajaa Al-Raddadi and Ubai Alsharif and Shirina Alsowaidi and Khalid A Altirkawi and Azmeraw T Amare and Erfan Amini and Walid Ammar and Yaw Ampem Amoako and Hjalte H Andersen and Carl Abelardo T Antonio and Palwasha Anwari and Johan Ärnlöv and Al Artaman and Krishna Kumar Aryal and Hamid Asayesh and Solomon W Asgedom and Reza Assadi and Tesfay Mehari Atey and Niguse Tadele Atnafu and Sachin R Atre and Leticia Avila-Burgos and Euripide Frinel G Arthur Avokphako and Ashish Awasthi and Umar Bacha and Alaa Badawi and Kalpana Balakrishnan and Amitava Banerjee and Marlena S Bannick and Aleksandra Barac and Ryan M Barber and Suzanne L Barker-Collo and Till Bärnighausen and Simon Barquera and Lars Barregard and Lope H Barrero and Sanjay Basu and Bob Battista and Katherine E Battle and Bernhard T Baune and Shahrzad Bazargan-Hejazi and Justin Beardsley and Neeraj Bedi and Ettore Beghi and Yannick Béjot and Bayu Begashaw Bekele and Michelle L Bell and Derrick A Bennett and Isabela M Bensenor and Jennifer Benson and Adugnaw Berhane and Derbew Fikadu Berhe and Eduardo Bernabé and Balem Demtsu Betsu and Mircea Beuran and Addisu Shunu Beyene and Neeraj Bhala and Anil Bhansali and Samir Bhatt and Zulfiqar A Bhutta and Sibhatu Biadgilign and Burcu Kucuk Bicer and Kelly Bienhoff and Boris Bikbov and Charles Birungi and Stan Biryukov and Donal Bisanzio and Habtamu Mellie Bizuayehu and Dube Jara Boneya and Soufiane Boufous and Rupert RA Bourne and Alexandra Brazinova and Traolach S Brugha and Rachelle Buchbinder and Lemma Negesa Bulto Bulto and Blair R Bumgarner and Zahid A Butt and Lucero Cahuana-Hurtado and Ewan Cameron and Mate Car and Hélène Carabin and Jonathan R Carapetis and Rosario Cárdenas and David O Carpenter and Juan Jesus Carrero and Austin Carter and Felix Carvalho and Daniel C Casey and Valeria Caso and Carlos A Castañeda-Orjuela and Chris D Castle and Ferrán Catalá-López and Hsing-Yi Chang and Jung-Chen Chang and Fiona J Charlson and Honglei Chen and Mirriam Chibalabala and Chioma Ezinne Chibueze and Vesper Hichilombwe Chisumpa and Abdulaal A Chitheer and Devasahayam Jesudas Christopher and Liliana G Ciobanu and Massimo Cirillo and Danny Colombara and Cyrus Cooper and Paolo Angelo Cortesi and Michael H Criqui\n",
      "Year: 2017\n",
      "\n",
      "Title: Precise determination of the mass of the Higgs boson and tests of compatibility of its couplings with the standard model predictions using proton collisions at 7 and 8 TeV\n",
      "Authors: CMS collaboration\n",
      "Year: 2015\n",
      "\n",
      "Title: Event generator tunes obtained from underlying event and multiparton scattering measurements\n",
      "Authors: Vardan Khachatryan and Albert M Sirunyan and Armen Tumasyan and Wolfgang Adam and E Asilar and Thomas Bergauer and Johannes Brandstetter and Erica Brondolin and Marko Dragicevic and Janos Erö and M Friedl and R Frühwirth and VM Ghete and C Hartl and N Hörmann and J Hrubec and M Jeitler and V Knünz and A König and M Krammer and I Krätschmer and D Liko and T Matsushita and I Mikulec and D Rabady and B Rahbaran and H Rohringer and J Schieck and R Schöfbeck and J Strauss and W Treberer-Treberspurg and W Waltenberger and C-E Wulz and V Mossolov and N Shumeiko and J Suarez Gonzalez and S Alderweireldt and Tom Cornelis and EA De Wolf and X Janssen and A Knutsson and J Lauwers and S Luyckx and M Van De Klundert and H Van Haevermaet and P Van Mechelen and N Van Remortel and A Van Spilbeeck and S Abu Zeid and F Blekman and J D’Hondt and N Daci and I De Bruyn and K Deroover and N Heracleous and J Keaveney and S Lowette and L Moreels and A Olbrechts and Q Python and D Strom and S Tavernier and W Van Doninck and Petra Van Mulders and Gerrit Patrick Van Onsem and Isis Van Parijs and P Barria and H Brun and C Caillol and B Clerbaux and G De Lentdecker and G Fasanella and L Favart and A Grebenyuk and G Karapostoli and T Lenzi and A Léonard and T Maerschalk and A Marinov and L Perniè and A Randle-Conde and T Seva and C Vander Velde and R Yonamine and P Vanlaer and R Yonamine and F Zenoni and F Zhang and V Adler and K Beernaert and L Benucci and A Cimmino and S Crucy and D Dobur and A Fagot and G Garcia and M Gul and J Mccartin and AA Ocampo Rios and D Poyraz and D Ryckbosch and S Salva and M Sigamani and M Tytgat and W Van Driessche and E Yazgan and N Zaganidis and S Basegmez and C Beluffi and O Bondu and S Brochet and G Bruno and A Caudron and L Ceard and GG Da Silveira and C Delaere and D Favart and L Forthomme and A Giammanco and J Hollar and A Jafari and P Jez and M Komm and V Lemaitre and A Mertens and M Musich and C Nuttens and L Perrini and A Pin and K Piotrzkowski and A Popov and L Quertenmont and M Selvaggi and M Vidal Marono and N Beliy and GH Hammad and WL Aldá Júnior and FL Alves and GA Alves and L Brito and M Correa Martins Junior and M Hamer and C Hensel and A Moraes and ME Pol and P Rebello Teles and E Belchior Batista Das Chagas and W Carvalho and J Chinellato and A Custódio\n",
      "Year: 2016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Evidence for the 125 GeV Higgs boson decaying to a pair of τ leptons\n",
      "Authors: Serguei Chatrchyan and Vardan Khachatryan and Albert M Sirunyan and Armen Tumasyan and Wolfgang Adam and Thomas Bergauer and Marko Dragicevic and Janos Erö and Christian Fabjan and Markus Friedl and Rudolf Fruehwirth and Vasile Mihai Ghete and Christian Hartl and Natascha Hörmann and Josef Hrubec and Manfred Jeitler and Wolfgang Kiesenhofer and Valentin Knünz and Manfred Krammer and Ilse Krätschmer and Dietrich Liko and Ivan Mikulec and Dinyar Rabady and Babak Rahbaran and Herbert Rohringer and Robert Schöfbeck and Josef Strauss and Anton Taurok and Wolfgang Treberer-Treberspurg and Wolfgang Waltenberger and C-E Wulz and N Shumeiko and S Alderweireldt and M Bansal and S Bansal and T Cornelis and EA De Wolf and X Janssen and A Knutsson and S Luyckx and S Ochesanu and B Roland and R Rougny and H Van Haevermaet and P Van Mechelen and A Van Spilbeeck and F Blekman and S Blyweert and J D’Hondt and N Heracleous and A Kalogeropoulos and J Keaveney and TJ Kim and S Lowette and M Maes and A Olbrechts and D Strom and S Tavernier and W Van Doninck and P Van Mulders and GP Van Onsem and I Villella and C Caillol and B Clerbaux and G De Lentdecker and L Favart and APR Gay and A Léonard and PE Marage and A Mohammadi and L Perniè and T Reis and T Seva and L Thomas and C Vander Velde and P Vanlaer and J Wang and V Adler and K Beernaert and L Benucci and A Cimmino and S Costantini and S Crucy and S Dildick and G Garcia and B Klein and J Lellouch and J Mccartin and AA Ocampo Rios and D Ryckbosch and S Salva Diblen and M Sigamani and N Strobbe and F Thyssen and M Tytgat and S Walsh and E Yazgan and N Zaganidis and S Basegmez and C Beluffi and G Bruno and R Castello and A Caudron and L Ceard and GG Da Silveira and C Delaere and T Du Pree and D Favart and L Forthomme and A Giammanco and J Hollar and P Jez and M Komm and V Lemaitre and J Liao and O Militaru and C Nuttens and Davide Pagano and L Perrini and Arnaud Pin and Krzysztof Piotrzkowski and Andrey Popov and Loic Quertenmont and Michele Selvaggi and Miguel Vidal Marono and Nikita Beliy and Thierry Caebergs and Evelyne Daubie and Gregory Habib Hammad and GA Alves and Marcos Correa Martins Junior and T Martins and Maria Elena Pol and MHG Souza and Walter Luiz Aldá Júnior and Wagner Carvalho and Jose Chinellato and Analu Custódio and EM Da Costa and Dilson De Jesus Damiao and Carley De Oliveira Martins and Sandro Fonseca De Souza and Helena Malbouisson and Magdalena Malek and Diego Matos Figueiredo and Luiz Mundim and Helio Nogima and Wanda Lucia Prado Da Silva and Javier Santaolalla and Alberto Santoro\n",
      "Year: 2014\n"
     ]
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "\n",
    "# Suche nach einem Autor\n",
    "search_query = scholarly.search_author('Fernández, R.')\n",
    "author = next(search_query)\n",
    "\n",
    "# Laden Sie das komplette Autorenprofil\n",
    "author = scholarly.fill(author)\n",
    "\n",
    "# Zeigen Sie einige Informationen über den Autor an\n",
    "print(f\"Name: {author['name']}\")\n",
    "print(f\"Affiliation: {author['affiliation']}\")\n",
    "\n",
    "# Zeigen Sie die ersten fünf Veröffentlichungen an\n",
    "for publication in author['publications'][:5]:\n",
    "    pub = scholarly.fill(publication)\n",
    "    print(f\"\\nTitle: {pub['bib']['title']}\")\n",
    "    print(f\"Authors: {pub['bib']['author']}\")\n",
    "    print(f\"Year: {pub['bib']['pub_year']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0d9380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hallo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hallo\"[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b8a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
