{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad55d2b1",
   "metadata": {},
   "source": [
    "@book{advanced_physics, author = {Stephen Hawking and Brian Greene and Maria Trump and Susan Soy}, title = {Advanced Concepts in Theoretical Physics}, publisher = {Cambridge University Press}, year = {2025}, edition = {3}, volume = {1}, series = {Advanced Studies in Physics}, address = {Cambridge, UK}, month = {May}, note = {A seminal work in the field of theoretical physics}, isbn = {978-0-521-76948-0}, doi = {10.1017/example.book.2025}, url = {https://www.cambridge.org/advanced_physics}, annote = {Widely cited in the physics community}, abstract = {This book explores cutting-edge theories and concepts in theoretical physics...}, keywords = {Theoretical Physics, Quantum Mechanics, String Theory}, language = {English}, price = {75.00}, size = {600 pages}, lccn = {2020934576}, mrnumber = {MR3070071} }\n",
    "\n",
    "@article{quantum_entanglement,\n",
    "  author        = {Albert Einstein and Boris Podolsky and Nathan Rosen},\n",
    "  title         = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},\n",
    "  journal       = {Physical Review},\n",
    "  year          = {1935},\n",
    "  volume        = {47},\n",
    "  number        = {10},\n",
    "  pages         = {777-780},\n",
    "  month         = {May},\n",
    "  note          = {EPR Paradox paper, fundamental for quantum mechanics},\n",
    "  doi           = {10.1103/PhysRev.47.777},\n",
    "  url           = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},\n",
    "  abstract      = {In this paper, the authors discuss the EPR paradox and challenge the completeness of quantum mechanics...},\n",
    "  keywords      = {Quantum Mechanics, EPR Paradox, Physical Reality},\n",
    "  language      = {English},\n",
    "  publisher     = {American Physical Society}\n",
    "}\n",
    "\n",
    "@inproceedings{deep_learning,\n",
    "  author        = {Geoffrey Hinton and Yoshua Bengio and Yann LeCun},\n",
    "  title         = {Deep Learning for Artificial Intelligence},\n",
    "  booktitle     = {Proceedings of the IEEE International Conference on Neural Networks},\n",
    "  year          = {2021},\n",
    "  editor        = {Jane Smith and John Doe},\n",
    "  volume        = {1},\n",
    "  number = {5},\n",
    "  series        = {Advances in Neural Information Processing},\n",
    "  pages         = {100-120},\n",
    "  address       = {Montreal, Canada},\n",
    "  month         = {December},\n",
    "  organization  = {IEEE},\n",
    "  publisher     = {IEEE Press},\n",
    "  note          = {Keynote paper on recent advancements in deep learning},\n",
    "  isbn          = {978-1-5386-4637-1},\n",
    "  doi           = {10.1109/ICNN.2021.9483948},\n",
    "  url           = {https://ieeexplore.ieee.org/document/9483948},\n",
    "  annote        = {A seminal work on how deep learning transforms AI},\n",
    "  abstract      = {This paper explores cutting-edge deep learning techniques and their impact on the development of artificial intelligence...},\n",
    "  keywords      = {Deep Learning, Artificial Intelligence, Neural Networks},\n",
    "  language      = {English}\n",
    "}\n",
    "\n",
    "\n",
    "@incollection{quantum_computation,\n",
    "  author        = {Michael A. Nielsen and Isaac L. Chuang},\n",
    "  title         = {Quantum Computation and Quantum Information},\n",
    "  booktitle     = {Handbook of Quantum Information Science},\n",
    "  publisher     = {Springer},\n",
    "  year          = {2026},\n",
    "  editor        = {Charles H. Bennett and David P. DiVincenzo},\n",
    "  volume        = {4},\n",
    "  series        = {Quantum Science and Technology},\n",
    "  chapter       = {10},\n",
    "  pages         = {250-300},\n",
    "  address       = {Berlin, Germany},\n",
    "  month         = {October},\n",
    "  note          = {A comprehensive overview of the fundamentals of quantum computation},\n",
    "  isbn          = {978-3-540-88702-7},\n",
    "  doi           = {10.1007/springerreference-303198},\n",
    "  url           = {https://www.springer.com/gp/book/9783540887027},\n",
    "  annote        = {Essential reading for researchers entering the field of quantum information},\n",
    "  abstract      = {This chapter delves into the principles of quantum computing, offering an accessible yet thorough introduction...},\n",
    "  keywords      = {Quantum Computing, Quantum Information, Computational Models},\n",
    "  language      = {English},\n",
    "  price         = {45.00},\n",
    "  size          = {50 pages}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8acde04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "torch.cuda.is_available()\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import requests\n",
    "\n",
    "def custom_strip(text, replaceCharacter = []):\n",
    "    allowed_chars = string.punctuation + string.whitespace\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return text.strip(allowed_chars)\n",
    "\n",
    "\n",
    "def getIndexOfSubstring(text, regEx = [], reverse = False):\n",
    "    #if reverse = False then it finds the first occurance of a given regEx.\n",
    "    #if reverse = True, then it finds the last occurance of a given regEx.\n",
    "    #beceause the occurance with the max length is taken, it always chooses the regex that covers the most letters\n",
    "    length = 0\n",
    "    matches = []\n",
    "    substring = \"\"\n",
    "    for regExElement in regEx:\n",
    "        matches = list(re.finditer(regExElement, text))\n",
    "        if matches:\n",
    "            if reverse:\n",
    "                match = matches[-1]\n",
    "            else:\n",
    "                match = matches[0]\n",
    "            buffer = match.end() - match.start()\n",
    "            if buffer > length:\n",
    "                length = buffer\n",
    "                startIndex = match.start()\n",
    "                endIndex = match.end()\n",
    "                substring = text[match.start():match.end()]\n",
    "    if substring != \"\":\n",
    "        return startIndex, endIndex, substring   \n",
    "    return -1, -1, substring\n",
    "\n",
    "def is_SurenameFirst(names):\n",
    "    splitedNames = names.split(\" \")\n",
    "    #regex wie w+ erkennt bspw. KEIN è \n",
    "    if splitedNames[0].endswith(\".\") or not isSpeceficPunctuation(splitedNames[0][-1], []):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def is_NameShortened(df_PER):\n",
    "    for index in df_PER.index.values.tolist():\n",
    "        if \".\" == text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def isSpeceficPunctuation(text, replaceCharacter = []):\n",
    "    allowed_chars = string.punctuation + string.whitespace\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return all(char in allowed_chars for char in text)\n",
    "\n",
    "def is_Editor(editorRegEx, textBetweenNames, index, text):\n",
    "    if re.search(editorRegEx, textBetweenNames):\n",
    "        x = re.search(editorRegEx, text)\n",
    "        #print(f'x: {x.start()}')\n",
    "        if isSpeceficPunctuation(text[index:x.start()], [\"&\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def processNames(authors):\n",
    "    finalAuthors = \"\"\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "    #andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "    surenameFirst = is_SurenameFirst(authors.strip())\n",
    "    print(f'function processNames, surenameFirst: {surenameFirst}')\n",
    "    if surenameFirst:\n",
    "        startIndex, endIndex, andInAuthors = getIndexOfSubstring(authors, search_terms)\n",
    "        print(\"Fall surenameFirst\".format(authors))\n",
    "        #hier völlig egal, ob er einzelne Initialen in ein eigenes Word gesteckt hat, obwohl es noch Nachnamen gib\n",
    "        if startIndex >= 0:\n",
    "            authors = authors.replace(andInAuthors, \" and \")\n",
    "            #print(f'authors: {authors}')\n",
    "            finalAuthors = authors.replace(\", \", \" and \")\n",
    "        else:\n",
    "            finalAuthors = authors\n",
    "    elif \"., \" in authors:\n",
    "        print(\"Fall ., {0}\".format(authors))\n",
    "        search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        if andInAuthors != \"\":\n",
    "            authors = authors.replace(andInAuthors, \"., \")\n",
    "        authors = authors.split(\"., \")\n",
    "        authors = [name + \".\" for name in authors]\n",
    "        authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "        for author in authors[:-1]:\n",
    "            buffer = author.split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "        buffer = authors[-1].split(\", \")\n",
    "        finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "    elif \".; \" in authors:\n",
    "        print(\"Fall .; {0}\".format(authors))\n",
    "        search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        if andInAuthors != \"\":\n",
    "            authors = authors.replace(andInAuthors, \".; \")\n",
    "        authors = authors.split(\".; \")\n",
    "        authors = [name + \".\" for name in authors]\n",
    "        authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "        for author in authors[:-1]:\n",
    "            buffer = author.split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "        buffer = authors[-1].split(\", \")\n",
    "        finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "    elif \", \" in authors:\n",
    "        print(\"Fall , {0}\".format(authors))\n",
    "        search_terms = [\", and \", \", & \", \" and \", \" & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        if andInAuthors != \"\":  \n",
    "            authors = authors.replace(andInAuthors, \", \")\n",
    "        authors = authors.split(\", \")\n",
    "        for i in range(0, len(authors) - 3, 2):\n",
    "            finalAuthors = finalAuthors + authors[i+1] + \" \" + authors[i] + \" and \"\n",
    "        finalAuthors = finalAuthors + authors[len(authors) - 1] + \" \" + authors[len(authors) - 2]\n",
    "    return custom_strip(finalAuthors)\n",
    "\n",
    "def getAuthors(text):\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    \n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    authorsDetected = False\n",
    "    setChainStart = True\n",
    "    startIndexAuthors = -1\n",
    "    endIndexAuthors = -1\n",
    "    chainStartIndex = -1\n",
    "    changedText = \"\"\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    \n",
    "    for index in index_df_PER_List:\n",
    "        #beachte: Hiermit lese ich immer schon vor!\n",
    "        if index < len(index_df_PER_List) - 1:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "        else:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "        onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "        firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "        onlyAnd = textBetweenNames == andTyp\n",
    "        if setChainStart: \n",
    "            chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "            setChainStart = False\n",
    "        if not onlyPunctuation and not onlyAnd:\n",
    "            setChainStart = True\n",
    "            startIndexAuthors = chainStartIndex\n",
    "            endIndexAuthors = df_PER[\"end\"].iloc[index]\n",
    "            break\n",
    "    if startIndexAuthors > -1:\n",
    "        changedText, author = replaceSubstring(startIndexAuthors, endIndexAuthors, text, \".\")\n",
    "        author = processNames(author)\n",
    "        return changedText, author\n",
    "    return text, \"\"\n",
    "\n",
    "def getEditors(text):\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    editorRegEx = \"(\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\"\n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    setChainStart = True\n",
    "    isEditor = False\n",
    "    startIndexEditors = -1\n",
    "    endIndexEditors = -1\n",
    "    chainStartIndex = -1\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "\n",
    "    for index in index_df_PER_List:\n",
    "        if index < len(index_df_PER_List) - 1:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "        else:\n",
    "            textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "        onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "        firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "        onlyAnd = textBetweenNames == andTyp\n",
    "        #wenn true, dann beginnt eine neue Autorenkette\n",
    "        if setChainStart: \n",
    "            chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "            #Solange das auf False, sollen der Substring erweitert werden, also start bleibt konstant\n",
    "            setChainStart = False\n",
    "        #Dann ist die Autorenkette zu Ende\n",
    "        if not onlyPunctuation and not onlyAnd:\n",
    "            setChainStart = True\n",
    "            #Es können auch nur Editoren und keine Autoren vorkommen\n",
    "            isEditor = is_Editor(editorRegEx, textBetweenNames, df_PER[\"end\"].iloc[index], text)\n",
    "            if isEditor:\n",
    "                startIndexEditors = chainStartIndex\n",
    "                endIndexEditors = df_PER[\"end\"].iloc[index]\n",
    "                break\n",
    "    #print(f'getAuthorsAndEditors: return: {[startIndexAuthors,endIndexAuthors],[startIndexEditors, endIndexEditors]}')\n",
    "    if startIndexEditors > -1:\n",
    "        changedText, editor = replaceSubstring(startIndexEditors, endIndexEditors, text, \"#EDITOR#\")\n",
    "        editor = processNames(editor)\n",
    "        print(f'text after replace editors : {changedText}')\n",
    "        #es soll erst ab Editors gesucht werden, daher text[endIndexEditors:]. Sonst Verwechslungsgefahr\n",
    "        startIndexEditors, endIndexEditors, buffer = getIndexOfSubstring(changedText, [\"#EDITOR#\"])\n",
    "        startIndexEditorMarker, endIndexEditorMarker, buffer = getIndexOfSubstring(changedText[endIndexEditors:], [editorRegEx])\n",
    "        startIndexEditorMarker = startIndexEditorMarker + endIndexEditors\n",
    "        endIndexEditorMarker = endIndexEditorMarker + endIndexEditors\n",
    "        changedText, replacedEditorMarker = replaceSubstring(startIndexEditorMarker, endIndexEditorMarker, changedText, \".\")\n",
    "        print(f'text after replace editorsMarker : {changedText}')\n",
    "        startIndexEditors, endIndexEditors, buffer = getIndexOfSubstring(changedText, [\"#EDITOR#\"])\n",
    "        startIndexIn = 0 \n",
    "        for i in range(startIndexEditors-1, -1, -1):\n",
    "            if isSpeceficPunctuation(changedText[i], [\":\", \" \"]):\n",
    "                startIndexIn = i + 1\n",
    "                break\n",
    "        print(f' getEditors, startIndexIn : {startIndexIn}')\n",
    "        print(f' getEditors, startIndexEditors : {startIndexEditors}')\n",
    "        changedText, replacedEditorMarker = replaceSubstring(startIndexIn, startIndexEditors, changedText, \".\")\n",
    "        \n",
    "        # #.# tritt auf, weil ich # nicht ignoriere\n",
    "        startIndexEditor, endIndexEditor, buffer = getIndexOfSubstring(changedText, [\"#EDITOR#\"])\n",
    "        changedText, replacedEditorMarker = replaceSubstring(startIndexEditor, endIndexEditor, changedText, \".\")\n",
    "        return changedText, editor\n",
    "    return text, \"\"\n",
    "\n",
    "def getPublisher(text, doi):\n",
    "    publisher = \"\"\n",
    "    if doi != \"\":\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            publisher = data['message'].get('publisher', 'Publisher not found')\n",
    "    if publisher != \"\":\n",
    "        startIndex, endIndex, publisher = getIndexOfSubstring(text, [publisher], True)\n",
    "        #double check\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, publisher\n",
    "        changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        if publisher != \"\":\n",
    "            return changedText, custom_strip(publisher)\n",
    "    df_ORG = getORGTag(text, 0.8)\n",
    "    if not df_ORG.empty:\n",
    "        startIndex = df_ORG[\"start\"].iloc[0]\n",
    "        endIndex = df_ORG[\"end\"].iloc[0]\n",
    "        publisher = text[startIndex:endIndex]\n",
    "        #If the range determined by the tagger corresponds to a string \n",
    "        #that is only delimited by punctuation before and after, then it is most likely a publisher.\n",
    "        #startIndex - 2 because of a space inbetween\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "    return text, \"\"\n",
    "            \n",
    "\n",
    "\n",
    "def replaceSubstring (startIndex, endIndex, text, substituteString, ignorePunctuation = [\"&\", \"(\", \")\"]):\n",
    "    #The regex also checks for punctuation so that it is particularly precise. \n",
    "    #The cut text however should in normale mode be without the front delimiter of the bibTex fields in the bibiography, \n",
    "    #so that future regex are not affected. But the last delimiter belongs to the cut word so this should be removed\n",
    "    if endIndex > 0:\n",
    "        startIndexReplace = 0\n",
    "        endIndexReplace = 0\n",
    "        if startIndex > 0:\n",
    "            for i in range(startIndex, -1, -1):\n",
    "                if isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    startIndexReplace = i + 1\n",
    "                    break\n",
    "        else:\n",
    "            startIndexReplace = 0            \n",
    "        #print(f' replaceSubstring, startIndexReplace={{{startIndexReplace}}}')\n",
    "        if endIndex < len(text):\n",
    "            for i in range(endIndex-1, len(text), 1):\n",
    "                if isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    endIndexReplace = i + 1\n",
    "                    break\n",
    "                elif i == len(text)-1:\n",
    "                    endIndexReplace = len(text)\n",
    "        else:\n",
    "            endIndexReplace = len(text)\n",
    "        #print(f' replaceSubstring, endIndexReplace={{{endIndexReplace}}}')\n",
    "        if endIndexReplace > 0:\n",
    "            changedText = text[0:startIndexReplace] + substituteString + text[endIndexReplace:len(text)]\n",
    "            return changedText, text[startIndexReplace:endIndexReplace]\n",
    "    return text, \"\"\n",
    "\n",
    "def getAddress(text):\n",
    "    overflow = 0\n",
    "    df_LOC = getLOCTag(text)\n",
    "    addressFound = False\n",
    "    if not df_LOC.empty:\n",
    "        startIndex = df_LOC[\"start\"].iloc[-1]\n",
    "        endIndex = df_LOC[\"end\"].iloc[-1]\n",
    "        address = text[startIndex:endIndex]\n",
    "        #If the range determined by the tagger corresponds to a string \n",
    "        #that is only delimited by punctuation before and after, then it is most likely a publisher.\n",
    "        #startIndex - 2 because of a space inbetween\n",
    "        print(f' getAddress, len(text)={{{len(text)}}}')\n",
    "        print(f' getAddress, endIndex={{{endIndex}}}')\n",
    "        print(f' getAddress, text={{{text}}}')\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                addressFound = True\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                addressFound = True\n",
    "        if len(df_LOC) > 1 and addressFound:\n",
    "            startIndex2 = df_LOC[\"start\"].iloc[0]\n",
    "            endIndex2 = df_LOC[\"end\"].iloc[0]\n",
    "            address2 = text[startIndex2:endIndex2]\n",
    "            if isSpeceficPunctuation(text[startIndex2 - 2]) and isSpeceficPunctuation(text[endIndex2:startIndex]):\n",
    "                changedText, address = replaceSubstring(startIndex2, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(address)\n",
    "        if addressFound:\n",
    "            changedText, address = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "            return changedText, custom_strip(address)\n",
    "    return text, \"\"\n",
    "\n",
    "def getDate(text):\n",
    "    monthYearRegex = \"\\b(January|Jan\\.?|February|Feb\\.?|March|Mar\\.?|April|Apr\\.?\" \\\n",
    "    \"|May|May\\.?|June|Jun\\.?|July|Jul\\.?|August|Aug\\.?|September|Sep\\.?|Sept\\.?|October|\" \\\n",
    "    \"Oct\\.?|November|Nov\\.?|December|Dec\\.?)\\s\\d{4}\\b\"\n",
    "    changedText, monthYear  = getSubstringByRegEx(text, [monthYearRegex])\n",
    "    print(f' getDate, text={{{text}}}')\n",
    "    if monthYear == \"\":\n",
    "        yearRegEx1 = \"(\\(\\d{4}\\)|\\. \\d{4}\\.)\"\n",
    "        yearRegEx2 = \"(\\.|,) \\d{4}(\\.|,|;)\"\n",
    "        changedText, year  = getSubstringByRegEx(text, [yearRegEx1, yearRegEx2])\n",
    "        print(f' getDate, year={{{year}}}')\n",
    "        return changedText, \"\", f'year={{{year}}}'\n",
    "    monthYear = monthYear.split(' ')\n",
    "    return changedText, f'month={{{monthYear[0]}}}', f'year={{{monthYear[1]}}}'\n",
    "    \n",
    "def getTitel(text):\n",
    "    print(f' getTitel 1, text={{{text}}}')\n",
    "    ignoreCharacters = [\"?\", \":\", \"-\", \"(\", \")\"]\n",
    "    text = custom_strip(text, ignoreCharacters)\n",
    "    limit = len(text) - 1\n",
    "    i = 0\n",
    "    #remove pairs of punctuation marks \n",
    "    #Bug: AKtuell wird auch \", \" dann entfernt. Prüfen, ob \", ,\" vorliegt\n",
    "    while i < limit:\n",
    "        if (i + 2 < limit) and not (text[i] == \",\" and text[i+1] == \" \" and not isSpeceficPunctuation(text[i+2])):\n",
    "            if isSpeceficPunctuation(text[i], ignoreCharacters) and isSpeceficPunctuation(text[i+1], ignoreCharacters):\n",
    "                text = text[:i] + \".\" + text[i+2:]\n",
    "                i = i - 1\n",
    "                limit = limit - 1\n",
    "        i = i +1\n",
    "    print(f' getTitel 2, text={{{text}}}')\n",
    "    if text.count(\".\") == 1:\n",
    "        text = text.split(\".\")\n",
    "    elif text.count(\",\") == 1: \n",
    "        text = text.split(\",\")\n",
    "    elif text.count(\".\") == 2:\n",
    "        text = text.split(\".\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), custom_strip(text[2])\n",
    "    elif text.count(\",\") > 1:\n",
    "        text = text.rsplit(',', 1)\n",
    "    return text[0], text[1], \"\"\n",
    "\n",
    "def getPersonTags(text):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty: \n",
    "        return df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getORGTag(text, score):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"ORG\") & (df_outputs[\"score\"] >= score)].reset_index(drop=True).tail(1)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getLOCTag(text):\n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"LOC\")].reset_index(drop=True).tail(2)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getDoi(text):\n",
    "    doiUrlRegEx1 = \"https:\\/\\/doi\\.org(\\/[^\\s]*)?$\"\n",
    "    doiUrlRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org)?([^\\s]*)+$\"\n",
    "    text, doi  = getSubstringByRegEx(text, [doiUrlRegEx1, doiUrlRegEx2])\n",
    "    httpsDomainRegEx1 = \"https:\\/\\/doi\\.org\\/\"\n",
    "    httpsDomainRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org\\/)?\"\n",
    "    doi, httpsDomain = getSubstringByRegEx(doi, [httpsDomainRegEx1, httpsDomainRegEx2])\n",
    "    return text, custom_strip(doi)\n",
    "\n",
    "def getSubstringByRegEx(text, regex = []):\n",
    "    startIndex, endIndex, substring = getIndexOfSubstring(text, regex, True)\n",
    "    #print(f' getSubstringByRegEx, startIndex={{{startIndex}}}')\n",
    "    #print(f' getSubstringByRegEx, startIndex={{{endIndex}}}')\n",
    "    #print(f' getSubstringByRegEx, text={{{substring}}}')\n",
    "    changedText, substring = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "    #print(f' getSubstringByRegEx, changedText={{{changedText}}}')\n",
    "    return changedText, custom_strip(substring)\n",
    "\n",
    "#search_terms = [\", et al.\", \" et al.\"]\n",
    "#firstStartIndex, firstEndIndex, etAl = find_First_Term(text, search_terms)\n",
    "#if firstStartIndex > -1:\n",
    "    #text = replaceSubstring(firstStartIndex, firstEndIndex, text, \", \")\n",
    "\n",
    "def create_bibtex(text):\n",
    "    author = \"\"\n",
    "    editor = \"\"\n",
    "    title = \"\"\n",
    "    booktitle = \"\"\n",
    "    journal = \"\"\n",
    "    series = \"\"\n",
    "    year = \"\"\n",
    "    volume = \"\"\n",
    "    number = \"\"\n",
    "    edition = \"\"\n",
    "    pages = \"\"\n",
    "    month = \"\"\n",
    "    publisher = \"\"\n",
    "    school = \"\"\n",
    "    address = \"\"\n",
    "    note = \"\"\n",
    "    annote = \"\"\n",
    "    doi = \"\"\n",
    "    url = \"\"\n",
    "    book = False\n",
    "    article = False\n",
    "    proceedings = False\n",
    "    inproceedings = False\n",
    "    incollection = False\n",
    "    phdThesis = False\n",
    "    \n",
    "    urlRegEx = \"https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?::\\d+)?(?:/[^\\s]*)?\"\n",
    "    pageRegEx = \"(?:pp\\.? )?\\d+(-|–)\\d+\"\n",
    "    volumeRegEx = \"(V|v)ol\\. \\d+\"\n",
    "    number1RegEx = \"no\\. \\d+\"\n",
    "    number2RegEx = \"Issue \\d+\"\n",
    "    #number3RegEx = \"\\d+\"\n",
    "    edition1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "    edition2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "    \n",
    "    text, author = getAuthors(text)\n",
    "    text, editor = getEditors(text)\n",
    "    text, doi = getDoi(text)\n",
    "    text, url = getSubstringByRegEx(text, [urlRegEx])\n",
    "    text, month, year = getDate(text)\n",
    "    text, page = getSubstringByRegEx(text, [pageRegEx])\n",
    "    print(f' main, text={{{text}}}')\n",
    "    text, volume = getSubstringByRegEx(text, [volumeRegEx])\n",
    "    text, number = getSubstringByRegEx(text, [number1RegEx, number2RegEx])\n",
    "    text, edition = getSubstringByRegEx(text, [edition1, edition2])\n",
    "    #BUGFIX: Wenn nur num vorkommt, dann schneidet volume die Zahl von num aus!!!!!!\n",
    "    #volume3 darf also erst geprüft werden, wenn num1 und num2 geprüft wurden.\n",
    "    #VOlumer erscheint aber immre vor number\n",
    "    print(f' main, text2={{{text}}}')\n",
    "    text, address = getAddress(text)\n",
    "    text, publisher = getPublisher(text, doi)\n",
    "    titel, booktitel, series = getTitel(text)\n",
    "    \n",
    "    #Idee: Mit Pos-Tagging herausfinden, wo Nomen etc. vorkommen und dann titel und Booktitel eingrenzen\n",
    "\n",
    "    return f'authors: {author}' + \", \\r\\n\" + f'editors: {editor}' \\\n",
    "+ \", \\r\\n\" + f'doi: {doi}' +  \", \\r\\n\"  +  f'{year}' +  \", \\r\\n\"  + f'number : {number}' \\\n",
    "+  \", \\r\\n\" + f'volume : {volume}' +  \", \\r\\n\"  + f'edition: {edition}' +  \", \\r\\n\"  + f'page: {page}' \\\n",
    "+  \", \\r\\n\"  + f'url: {url}' +  \", \\r\\n\"  + f'publisher: {publisher}' +  \", \\r\\n\"  + f'address: {address}' \\\n",
    "+  \", \\r\\n\" + f'titel: {titel}' +  \", \\r\\n\"  + f'booktitel: {booktitel}' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4a1358ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function processNames, surenameFirst: True\n",
      "Fall surenameFirst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " getDate, text={. 2023. On the Linguistic Limitations of ChatGPT: An Experimental Case Study. In 2023 International Symposium on Networks, Computers and Communications (ISNCC), 1–6. }\n",
      " getDate, year={2023}\n",
      " main, text={ On the Linguistic Limitations of ChatGPT: An Experimental Case Study. In 2023 International Symposium on Networks, Computers and Communications (ISNCC),  }\n",
      " main, text2={ On the Linguistic Limitations of ChatGPT: An Experimental Case Study. In 2023 International Symposium on Networks, Computers and Communications (ISNCC),  }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " getTitel 1, text={ On the Linguistic Limitations of ChatGPT: An Experimental Case Study. In 2023 International Symposium on Networks, Computers and Communications (ISNCC),  }\n",
      " getTitel 2, text={On the Linguistic Limitations of ChatGPT: An Experimental Case Study.In 2023 International Symposium on Networks, Computers and Communications (ISNCC)}\n",
      "authors: Mohammed Baziyad and Ibrahim Kamel and Tamer Rabie, \r\n",
      "editors: , \r\n",
      "doi: 10.1109/ISNCC58260.2023.10323661, \r\n",
      "year={2023}, \r\n",
      "number : , \r\n",
      "volume : , \r\n",
      "edition: , \r\n",
      "page: 1–6, \r\n",
      "url: , \r\n",
      "publisher: , \r\n",
      "address: , \r\n",
      "titel: On the Linguistic Limitations of ChatGPT: An Experimental Case Study, \r\n",
      "booktitel: In 2023 International Symposium on Networks, Computers and Communications (ISNCC)\n"
     ]
    }
   ],
   "source": [
    "text=\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "#text = \"M. A. Nielsen and I. L. Chuang, “Quantum Computation and Quantum Information,” in Handbook of Quantum Information Science, vol. 4, C. H. Bennett and D. P. DiVincenzo, Eds. Berlin, Germany: Springer, 2026, pp. 250–300. doi: 10.1007/springerreference-303198.\"\n",
    "#text = \"Nielsen, M. A.; Chuang, I. L. Quantum Computation and Quantum Information. In Handbook of Quantum Information Science; Bennett, C. H., DiVincenzo, D. P., Eds.; Quantum Science and Technology; Springer: Berlin, Germany, 2026; Vol. 4, pp 250–300. https://doi.org/10.1007/springerreference-303198.\"\n",
    "\n",
    "#BUG: startIndexReplace={-1} ist hier bei getYear! Deswegen doppelter String drin\n",
    "text = \"\"\"Alahmed, Y., Abadla, R., Badri, A. A., & Ameen, N. (2023). “How Does ChatGPT Work” Examining Functionality, To The Creative AI CHATGPT on X’s (Twitter) Platform. 2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS), 1–7. https://doi.org/10.1109/SNAMS60348.2023.10375450\"\"\"\n",
    "text = \"David Mertz, Regular Expression Puzzles and AI Coding Assistants: 24 puzzles solved by the author, with and without assistance from Copilot, ChatGPT and more , Manning, 2023.\"\n",
    "text = \"\"\"Mohammed Baziyad, Ibrahim Kamel, and Tamer Rabie. 2023. On the Linguistic Limitations of ChatGPT: An Experimental Case Study. In 2023 International Symposium on Networks, Computers and Communications (ISNCC), 1–6. DOI:https://doi.org/10.1109/ISNCC58260.2023.10323661\"\"\"\n",
    "print(create_bibtex(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11af99c",
   "metadata": {},
   "source": [
    "Idee: Nun zunächst Jahr, Volume, Seiten, Edition und URL/DOI extrahieren. Den Rest (also Titel, Publisher, Series), dann nochmal den SpacyParer drüber laufen lassen, weil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ad04c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3af1376efbe84c61b6104ef41acda615-0\" class=\"displacy\" width=\"5125\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">&quot;</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">What</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Drives</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">IT</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Students</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Toward</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">ChatGPT?</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Analyzing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">Factors</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">Influencing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">Students'</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">Intention</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">Use</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">ChatGPT</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">Educational</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">Purposes,&quot;</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">2024</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">21st</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">International</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">Multi-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">Conference</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">Systems,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4600\">Signals &amp;</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4600\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4775\">Devices (</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4775\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4950\">SSD)</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4950\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,89.5 745.0,89.5 745.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">csubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-4\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,354.0 L918.0,342.0 902.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-6\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,177.0 1615.0,177.0 1615.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1615.0,354.0 L1623.0,342.0 1607.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-8\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1785.0,354.0 L1793.0,342.0 1777.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-9\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,354.0 L1987,342.0 2003,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-10\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,177.0 2140.0,177.0 2140.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2140.0,354.0 L2148.0,342.0 2132.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-11\" stroke-width=\"2px\" d=\"M2345,352.0 C2345,264.5 2485.0,264.5 2485.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2345,354.0 L2337,342.0 2353,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-12\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,89.5 2495.0,89.5 2495.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2495.0,354.0 L2503.0,342.0 2487.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-13\" stroke-width=\"2px\" d=\"M2520,352.0 C2520,264.5 2660.0,264.5 2660.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2660.0,354.0 L2668.0,342.0 2652.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-14\" stroke-width=\"2px\" d=\"M2695,352.0 C2695,264.5 2835.0,264.5 2835.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2835.0,354.0 L2843.0,342.0 2827.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-15\" stroke-width=\"2px\" d=\"M3045,352.0 C3045,264.5 3185.0,264.5 3185.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3045,354.0 L3037,342.0 3053,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-16\" stroke-width=\"2px\" d=\"M2870,352.0 C2870,177.0 3190.0,177.0 3190.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3190.0,354.0 L3198.0,342.0 3182.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-17\" stroke-width=\"2px\" d=\"M3395,352.0 C3395,264.5 3535.0,264.5 3535.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3395,354.0 L3387,342.0 3403,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-18\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,2.0 3550.0,2.0 3550.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3550.0,354.0 L3558.0,342.0 3542.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-19\" stroke-width=\"2px\" d=\"M3745,352.0 C3745,264.5 3885.0,264.5 3885.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3745,354.0 L3737,342.0 3753,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-20\" stroke-width=\"2px\" d=\"M3920,352.0 C3920,264.5 4060.0,264.5 4060.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4060.0,354.0 L4068.0,342.0 4052.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-21\" stroke-width=\"2px\" d=\"M4095,352.0 C4095,264.5 4235.0,264.5 4235.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4235.0,354.0 L4243.0,342.0 4227.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-22\" stroke-width=\"2px\" d=\"M4270,352.0 C4270,264.5 4410.0,264.5 4410.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4410.0,354.0 L4418.0,342.0 4402.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-23\" stroke-width=\"2px\" d=\"M4445,352.0 C4445,264.5 4585.0,264.5 4585.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4585.0,354.0 L4593.0,342.0 4577.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-24\" stroke-width=\"2px\" d=\"M4620,352.0 C4620,264.5 4760.0,264.5 4760.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4760.0,354.0 L4768.0,342.0 4752.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3af1376efbe84c61b6104ef41acda615-0-25\" stroke-width=\"2px\" d=\"M4620,352.0 C4620,177.0 4940.0,177.0 4940.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3af1376efbe84c61b6104ef41acda615-0-25\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4940.0,354.0 L4948.0,342.0 4932.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \", POS: PUNCT, Head: Students, Dep: punct\n",
      "Text: What, POS: PRON, Head: Drives, Dep: nsubj\n",
      "Text: Drives, POS: VERB, Head: Students, Dep: csubj\n",
      "Text: IT, POS: PRON, Head: Students, Dep: compound\n",
      "Text: Students, POS: NOUN, Head: Students, Dep: ROOT\n",
      "Text: Toward, POS: ADP, Head: Students, Dep: prep\n",
      "Text: ChatGPT, POS: NOUN, Head: Toward, Dep: pobj\n",
      "Text: ?, POS: PUNCT, Head: Students, Dep: punct\n",
      "Text: Analyzing, POS: VERB, Head: Analyzing, Dep: ROOT\n",
      "Text: the, POS: DET, Head: Factors, Dep: det\n",
      "Text: Factors, POS: NOUN, Head: Analyzing, Dep: dobj\n",
      "Text: Influencing, POS: VERB, Head: Factors, Dep: acl\n",
      "Text: Students, POS: PROPN, Head: Intention, Dep: poss\n",
      "Text: ', POS: PART, Head: Students, Dep: case\n",
      "Text: Intention, POS: NOUN, Head: Influencing, Dep: dobj\n",
      "Text: to, POS: PART, Head: Use, Dep: aux\n",
      "Text: Use, POS: VERB, Head: Influencing, Dep: xcomp\n",
      "Text: ChatGPT, POS: NOUN, Head: Use, Dep: dobj\n",
      "Text: for, POS: ADP, Head: ChatGPT, Dep: prep\n",
      "Text: Educational, POS: PROPN, Head: Purposes, Dep: compound\n",
      "Text: Purposes, POS: PROPN, Head: for, Dep: pobj\n",
      "Text: ,, POS: PUNCT, Head: Analyzing, Dep: punct\n",
      "Text: \", POS: PUNCT, Head: Analyzing, Dep: punct\n",
      "Text: 2024, POS: NUM, Head: 21st, Dep: nummod\n",
      "Text: 21st, POS: NOUN, Head: Analyzing, Dep: npadvmod\n",
      "Text: International, POS: PROPN, Head: Multi, Dep: compound\n",
      "Text: Multi, POS: PROPN, Head: Multi, Dep: ROOT\n",
      "Text: -, POS: PROPN, Head: Multi, Dep: appos\n",
      "Text: Conference, POS: PROPN, Head: Multi, Dep: appos\n",
      "Text: on, POS: ADP, Head: Conference, Dep: prep\n",
      "Text: Systems, POS: PROPN, Head: on, Dep: pobj\n",
      "Text: ,, POS: PUNCT, Head: Systems, Dep: punct\n",
      "Text: Signals, POS: PROPN, Head: Systems, Dep: conj\n",
      "Text: &, POS: CCONJ, Head: Signals, Dep: cc\n",
      "Text: Devices, POS: PROPN, Head: Signals, Dep: conj\n",
      "Text: (, POS: PUNCT, Head: Signals, Dep: punct\n",
      "Text: SSD, POS: PROPN, Head: Signals, Dep: appos\n",
      "Text: ), POS: PUNCT, Head: Conference, Dep: punct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score                   word  start  end\n",
      "0          ORG  0.577290                     IT     13   15\n",
      "1         MISC  0.982862    International Multi    147  166\n",
      "2         MISC  0.767340  Conference on Systems    167  188\n",
      "3          ORG  0.379331                      ,    188  189\n",
      "4         MISC  0.555999                Signals    190  197\n",
      "5          ORG  0.612880                      &    198  199\n",
      "6         MISC  0.757536                Devices    200  207\n",
      "7         MISC  0.718369                    SSD    209  212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-ORG', 'score': 0.57729024, 'index': 5, 'word': 'IT', 'start': 13, 'end': 15}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9796957, 'index': 45, 'word': 'International', 'start': 147, 'end': 160}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.98602736, 'index': 46, 'word': 'Multi', 'start': 161, 'end': 166}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.9863149, 'index': 48, 'word': 'Conference', 'start': 167, 'end': 177}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.6964862, 'index': 49, 'word': 'on', 'start': 178, 'end': 180}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.61921793, 'index': 50, 'word': 'Systems', 'start': 181, 'end': 188}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-ORG', 'score': 0.37933114, 'index': 51, 'word': ',', 'start': 188, 'end': 189}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.57070875, 'index': 52, 'word': 'Signal', 'start': 190, 'end': 196}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.5412901, 'index': 53, 'word': '##s', 'start': 196, 'end': 197}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-ORG', 'score': 0.6128797, 'index': 54, 'word': '&', 'start': 198, 'end': 199}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.7827443, 'index': 55, 'word': 'Devi', 'start': 200, 'end': 204}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.732328, 'index': 56, 'word': '##ces', 'start': 204, 'end': 207}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.6014996, 'index': 58, 'word': 'SS', 'start': 209, 'end': 211}\n",
      "---------------------------------------------------------------------------\n",
      "{'entity': 'I-MISC', 'score': 0.83523804, 'index': 59, 'word': '##D', 'start': 211, 'end': 212}\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Test, um Datum Zuverlässig erkannt wird\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "\n",
    "text = \"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence, In Proceedings of the IEEE International Conference on Neural Networks, Physical Review.\"\n",
    "text = \"\"\"\"What Drives IT Students Toward ChatGPT? Analyzing the Factors Influencing Students' Intention to Use ChatGPT for Educational Purposes,\" 2024 21st International Multi-Conference on Systems, Signals & Devices (SSD)\"\"\"\n",
    "\n",
    "#text = \"dsfsdf London\"\n",
    "print(len(text))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "# Visualisierung der Abhängigkeiten\n",
    "displacy.render(doc, style='dep', jupyter=True)\n",
    "#for token in doc:\n",
    "#    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)\n",
    "#for ent in doc.ents:\n",
    "#    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Text: {token.text}, POS: {token.pos_}, Head: {token.head.text}, Dep: {token.dep_}')\n",
    "\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "print(df_outputs)\n",
    "pos_pipeline = pipeline(\"token-classification\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "results = pos_pipeline(text)\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print(\"---------------------------------------------------------------------------\")\n",
    "    #print(f\"Word: {result['word']}, POS Tag: {result['entity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "690d3437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: T.R. Fernandez Perez Tomei\n",
      "Affiliation: Researcher IV, Universidade Estadual Paulista\n",
      "\n",
      "Title: Observation of a new boson at a mass of 125 GeV with the CMS experiment at the LHC\n",
      "Authors: Serguei Chatrchyan and Vardan Khachatryan and Albert M Sirunyan and Armen Tumasyan and Wolfgang Adam and Ernest Aguilo and Thomas Bergauer and Marko Dragicevic and Janos Erö and Christian Fabjan and M Friedl and Rudolf Frühwirth and VM Ghete and J Hammer and M Hoch and N Hörmann and J Hrubec and M Jeitler and W Kiesenhofer and V Knünz and M Krammer and I Krätschmer and D Liko and W Majerotto and I Mikulec and M Pernicka and B Rahbaran and C Rohringer and H Rohringer and R Schöfbeck and J Strauss and F Szoncsó and A Taurok and W Waltenberger and G Walzel and E Widl and C-E Wulz and I Emeliantchik and V Makarenko and N Shumeiko and A Solin and R Stefanovitch and J Suarez Gonzalez and A Fedorov and M Korzhik and O Missevitch and R Zuyeuski and M Bansal and S Bansal and W Beaumont and Tom Cornelis and EA De Wolf and D Druzhkin and X Janssen and S Luyckx and L Mucibello and S Ochesanu and B Roland and R Rougny and M Selvaggi and Z Staykova and H Van Haevermaet and P Van Mechelen and A Van Spilbeeck and F Blekman and S Blyweert and J DʼHondt and O Devroede and R Gonzalez Suarez and R Goorens and A Kalogeropoulos and M Maes and A Olbrechts and S Tavernier and W Van Doninck and L Van Lancker and P Van Mulders and GP Van Onsem and I Villella and B Clerbaux and G De Lentdecker and V Dero and JP Dewulf and APR Gay and T Hreus and A Léonard and PE Marage and A Mohammadi and T Reis and S Rugovac and L Thomas and C Vander Velde and P Vanlaer and J Wang and J Wickens and Volker Adler and Kelly Beernaert and Anna Cimmino and Silvia Costantini and Guillaume Garcia and M Grunewald and Benjamin Klein and Jeremie Lellouch and Andrey Marinov and Joseph McCartin and AA Ocampo Rios and Dirk Ryckbosch and Nadja Strobbe and Filip Thyssen and Michael Tytgat and S Walsh and Efe Yazgan and Nikolaos Zaganidis and S Basegmez and G Bruno and R Castello and L Ceard and J De Favereau De Jeneret and C Delaere and P Demin and T du Pree and D Favart and L Forthomme and A Giammanco and G Grégoire and J Hollar and V Lemaitre and J Liao and O Militaru and C Nuttens and D Pagano and A Pin and K Piotrzkowski and N Schul and JM Vizan Garcia and N Beliy and T Caebergs and E Daubie and GH Hammad and GA Alves and L Brito and M Correa Martin Junior and T Martins and ME Pol and MHG Souza and WL Aldá Júnior and W Carvalho and A Custódio and EM Da Costa and D De Jesus Damiao\n",
      "Year: 2012\n",
      "\n",
      "Title: Global, regional, and national incidence, prevalence, and years lived with disability for 328 diseases and injuries for 195 countries, 1990–2016: a systematic analysis for the Global Burden of Disease Study 2016\n",
      "Authors: Theo Vos and Amanuel Alemu Abajobir and Kalkidan Hassen Abate and Cristiana Abbafati and Kaja M Abbas and Foad Abd-Allah and Rizwan Suliankatchi Abdulkader and Abdishakur M Abdulle and Teshome Abuka Abebo and Semaw Ferede Abera and Victor Aboyans and Laith J Abu-Raddad and Ilana N Ackerman and Abdu Abdullahi Adamu and Olatunji Adetokunboh and Mohsen Afarideh and Ashkan Afshin and Sanjay Kumar Agarwal and Rakesh Aggarwal and Anurag Agrawal and Sutapa Agrawal and Hamid Ahmadieh and Muktar Beshir Ahmed and Miloud Taki Eddine Aichour and Amani Nidhal Aichour and Ibtihel Aichour and Sneha Aiyar and Rufus Olusola Akinyemi and Nadia Akseer and Faris Hasan Al Lami and Fares Alahdab and Ziyad Al-Aly and Khurshid Alam and Noore Alam and Tahiya Alam and Deena Alasfoor and Kefyalew Addis Alene and Raghib Ali and Reza Alizadeh-Navaei and Ala'a Alkerwi and François Alla and Peter Allebeck and Christine Allen and Fatma Al-Maskari and Rajaa Al-Raddadi and Ubai Alsharif and Shirina Alsowaidi and Khalid A Altirkawi and Azmeraw T Amare and Erfan Amini and Walid Ammar and Yaw Ampem Amoako and Hjalte H Andersen and Carl Abelardo T Antonio and Palwasha Anwari and Johan Ärnlöv and Al Artaman and Krishna Kumar Aryal and Hamid Asayesh and Solomon W Asgedom and Reza Assadi and Tesfay Mehari Atey and Niguse Tadele Atnafu and Sachin R Atre and Leticia Avila-Burgos and Euripide Frinel G Arthur Avokphako and Ashish Awasthi and Umar Bacha and Alaa Badawi and Kalpana Balakrishnan and Amitava Banerjee and Marlena S Bannick and Aleksandra Barac and Ryan M Barber and Suzanne L Barker-Collo and Till Bärnighausen and Simon Barquera and Lars Barregard and Lope H Barrero and Sanjay Basu and Bob Battista and Katherine E Battle and Bernhard T Baune and Shahrzad Bazargan-Hejazi and Justin Beardsley and Neeraj Bedi and Ettore Beghi and Yannick Béjot and Bayu Begashaw Bekele and Michelle L Bell and Derrick A Bennett and Isabela M Bensenor and Jennifer Benson and Adugnaw Berhane and Derbew Fikadu Berhe and Eduardo Bernabé and Balem Demtsu Betsu and Mircea Beuran and Addisu Shunu Beyene and Neeraj Bhala and Anil Bhansali and Samir Bhatt and Zulfiqar A Bhutta and Sibhatu Biadgilign and Burcu Kucuk Bicer and Kelly Bienhoff and Boris Bikbov and Charles Birungi and Stan Biryukov and Donal Bisanzio and Habtamu Mellie Bizuayehu and Dube Jara Boneya and Soufiane Boufous and Rupert RA Bourne and Alexandra Brazinova and Traolach S Brugha and Rachelle Buchbinder and Lemma Negesa Bulto Bulto and Blair R Bumgarner and Zahid A Butt and Lucero Cahuana-Hurtado and Ewan Cameron and Mate Car and Hélène Carabin and Jonathan R Carapetis and Rosario Cárdenas and David O Carpenter and Juan Jesus Carrero and Austin Carter and Felix Carvalho and Daniel C Casey and Valeria Caso and Carlos A Castañeda-Orjuela and Chris D Castle and Ferrán Catalá-López and Hsing-Yi Chang and Jung-Chen Chang and Fiona J Charlson and Honglei Chen and Mirriam Chibalabala and Chioma Ezinne Chibueze and Vesper Hichilombwe Chisumpa and Abdulaal A Chitheer and Devasahayam Jesudas Christopher and Liliana G Ciobanu and Massimo Cirillo and Danny Colombara and Cyrus Cooper and Paolo Angelo Cortesi and Michael H Criqui\n",
      "Year: 2017\n",
      "\n",
      "Title: Precise determination of the mass of the Higgs boson and tests of compatibility of its couplings with the standard model predictions using proton collisions at 7 and 8 TeV\n",
      "Authors: CMS collaboration\n",
      "Year: 2015\n",
      "\n",
      "Title: Event generator tunes obtained from underlying event and multiparton scattering measurements\n",
      "Authors: Vardan Khachatryan and Albert M Sirunyan and Armen Tumasyan and Wolfgang Adam and E Asilar and Thomas Bergauer and Johannes Brandstetter and Erica Brondolin and Marko Dragicevic and Janos Erö and M Friedl and R Frühwirth and VM Ghete and C Hartl and N Hörmann and J Hrubec and M Jeitler and V Knünz and A König and M Krammer and I Krätschmer and D Liko and T Matsushita and I Mikulec and D Rabady and B Rahbaran and H Rohringer and J Schieck and R Schöfbeck and J Strauss and W Treberer-Treberspurg and W Waltenberger and C-E Wulz and V Mossolov and N Shumeiko and J Suarez Gonzalez and S Alderweireldt and Tom Cornelis and EA De Wolf and X Janssen and A Knutsson and J Lauwers and S Luyckx and M Van De Klundert and H Van Haevermaet and P Van Mechelen and N Van Remortel and A Van Spilbeeck and S Abu Zeid and F Blekman and J D’Hondt and N Daci and I De Bruyn and K Deroover and N Heracleous and J Keaveney and S Lowette and L Moreels and A Olbrechts and Q Python and D Strom and S Tavernier and W Van Doninck and Petra Van Mulders and Gerrit Patrick Van Onsem and Isis Van Parijs and P Barria and H Brun and C Caillol and B Clerbaux and G De Lentdecker and G Fasanella and L Favart and A Grebenyuk and G Karapostoli and T Lenzi and A Léonard and T Maerschalk and A Marinov and L Perniè and A Randle-Conde and T Seva and C Vander Velde and R Yonamine and P Vanlaer and R Yonamine and F Zenoni and F Zhang and V Adler and K Beernaert and L Benucci and A Cimmino and S Crucy and D Dobur and A Fagot and G Garcia and M Gul and J Mccartin and AA Ocampo Rios and D Poyraz and D Ryckbosch and S Salva and M Sigamani and M Tytgat and W Van Driessche and E Yazgan and N Zaganidis and S Basegmez and C Beluffi and O Bondu and S Brochet and G Bruno and A Caudron and L Ceard and GG Da Silveira and C Delaere and D Favart and L Forthomme and A Giammanco and J Hollar and A Jafari and P Jez and M Komm and V Lemaitre and A Mertens and M Musich and C Nuttens and L Perrini and A Pin and K Piotrzkowski and A Popov and L Quertenmont and M Selvaggi and M Vidal Marono and N Beliy and GH Hammad and WL Aldá Júnior and FL Alves and GA Alves and L Brito and M Correa Martins Junior and M Hamer and C Hensel and A Moraes and ME Pol and P Rebello Teles and E Belchior Batista Das Chagas and W Carvalho and J Chinellato and A Custódio\n",
      "Year: 2016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Evidence for the 125 GeV Higgs boson decaying to a pair of τ leptons\n",
      "Authors: Serguei Chatrchyan and Vardan Khachatryan and Albert M Sirunyan and Armen Tumasyan and Wolfgang Adam and Thomas Bergauer and Marko Dragicevic and Janos Erö and Christian Fabjan and Markus Friedl and Rudolf Fruehwirth and Vasile Mihai Ghete and Christian Hartl and Natascha Hörmann and Josef Hrubec and Manfred Jeitler and Wolfgang Kiesenhofer and Valentin Knünz and Manfred Krammer and Ilse Krätschmer and Dietrich Liko and Ivan Mikulec and Dinyar Rabady and Babak Rahbaran and Herbert Rohringer and Robert Schöfbeck and Josef Strauss and Anton Taurok and Wolfgang Treberer-Treberspurg and Wolfgang Waltenberger and C-E Wulz and N Shumeiko and S Alderweireldt and M Bansal and S Bansal and T Cornelis and EA De Wolf and X Janssen and A Knutsson and S Luyckx and S Ochesanu and B Roland and R Rougny and H Van Haevermaet and P Van Mechelen and A Van Spilbeeck and F Blekman and S Blyweert and J D’Hondt and N Heracleous and A Kalogeropoulos and J Keaveney and TJ Kim and S Lowette and M Maes and A Olbrechts and D Strom and S Tavernier and W Van Doninck and P Van Mulders and GP Van Onsem and I Villella and C Caillol and B Clerbaux and G De Lentdecker and L Favart and APR Gay and A Léonard and PE Marage and A Mohammadi and L Perniè and T Reis and T Seva and L Thomas and C Vander Velde and P Vanlaer and J Wang and V Adler and K Beernaert and L Benucci and A Cimmino and S Costantini and S Crucy and S Dildick and G Garcia and B Klein and J Lellouch and J Mccartin and AA Ocampo Rios and D Ryckbosch and S Salva Diblen and M Sigamani and N Strobbe and F Thyssen and M Tytgat and S Walsh and E Yazgan and N Zaganidis and S Basegmez and C Beluffi and G Bruno and R Castello and A Caudron and L Ceard and GG Da Silveira and C Delaere and T Du Pree and D Favart and L Forthomme and A Giammanco and J Hollar and P Jez and M Komm and V Lemaitre and J Liao and O Militaru and C Nuttens and Davide Pagano and L Perrini and Arnaud Pin and Krzysztof Piotrzkowski and Andrey Popov and Loic Quertenmont and Michele Selvaggi and Miguel Vidal Marono and Nikita Beliy and Thierry Caebergs and Evelyne Daubie and Gregory Habib Hammad and GA Alves and Marcos Correa Martins Junior and T Martins and Maria Elena Pol and MHG Souza and Walter Luiz Aldá Júnior and Wagner Carvalho and Jose Chinellato and Analu Custódio and EM Da Costa and Dilson De Jesus Damiao and Carley De Oliveira Martins and Sandro Fonseca De Souza and Helena Malbouisson and Magdalena Malek and Diego Matos Figueiredo and Luiz Mundim and Helio Nogima and Wanda Lucia Prado Da Silva and Javier Santaolalla and Alberto Santoro\n",
      "Year: 2014\n"
     ]
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "\n",
    "# Suche nach einem Autor\n",
    "search_query = scholarly.search_author('Fernández, R.')\n",
    "author = next(search_query)\n",
    "\n",
    "# Laden Sie das komplette Autorenprofil\n",
    "author = scholarly.fill(author)\n",
    "\n",
    "# Zeigen Sie einige Informationen über den Autor an\n",
    "print(f\"Name: {author['name']}\")\n",
    "print(f\"Affiliation: {author['affiliation']}\")\n",
    "\n",
    "# Zeigen Sie die ersten fünf Veröffentlichungen an\n",
    "for publication in author['publications'][:5]:\n",
    "    pub = scholarly.fill(publication)\n",
    "    print(f\"\\nTitle: {pub['bib']['title']}\")\n",
    "    print(f\"Authors: {pub['bib']['author']}\")\n",
    "    print(f\"Year: {pub['bib']['pub_year']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
