{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wenn man den typen bestimmte hat, sollte man zunächst alle Pflicht-Felder des Bibtex-Eintrages raussuchen und bestimmen.\n",
    "#Dazu die Pflichtattribute in einer Schleife durchgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bde3f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75bbdc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text=\"Hinton, G., Bengio, Y. and LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "df_PER = df_outputs[df_outputs[\"entity_group\"] == \"PER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b9bb15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.996079</td>\n",
       "      <td>Hinton</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998951</td>\n",
       "      <td>G</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.994906</td>\n",
       "      <td>Bengio</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998544</td>\n",
       "      <td>Y</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.963430</td>\n",
       "      <td>LeCun</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998110</td>\n",
       "      <td>Y</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.949492</td>\n",
       "      <td>J. Smith</td>\n",
       "      <td>91</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.992646</td>\n",
       "      <td>J. Doe</td>\n",
       "      <td>102</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score      word  start  end\n",
       "0          PER  0.996079    Hinton      0    6\n",
       "1          PER  0.998951         G      8    9\n",
       "2          PER  0.994906    Bengio     12   18\n",
       "3          PER  0.998544         Y     20   21\n",
       "4          PER  0.963430     LeCun     27   32\n",
       "5          PER  0.998110         Y     34   35\n",
       "8          PER  0.949492  J. Smith     91   99\n",
       "9          PER  0.992646    J. Doe    102  108"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e9f3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "startIndexPER = df_PER[\"start\"].iloc[0]\n",
    "endIndexPER = df_PER[\"end\"].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2df9df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1\n",
      "B    4\n",
      "C    7\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispiel DataFrame erstellen\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "})\n",
    "\n",
    "# Zugriff auf die erste Zeile\n",
    "erste_zeile = df.iloc[0]\n",
    "print(erste_zeile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daa1be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinton 0 6 GPE\n",
      "G. 8 10 GPE\n",
      "Bengio 12 18 PERSON\n",
      "Y. 20 22 PERSON\n",
      "Y. 34 36 PERSON\n",
      "2021 38 42 DATE\n",
      "Deep Learning for Artificial Intelligence 0 41 ORG\n",
      "J. Smith & J. Doe (Eds 3 25 ORG\n",
      "Proceedings of the IEEE International Conference on Neural Networks 29 96 ORG\n",
      "1 103 104 CARDINAL\n",
      "5 112 113 DATE\n",
      "100–120 119 126 CARDINAL\n",
      "IEEE Press 0 10 ORG\n",
      "https://doi.org/10.1109/ICNN.2021.9483948 0 41 GPE\n"
     ]
    }
   ],
   "source": [
    "#Test, um Datum Zuverlässig erkannt wird\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hinton, G., Bengio, Y. and LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\")\n",
    "for ent in doc.ents:\n",
    "  print(ent, ent.start_char-ent.sent.start_char, ent.end_char-ent.sent.start_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55d2b1",
   "metadata": {},
   "source": [
    "@book{advanced_physics, author = {Stephen Hawking and Brian Greene and Maria Trump and Susan Soy}, title = {Advanced Concepts in Theoretical Physics}, publisher = {Cambridge University Press}, year = {2025}, edition = {3}, volume = {1}, series = {Advanced Studies in Physics}, address = {Cambridge, UK}, month = {May}, note = {A seminal work in the field of theoretical physics}, isbn = {978-0-521-76948-0}, doi = {10.1017/example.book.2025}, url = {https://www.cambridge.org/advanced_physics}, annote = {Widely cited in the physics community}, abstract = {This book explores cutting-edge theories and concepts in theoretical physics...}, keywords = {Theoretical Physics, Quantum Mechanics, String Theory}, language = {English}, price = {75.00}, size = {600 pages}, lccn = {2020934576}, mrnumber = {MR3070071} }\n",
    "\n",
    "@article{quantum_entanglement,\n",
    "  author        = {Albert Einstein and Boris Podolsky and Nathan Rosen},\n",
    "  title         = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},\n",
    "  journal       = {Physical Review},\n",
    "  year          = {1935},\n",
    "  volume        = {47},\n",
    "  number        = {10},\n",
    "  pages         = {777-780},\n",
    "  month         = {May},\n",
    "  note          = {EPR Paradox paper, fundamental for quantum mechanics},\n",
    "  doi           = {10.1103/PhysRev.47.777},\n",
    "  url           = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},\n",
    "  abstract      = {In this paper, the authors discuss the EPR paradox and challenge the completeness of quantum mechanics...},\n",
    "  keywords      = {Quantum Mechanics, EPR Paradox, Physical Reality},\n",
    "  language      = {English},\n",
    "  publisher     = {American Physical Society}\n",
    "}\n",
    "\n",
    "@inproceedings{deep_learning,\n",
    "  author        = {Geoffrey Hinton and Yoshua Bengio and Yann LeCun},\n",
    "  title         = {Deep Learning for Artificial Intelligence},\n",
    "  booktitle     = {Proceedings of the IEEE International Conference on Neural Networks},\n",
    "  year          = {2021},\n",
    "  editor        = {Jane Smith and John Doe},\n",
    "  volume        = {1},\n",
    "  number = {5},\n",
    "  series        = {Advances in Neural Information Processing},\n",
    "  pages         = {100-120},\n",
    "  address       = {Montreal, Canada},\n",
    "  month         = {December},\n",
    "  organization  = {IEEE},\n",
    "  publisher     = {IEEE Press},\n",
    "  note          = {Keynote paper on recent advancements in deep learning},\n",
    "  isbn          = {978-1-5386-4637-1},\n",
    "  doi           = {10.1109/ICNN.2021.9483948},\n",
    "  url           = {https://ieeexplore.ieee.org/document/9483948},\n",
    "  annote        = {A seminal work on how deep learning transforms AI},\n",
    "  abstract      = {This paper explores cutting-edge deep learning techniques and their impact on the development of artificial intelligence...},\n",
    "  keywords      = {Deep Learning, Artificial Intelligence, Neural Networks},\n",
    "  language      = {English}\n",
    "}\n",
    "\n",
    "\n",
    "@incollection{quantum_computation,\n",
    "  author        = {Michael A. Nielsen and Isaac L. Chuang},\n",
    "  title         = {Quantum Computation and Quantum Information},\n",
    "  booktitle     = {Handbook of Quantum Information Science},\n",
    "  publisher     = {Springer},\n",
    "  year          = {2026},\n",
    "  editor        = {Charles H. Bennett and David P. DiVincenzo},\n",
    "  volume        = {4},\n",
    "  series        = {Quantum Science and Technology},\n",
    "  chapter       = {10},\n",
    "  pages         = {250-300},\n",
    "  address       = {Berlin, Germany},\n",
    "  month         = {October},\n",
    "  note          = {A comprehensive overview of the fundamentals of quantum computation},\n",
    "  isbn          = {978-3-540-88702-7},\n",
    "  doi           = {10.1007/springerreference_303198},\n",
    "  url           = {https://www.springer.com/gp/book/9783540887027},\n",
    "  annote        = {Essential reading for researchers entering the field of quantum information},\n",
    "  abstract      = {This chapter delves into the principles of quantum computing, offering an accessible yet thorough introduction...},\n",
    "  keywords      = {Quantum Computing, Quantum Information, Computational Models},\n",
    "  language      = {English},\n",
    "  price         = {45.00},\n",
    "  size          = {50 pages}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93a7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RegEx zum FInden der Seiten\n",
    "pageFinder = \", (?:pp\\.? )?\\d+-\\d+,\"\n",
    "doi1 = \"https:\\/\\/doi\\.org\"\n",
    "doi2 = \"DOI:\"\n",
    "\n",
    "#if havard und book:\n",
    "volume = \", \\d+,\"\n",
    "number = \", \\d+,\"\n",
    "edition1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "edition2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "\n",
    "#if APA or Havard then\n",
    "volume = \"\\(Vol\\. \\d+\" #für Volume und number noch als Bedingung, dass es nach Titel stehen muss, also Index > Titel-Index\n",
    "number = \"Issue \\d+\"\n",
    "yearApaHavard = \"\\(\\d{4}\\)\"\n",
    "editorApaHavard = \"\\((Eds\\.|Eds|Ed|ed|eds\\.)\\)\"\n",
    "\n",
    "#editor bei anderen\n",
    "editorIEEEMLAACM = \"(Eds\\.|Eds|Ed|ed|eds\\.)\"\n",
    "\n",
    "#Beachte: Editor und Edition werden oft mit gleichen Abkürzungen versehen, auch im gleichen Stil! Nur Edition wird oft\n",
    "#klein geschrieben. Daher zustzlich prüfen, ob NER das als Person erkennt.\n",
    "\n",
    "#Beachte: ieee und mla trennen mit , und nicht mit .\n",
    "#if ieee and mla\n",
    "volumeIeeeMla = \", vol\\. \\d+\" \n",
    "noIeeeMla = \", no\\. \\d+\"\n",
    "\n",
    "etAl = \"et al.\"\n",
    "\n",
    "yearACM = \"\\. \\d{4}\\.\"\n",
    "\n",
    "#Man sollte zunächst die Felder extrahieren, die zu 100% sicher erkennen kann wie Volume oder Number, doi, Autoren...-> \n",
    "#Titel, booktitle, series und journal danach\n",
    "\n",
    "#man sollte eine Überdeckungsprüfung machen: Also Bspw. sagt Space im Bereich 25-50 ist eine Orga, Regex sagt, \n",
    "#dort ist ein (Eds.)\n",
    "#zu finden und Huggingface sagt, von 25-45 ist was. Dann sollte der minimalteste Wert und maximalste Wert \n",
    "#genommen werden und so lange nach links und analog nach rechts gehen, bis wieder ein Punkt kommt.\n",
    "# Da erkannte Felder aus dem String entfernt werden, muss der String am Ende leer sein. \n",
    "#Daher sollten zunächste die Dinge ausgeschnitten werden wo der Algo sich am sichersten ist\n",
    "#TODO: Grundsätzliche Reihenfolge der BibTex-Einträge ermitteln.\n",
    "#TODO: Für jede Extrahierung eines Feldes eine eigene Unterroutine schreiben? -> In der Oberschleife sollte Style sein\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80320cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first white-space character is located in position: 25\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = \"Tpp. 250–300). Springer. https://doi.org/10.1007/springerreference_303198\"\n",
    "x = re.search(doi1, txt)\n",
    "startIndex = x.start()\n",
    "endIndex = x.end()\n",
    "\n",
    "print(\"The first white-space character is located in position:\", x.start()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e11e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "listRegEx = [pageFinder, doi]\n",
    "for regex in listRegEx:\n",
    "    x = re.search(regex, txt)\n",
    "    if x:\n",
    "        startIndex = x.start()\n",
    "        endIndex = x.end()\n",
    "        print(startIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd2124",
   "metadata": {},
   "source": [
    "Test Parser APA + Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8acde04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score         word  start  end\n",
      "0          PER  0.988260   S. Hawking      0   10\n",
      "1          PER  0.990276    B. Greene     12   21\n",
      "2          PER  0.990631     M. Trump     23   31\n",
      "3          PER  0.963512       S. Soy     36   42\n",
      "4          PER  0.996898  John Miller     74   85\n",
      "index: 0\n",
      "Vor- und Nachname als ein Wort\n",
      "index: 1\n",
      "Vor- und Nachname als ein Wort\n",
      "index: 2\n",
      "Vor- und Nachname als ein Wort\n",
      "index: 3\n",
      "Vor- und Nachname als ein Wort\n",
      "index: 4\n",
      "Vor- und Nachname als ein Wort\n",
      "onlyOneAuhtor: False\n",
      "indexEndLastAuthorAfterGap: 42\n",
      "authorsWithGap: True\n",
      "index: 3\n",
      "-------------------------------------------------------------------------------\n",
      "es liegen wahrscheinlich Autoren mit und vor\n",
      "es liegen höchstwahrscheinlich Autoren mit und vor\n",
      "indexAndInAuthors: 31\n",
      "IF authorWithAnd_check2: True\n",
      "index: 0\n",
      "index: 1\n",
      "index: 2\n",
      "index: 3\n",
      "df_PER after authorWithAnd_check2:   entity_group     score        word  start  end\n",
      "0          PER  0.988260  S. Hawking      0   10\n",
      "1          PER  0.990276   B. Greene     12   21\n",
      "2          PER  0.990631    M. Trump     23   31\n",
      "3          PER  0.963512      S. Soy     36   42\n",
      "-----------------------------------------------------------------------------------------\n",
      "  entity_group     score        word  start  end\n",
      "0          PER  0.988260  S. Hawking      0   10\n",
      "1          PER  0.990276   B. Greene     12   21\n",
      "2          PER  0.990631    M. Trump     23   31\n",
      "3          PER  0.963512      S. Soy     36   42\n",
      "authors after authorWithAnd: S. Hawking, B. Greene, M. Trump and S. Soy.\n",
      "authorsList: ['S. Hawking', ' B. Greene', ' M. Trump ', ' S. Soy.']\n",
      "allNamesInOneWord: True\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      " S. Hawking and B. Greene and M. Trump and S. Soy.}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text=\"S. Hawking, B. Greene, M. Trump and S. Soy. (2025). Advanced Concepts and John Miller in Theoretical Physics (3rd ed., Vol. 1). Cambridge University Press. https://doi.org/10.1017/example.book.2025\"\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "#index neu setzen, da diese nicht automatich geupdates werden\n",
    "df_PER = df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "df_PER_len = len(df_PER.index)\n",
    "\n",
    "\n",
    "print(df_PER)\n",
    "\n",
    "fullNameInOneWord = False\n",
    "allNamesInOneWord = True\n",
    "\n",
    "for index in df_PER.index.values.tolist():\n",
    "    print(\"index: {0}\".format(index))\n",
    "    if \" \" in df_PER[\"word\"].iloc[index].strip():\n",
    "        print(\"Vor- und Nachname als ein Wort\")\n",
    "        fullNameInOneWord = True\n",
    "        #dann wurd ein Vorname und Nachname als ein Wort erkannt, also bspw. \"John Miller\"\n",
    "    if \" \" not in df_PER[\"word\"].iloc[index].strip():\n",
    "        allNamesInOneWord = False\n",
    "\n",
    "\n",
    "onlyOneAuhtor = False \n",
    "if allNamesInOneWord and df_PER_len == 1:\n",
    "    onlyOneAuhtor = True\n",
    "elif df_PER_len == 2:\n",
    "    onlyOneAuhtor = True\n",
    "print(\"onlyOneAuhtor: {0}\".format(onlyOneAuhtor))\n",
    "\n",
    "if fullNameInOneWord and not allNamesInOneWord:\n",
    "    print(\"Das ist problematisch\")\n",
    "\n",
    "\n",
    "startIndexPER = df_PER[\"start\"].iloc[0]\n",
    "endIndexPER = df_PER[\"end\"].iloc[-1] + 1\n",
    "\n",
    "#prüfe, wo der letzte Autor in der Autorenkette ist, um Namen auszuschließen, die im Titel sein können\n",
    "#vorher prüfen, wo et al. steht, weil so erkennt man ende sicher\n",
    "#zusäzich kann man noch prüfen, wo das erst \"and\" oder \"&\" kommt\n",
    "authorsWithGap = False\n",
    "authorWithAnd_check1 = False\n",
    "authorWithAnd_check2 = False\n",
    "indexEndLastAuthorAfterGap = 0\n",
    "indexStartAuthorAfterGap = 0\n",
    "if not onlyOneAuhtor:\n",
    "    for index in df_PER.index.values.tolist():\n",
    "        if authorsWithGap:\n",
    "            indexStartAuthorAfterGap = df_PER[\"start\"].iloc[index]\n",
    "            indexEndLastAuthorAfterGap = df_PER[\"end\"].iloc[index] \n",
    "            authorsWithGap = True\n",
    "            print(\"indexEndLastAuthorAfterGap: {0}\".format(indexEndLastAuthorAfterGap))\n",
    "            print(\"authorsWithGap: {0}\".format(authorsWithGap))\n",
    "            print(\"index: {0}\".format(index))\n",
    "            print(\"-------------------------------------------------------------------------------\")\n",
    "            break\n",
    "        elif (df_PER[\"start\"].iloc[index + 1] - df_PER[\"end\"].iloc[index] > 2) \\\n",
    "        and (df_PER[\"start\"].iloc[index + 1] - df_PER[\"end\"].iloc[index] < 8):\n",
    "            authorsWithGap = True\n",
    "\n",
    "    authors = text[startIndexPER:endIndexPER]\n",
    "    indexAndInAuthors = authors.find(\" and \")\n",
    "    if indexAndInAuthors > 0:\n",
    "        if authors.find(\" & \") > 0 and indexAndInAuthors > authors.find(\" & \"):\n",
    "            indexAndInAuthors = authors.find(\" & \")\n",
    "\n",
    "        \n",
    "    if indexAndInAuthors > 0:\n",
    "        #es gibt irgendwo ein and zwischen Autoren\n",
    "        authorWithAnd_check1 = True\n",
    "        print(\"es liegen wahrscheinlich Autoren mit und vor\")\n",
    "        if indexAndInAuthors and authorsWithGap:\n",
    "            #das and liegt zwischen zwei Autoren in einem kleinem Bereich, der für gewöhnlich von einem and überdeckt wird\n",
    "            print(\"es liegen höchstwahrscheinlich Autoren mit und vor\")\n",
    "            authorWithAnd_check2 = True\n",
    "\n",
    "print(\"indexAndInAuthors: {0}\".format(indexAndInAuthors))            \n",
    "#Nun Prüfen, ob der start des letzten Autors direkt hinter dem Index vom and kommt\n",
    "if authorWithAnd_check2:\n",
    "    print(\"IF authorWithAnd_check2: {0}\".format(authorWithAnd_check2))\n",
    "    for index in df_PER.index.values.tolist():\n",
    "        print(\"index: {0}\".format(index))\n",
    "        if df_PER[\"end\"].iloc[index] > indexAndInAuthors:\n",
    "            df_PER = df_PER[df_PER[\"end\"] <= df_PER[\"end\"].iloc[index]]\n",
    "            print(\"df_PER after authorWithAnd_check2: {0}\".format(df_PER))\n",
    "            break\n",
    "elif authorWithAnd_check1:\n",
    "    for index in df_PER.index.values.tolist():\n",
    "        if df_PER[\"end\"].iloc[index] == indexEndLastAuthorAfterGap:\n",
    "            df_PER = df_PER[df_PER[\"end\"] <= df_PER[\"end\"].iloc[index]]\n",
    "            break\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------------------\") \n",
    "print(df_PER)\n",
    "startIndexPER = df_PER[\"start\"].iloc[0]\n",
    "endIndexPER = df_PER[\"end\"].iloc[-1] + 1\n",
    "authors = authors.replace(\", \",\",\")\n",
    "#wenn countKomma == df_PER_len - 1, dann liegt sowas vor: Hawking, S., Greene, B., Trump, M., & Soy, S.\n",
    "\n",
    "authors = text[startIndexPER:endIndexPER]\n",
    "print(\"authors after authorWithAnd: {0}\".format(authors))\n",
    "\n",
    "\n",
    "countKomma = authors.count(\",\")\n",
    "authors = authors.replace(\"&\", \"\")\n",
    "authors = authors.replace(\"and\", \",\")\n",
    "authors = authors.replace(\"et al.\", \"\")\n",
    "authorsList = authors.split(\",\") #hier aufpassen, wenn Vor- und Nachname im gleichen Wort sind\n",
    "\n",
    "surenameFirst = authorsList[0].strip().endswith(\".\") #Dann stehen Vornamen wohl zuerst. Hier muss man die Wörte aber \n",
    "#vorher gesplittet haben\n",
    "\n",
    "print(\"authorsList: {0}\".format(authorsList))\n",
    "\n",
    "maxRangeIndex = len(authorsList)\n",
    "stepSize = 2\n",
    "if allNamesInOneWord:\n",
    "    stepSize = 1\n",
    "\n",
    "print(\"allNamesInOneWord: {0}\".format(allNamesInOneWord))\n",
    "authors = \"\"\n",
    "if allNamesInOneWord:\n",
    "    for i in range(0, maxRangeIndex , stepSize):\n",
    "        authors = authors + \" \" + authorsList[i].strip()\n",
    "        print(i)\n",
    "        if i < (maxRangeIndex - stepSize):\n",
    "            authors = authors + \" and\"\n",
    "    authors = authors + \"}\"\n",
    "else:\n",
    "    if not surenameFirst:\n",
    "        for i in range(0, maxRangeIndex , stepSize):\n",
    "            authorsList[i], authorsList[i+1] = authorsList[i+1].strip(), authorsList[i].strip()\n",
    "    authors = \"author = {\"\n",
    "    for i in range(0, maxRangeIndex , stepSize):\n",
    "        authors = authors + \" \" + authorsList[i] + \" \" + authorsList[i+1]\n",
    "        print(i)\n",
    "        if i < (maxRangeIndex - stepSize):\n",
    "            authors = authors + \" and\"\n",
    "    authors = authors + \"}\"\n",
    "print(authors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1358ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
