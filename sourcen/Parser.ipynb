{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wenn man den typen bestimmte hat, sollte man zunächst alle Pflicht-Felder des Bibtex-Eintrages raussuchen und bestimmen.\n",
    "#Dazu die Pflichtattribute in einer Schleife durchgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bde3f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75bbdc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text=\"Hinton, G., Bengio, Y. and LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "df_PER = df_outputs[df_outputs[\"entity_group\"] == \"PER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b9bb15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.996079</td>\n",
       "      <td>Hinton</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998951</td>\n",
       "      <td>G</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.994906</td>\n",
       "      <td>Bengio</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998544</td>\n",
       "      <td>Y</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.963430</td>\n",
       "      <td>LeCun</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.998110</td>\n",
       "      <td>Y</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.949492</td>\n",
       "      <td>J. Smith</td>\n",
       "      <td>91</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.992646</td>\n",
       "      <td>J. Doe</td>\n",
       "      <td>102</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score      word  start  end\n",
       "0          PER  0.996079    Hinton      0    6\n",
       "1          PER  0.998951         G      8    9\n",
       "2          PER  0.994906    Bengio     12   18\n",
       "3          PER  0.998544         Y     20   21\n",
       "4          PER  0.963430     LeCun     27   32\n",
       "5          PER  0.998110         Y     34   35\n",
       "8          PER  0.949492  J. Smith     91   99\n",
       "9          PER  0.992646    J. Doe    102  108"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e9f3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "startIndexPER = df_PER[\"start\"].iloc[0]\n",
    "endIndexPER = df_PER[\"end\"].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2df9df5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1\n",
      "B    4\n",
      "C    7\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispiel DataFrame erstellen\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "})\n",
    "\n",
    "# Zugriff auf die erste Zeile\n",
    "erste_zeile = df.iloc[0]\n",
    "print(erste_zeile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daa1be46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinton 0 6 GPE\n",
      "G. 8 10 GPE\n",
      "Bengio 12 18 PERSON\n",
      "Y. 20 22 PERSON\n",
      "Y. 34 36 PERSON\n",
      "2021 38 42 DATE\n",
      "Deep Learning for Artificial Intelligence 0 41 ORG\n",
      "J. Smith & J. Doe (Eds 3 25 ORG\n",
      "Proceedings of the IEEE International Conference on Neural Networks 29 96 ORG\n",
      "1 103 104 CARDINAL\n",
      "5 112 113 DATE\n",
      "100–120 119 126 CARDINAL\n",
      "IEEE Press 0 10 ORG\n",
      "https://doi.org/10.1109/ICNN.2021.9483948 0 41 GPE\n"
     ]
    }
   ],
   "source": [
    "#Test, um Datum Zuverlässig erkannt wird\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hinton, G., Bengio, Y. and LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\")\n",
    "for ent in doc.ents:\n",
    "  print(ent, ent.start_char-ent.sent.start_char, ent.end_char-ent.sent.start_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55d2b1",
   "metadata": {},
   "source": [
    "@book{advanced_physics, author = {Stephen Hawking and Brian Greene and Maria Trump and Susan Soy}, title = {Advanced Concepts in Theoretical Physics}, publisher = {Cambridge University Press}, year = {2025}, edition = {3}, volume = {1}, series = {Advanced Studies in Physics}, address = {Cambridge, UK}, month = {May}, note = {A seminal work in the field of theoretical physics}, isbn = {978-0-521-76948-0}, doi = {10.1017/example.book.2025}, url = {https://www.cambridge.org/advanced_physics}, annote = {Widely cited in the physics community}, abstract = {This book explores cutting-edge theories and concepts in theoretical physics...}, keywords = {Theoretical Physics, Quantum Mechanics, String Theory}, language = {English}, price = {75.00}, size = {600 pages}, lccn = {2020934576}, mrnumber = {MR3070071} }\n",
    "\n",
    "@article{quantum_entanglement,\n",
    "  author        = {Albert Einstein and Boris Podolsky and Nathan Rosen},\n",
    "  title         = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},\n",
    "  journal       = {Physical Review},\n",
    "  year          = {1935},\n",
    "  volume        = {47},\n",
    "  number        = {10},\n",
    "  pages         = {777-780},\n",
    "  month         = {May},\n",
    "  note          = {EPR Paradox paper, fundamental for quantum mechanics},\n",
    "  doi           = {10.1103/PhysRev.47.777},\n",
    "  url           = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},\n",
    "  abstract      = {In this paper, the authors discuss the EPR paradox and challenge the completeness of quantum mechanics...},\n",
    "  keywords      = {Quantum Mechanics, EPR Paradox, Physical Reality},\n",
    "  language      = {English},\n",
    "  publisher     = {American Physical Society}\n",
    "}\n",
    "\n",
    "@inproceedings{deep_learning,\n",
    "  author        = {Geoffrey Hinton and Yoshua Bengio and Yann LeCun},\n",
    "  title         = {Deep Learning for Artificial Intelligence},\n",
    "  booktitle     = {Proceedings of the IEEE International Conference on Neural Networks},\n",
    "  year          = {2021},\n",
    "  editor        = {Jane Smith and John Doe},\n",
    "  volume        = {1},\n",
    "  number = {5},\n",
    "  series        = {Advances in Neural Information Processing},\n",
    "  pages         = {100-120},\n",
    "  address       = {Montreal, Canada},\n",
    "  month         = {December},\n",
    "  organization  = {IEEE},\n",
    "  publisher     = {IEEE Press},\n",
    "  note          = {Keynote paper on recent advancements in deep learning},\n",
    "  isbn          = {978-1-5386-4637-1},\n",
    "  doi           = {10.1109/ICNN.2021.9483948},\n",
    "  url           = {https://ieeexplore.ieee.org/document/9483948},\n",
    "  annote        = {A seminal work on how deep learning transforms AI},\n",
    "  abstract      = {This paper explores cutting-edge deep learning techniques and their impact on the development of artificial intelligence...},\n",
    "  keywords      = {Deep Learning, Artificial Intelligence, Neural Networks},\n",
    "  language      = {English}\n",
    "}\n",
    "\n",
    "\n",
    "@incollection{quantum_computation,\n",
    "  author        = {Michael A. Nielsen and Isaac L. Chuang},\n",
    "  title         = {Quantum Computation and Quantum Information},\n",
    "  booktitle     = {Handbook of Quantum Information Science},\n",
    "  publisher     = {Springer},\n",
    "  year          = {2026},\n",
    "  editor        = {Charles H. Bennett and David P. DiVincenzo},\n",
    "  volume        = {4},\n",
    "  series        = {Quantum Science and Technology},\n",
    "  chapter       = {10},\n",
    "  pages         = {250-300},\n",
    "  address       = {Berlin, Germany},\n",
    "  month         = {October},\n",
    "  note          = {A comprehensive overview of the fundamentals of quantum computation},\n",
    "  isbn          = {978-3-540-88702-7},\n",
    "  doi           = {10.1007/springerreference_303198},\n",
    "  url           = {https://www.springer.com/gp/book/9783540887027},\n",
    "  annote        = {Essential reading for researchers entering the field of quantum information},\n",
    "  abstract      = {This chapter delves into the principles of quantum computing, offering an accessible yet thorough introduction...},\n",
    "  keywords      = {Quantum Computing, Quantum Information, Computational Models},\n",
    "  language      = {English},\n",
    "  price         = {45.00},\n",
    "  size          = {50 pages}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b93a7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RegEx zum FInden der Seiten\n",
    "pageFinder = \", (?:pp\\.? )?\\d+-\\d+,\"\n",
    "doi1 = \"https:\\/\\/doi\\.org\"\n",
    "doi2 = \"DOI:\"\n",
    "\n",
    "#if havard und book:\n",
    "volume = \", \\d+,\"\n",
    "number = \", \\d+,\"\n",
    "edition1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "edition2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "\n",
    "#if APA or Havard then\n",
    "volume = \"\\(Vol\\. \\d+\" #für Volume und number noch als Bedingung, dass es nach Titel stehen muss, also Index > Titel-Index\n",
    "number = \"Issue \\d+\"\n",
    "yearApaHavard = \"\\(\\d{4}\\)\"\n",
    "editorApaHavard = \" (\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors|edited by )(\\))?\"\n",
    "\n",
    "#editor bei anderen\n",
    "\n",
    "\n",
    "#Beachte: Editor und Edition werden oft mit gleichen Abkürzungen versehen, auch im gleichen Stil! Nur Edition wird oft\n",
    "#klein geschrieben. Daher zustzlich prüfen, ob NER das als Person erkennt.\n",
    "\n",
    "#Beachte: ieee und mla trennen mit , und nicht mit .\n",
    "#if ieee and mla\n",
    "volumeIeeeMla = \", vol\\. \\d+\" \n",
    "noIeeeMla = \", no\\. \\d+\"\n",
    "\n",
    "etAl = \"et al.\"\n",
    "\n",
    "yearACM = \"\\. \\d{4}\\.\"\n",
    "\n",
    "#Man sollte zunächst die Felder extrahieren, die zu 100% sicher erkennen kann wie Volume oder Number, doi, Autoren...-> \n",
    "#Titel, booktitle, series und journal danach\n",
    "\n",
    "#man sollte eine Überdeckungsprüfung machen: Also Bspw. sagt Space im Bereich 25-50 ist eine Orga, Regex sagt, \n",
    "#dort ist ein (Eds.)\n",
    "#zu finden und Huggingface sagt, von 25-45 ist was. Dann sollte der minimalteste Wert und maximalste Wert \n",
    "#genommen werden und so lange nach links und analog nach rechts gehen, bis wieder ein Punkt kommt.\n",
    "# Da erkannte Felder aus dem String entfernt werden, muss der String am Ende leer sein. \n",
    "#Daher sollten zunächste die Dinge ausgeschnitten werden wo der Algo sich am sichersten ist\n",
    "#TODO: Grundsätzliche Reihenfolge der BibTex-Einträge ermitteln.\n",
    "#TODO: Für jede Extrahierung eines Feldes eine eigene Unterroutine schreiben? -> In der Oberschleife sollte Style sein\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80320cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first white-space character is located in position: 25\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt = \"Tpp. 250–300). Springer. https://doi.org/10.1007/springerreference_303198\"\n",
    "x = re.search(doi1, txt)\n",
    "startIndex = x.start()\n",
    "endIndex = x.end()\n",
    "\n",
    "print(\"The first white-space character is located in position:\", x.start()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e11e167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "listRegEx = [pageFinder, doi1]\n",
    "for regex in listRegEx:\n",
    "    x = re.search(regex, txt)\n",
    "    if x:\n",
    "        startIndex = x.start()\n",
    "        endIndex = x.end()\n",
    "        print(startIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cd2124",
   "metadata": {},
   "source": [
    "Test Parser APA + Book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8acde04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score        word  start  end\n",
      "0          PER  0.999442       Singh      0    5\n",
      "1          PER  0.998186           S      7    8\n",
      "2          PER  0.998922           K     10   11\n",
      "3          PER  0.987951           E     39   40\n",
      "4          PER  0.983496  F. Johnson     42   52\n",
      "index: 0\n",
      "df_PER[\"word\"].iloc[index].strip(): Singh\n",
      "index: 1\n",
      "df_PER[\"word\"].iloc[index].strip(): S\n",
      "nameAbgekürzt: True\n",
      "index: 2\n",
      "df_PER[\"word\"].iloc[index].strip(): K\n",
      "nameAbgekürzt: True\n",
      "index: 3\n",
      "df_PER[\"word\"].iloc[index].strip(): E\n",
      "nameAbgekürzt: True\n",
      "index: 4\n",
      "Vor- und Nachname als ein Wort\n",
      "Das ist problematisch\n",
      "Fall 3\n",
      "authorsList: ['Singh', ' S. K. (2023). Chapter Title. In E. F. Johnson ']\n",
      "allFullNamesInOneWord: False\n",
      "0\n",
      "author = { S. K. (2023). Chapter Title. In E. F. Johnson Singh}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text=\"Hawking, S., Greene, B., Trump, M. and Soy, S. (2025). Advanced Concepts and John Miller in Theoretical Physics (3rd ed., Vol. 1). Cambridge University Press. https://doi.org/10.1017/example.book.2025\"\n",
    "text = \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chat GPT & Google Bard AI: A Review. 2023 International Conference on IoT, Communication and Automation Technology (ICICAT), 1–6. https://doi.org/10.1109/ICICAT57735.2023.10263706\"\n",
    "text = \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chapter Title. In E. F. Johnson & R. H. Lee (Eds.), Book Title. Publisher Name.\"\n",
    "text = \"Singh, S. K. (2023). Chapter Title. In E. F. Johnson (Ed.), Book Title. Publisher Name.\"\n",
    "\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "#index neu setzen, da diese nicht automatich geupdates werden\n",
    "df_PER = df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "df_PER_len = len(df_PER.index)\n",
    "\n",
    "\n",
    "print(df_PER)\n",
    "\n",
    "fullNameInOneWord = False\n",
    "allFullNamesInOneWord = True\n",
    "nameAbgekürzt = False\n",
    "surenameFirst = False\n",
    "\n",
    "for index in df_PER.index.values.tolist():\n",
    "    print(\"index: {0}\".format(index))\n",
    "    if \" \" in df_PER[\"word\"].iloc[index].strip():\n",
    "        print(\"Vor- und Nachname als ein Wort\")\n",
    "        fullNameInOneWord = True\n",
    "        #dann wurd ein Vorname und Nachname als ein Wort erkannt, also bspw. \"John Miller\"\n",
    "    if \" \" not in df_PER[\"word\"].iloc[index].strip():\n",
    "        print(\"df_PER[\\\"word\\\"].iloc[index].strip(): {0}\".format(df_PER[\"word\"].iloc[index].strip()))\n",
    "        allFullNamesInOneWord = False\n",
    "    if \".\" in text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "        nameAbgekürzt = True\n",
    "        if index == 0:\n",
    "            surenameFirst = True\n",
    "        print(\"nameAbgekürzt: {0}\".format(nameAbgekürzt))\n",
    "\n",
    "# dann gehe ich davon aus, dass sie schon in richtiger Reihenfolge liegen, da ich Nach- und Vorname nicht unterscheiden kann\n",
    "if allFullNamesInOneWord:\n",
    "    surenameFirst = True\n",
    "\n",
    "editorRegEx = \" (\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\"\n",
    "startIndexEditorMarker = -1\n",
    "if re.search(editorRegEx, txt):\n",
    "    startIndexEditorMarker = x.start()\n",
    "\n",
    "\n",
    "\n",
    "if fullNameInOneWord and not allFullNamesInOneWord:\n",
    "    print(\"Das ist problematisch\")\n",
    "\n",
    "\n",
    "startIndexPER = df_PER[\"start\"].iloc[0]\n",
    "endIndexPER = df_PER[\"end\"].iloc[-1] + 1\n",
    "#Beachte, ein ., auch zwischen Autoren und Titel vorkommen kann\n",
    "authors = text[startIndexPER:endIndexPER]\n",
    "andInAuthors = False\n",
    "dotKommaInAuthors = False\n",
    "firstStartIndexAnd = -1\n",
    "firstEndIndexAnd = -1\n",
    "search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "\n",
    "if \"et al.\" not in text:\n",
    "    firstStartIndexAnd, firstEndIndexAnd = find_First_And_index(authors, search_terms)\n",
    "    if firstStartIndexAnd > -1:\n",
    "        andInAuthors = True\n",
    "else:\n",
    "    print(\"et. al\")\n",
    "\n",
    "if not surenameFirst:\n",
    "    if authors.find(\".,\") > -1:\n",
    "        dotKommaInAuthors = True\n",
    "\n",
    "\n",
    "#prüfe, wo der letzte Autor in der Autorenkette ist, um Namen auszuschließen, die im Titel sein können\n",
    "#vorher prüfen, wo et al. steht, weil so erkennt man ende sicher\n",
    "#zusäzich kann man noch prüfen, wo das erst \"and\" oder \"&\" kommt\n",
    "\n",
    "\n",
    "index_df_PER_List = df_PER.index.values.tolist()\n",
    "\n",
    "distanceAuthorNames = 0\n",
    "maxDistanceAuthorNames = 0\n",
    "startDistanceAuthorNames = 0\n",
    "endDistanceAuthorNames = 0\n",
    "authorSeparatedByAnd = False\n",
    "possibleEditors = False\n",
    "\n",
    "firstStartIndexAnd = -1\n",
    "firstEndIndexAnd = -1\n",
    "listIndicesAnd = []\n",
    "for index in index_df_PER_List[:-1]:\n",
    "    textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "    firstStartIndexAnd, firstEndIndexAnd = find_First_And_index(textBetweenNames, search_terms)\n",
    "    distanceAuthorNames = df_PER[\"start\"].iloc[index + 1] - df_PER[\"end\"].iloc[index]\n",
    "    if firstStartIndexAnd > -1:\n",
    "        listIndicesAnd.append([df_PER[\"start\"].iloc[index + 1],df_PER[\"end\"].iloc[index]])\n",
    "    if distanceAuthorNames > maxDistanceAuthorNames:\n",
    "        maxDistanceAuthorNames = distanceAuthorNames\n",
    "        #die endDistanceAuthorNames braucht man, um dann den STart der Editors zu finden\n",
    "        endDistanceAuthorNames = df_PER[\"start\"].iloc[index + 1]\n",
    "        startDistanceAuthorNames = df_PER[\"end\"].iloc[index]\n",
    "\n",
    "if len(listIndicesAnd) > 0:\n",
    "    authorSeparatedByAnd = True\n",
    "    for element in listIndicesAnd:\n",
    "        if maxDistanceAuthorNames > (element[0] - element[1]):\n",
    "            #Dann gibt es zwischen den Namen von Autoren eine größere Lücke als ein And. And muss aber vorhanden sein.\n",
    "            #endDistanceAuthorNames und startDistanceAuthorNames zeigen dann Range der Lücke an\n",
    "            possibleEditors = True\n",
    "else:\n",
    "    # Dann kann der maximalste Abstand \"., \" sein, wenn keine Autoren als Editor folden\n",
    "    if maxDistanceAuthorNames > 3:\n",
    "        possibleEditors = True\n",
    "\n",
    "#wenn possibleEditors == True, dann Trennung Autoren und Editoren bei startDistanceAuthorNames\n",
    "if possibleEditors and authorSeparatedByAnd: \n",
    "    if startDistanceAuthorNames < listIndicesAnd[0][1]:\n",
    "        print(\"Fall 1\")\n",
    "    #startDistanceAuthorNames < listIndicesAnd[0]: Dann gibt es nur ein and bei den Editors. Damit liegt nur ein Autor vor\n",
    "    else:\n",
    "        if len(listIndicesAnd) > 1:\n",
    "        #dann muss es ein und zwischen den Autoren und eins zwischen den Editorengeben\n",
    "            print(\"Fall 2.1\")\n",
    "        else:\n",
    "        #dann gibt es nur ein und bei den Autoren\n",
    "            print(\"Fall 2.2\")  \n",
    "elif possibleEditors and not authorSeparatedByAnd:\n",
    "    #dann gibt es nur einen Autor und einen Editor\n",
    "    print(\"Fall 3\")\n",
    "elif not possibleEditors and not authorSeparatedByAnd:\n",
    "    #dann gibt es nur einen Autor\n",
    "    print(\"Fall 4\")\n",
    "elif not possibleEditors and authorSeparatedByAnd:\n",
    "    #dann gibt es mehrere Autoren, aber keine Editoren\n",
    "    print(\"Fall 5\")\n",
    "    \n",
    "\n",
    "if surenameFirst:\n",
    "    dummy = 0\n",
    "    countKomma = authors.count(\",\")\n",
    "\n",
    "\n",
    "countKomma = authors.count(\",\")\n",
    "authors = authors.replace(\"&\", \"\")\n",
    "authors = authors.replace(\"and\", \",\")\n",
    "authors = authors.replace(\"et al.\", \"\")\n",
    "authorsList = authors.split(\",\") #hier aufpassen, wenn Vor- und Nachname im gleichen Wort sind\n",
    "\n",
    "#vorher gesplittet haben\n",
    "\n",
    "print(\"authorsList: {0}\".format(authorsList))\n",
    "\n",
    "maxRangeIndex = len(authorsList)\n",
    "stepSize = 2\n",
    "if allFullNamesInOneWord:\n",
    "    stepSize = 1\n",
    "\n",
    "print(\"allFullNamesInOneWord: {0}\".format(allFullNamesInOneWord))\n",
    "authors = \"\"\n",
    "if allFullNamesInOneWord:\n",
    "    for i in range(0, maxRangeIndex , stepSize):\n",
    "        authors = authors + \" \" + authorsList[i].strip()\n",
    "        print(i)\n",
    "        if i < (maxRangeIndex - stepSize):\n",
    "            authors = authors + \" and\"\n",
    "    authors = authors + \"}\"\n",
    "else:\n",
    "    if not surenameFirst:\n",
    "        for i in range(0, maxRangeIndex , stepSize):\n",
    "            authorsList[i], authorsList[i+1] = authorsList[i+1].strip(), authorsList[i].strip()\n",
    "    authors = \"author = {\"\n",
    "    for i in range(0, maxRangeIndex , stepSize):\n",
    "        authors = authors + \" \" + authorsList[i] + \" \" + authorsList[i+1]\n",
    "        print(i)\n",
    "        if i < (maxRangeIndex - stepSize):\n",
    "            authors = authors + \" and\"\n",
    "    authors = authors + \"}\"\n",
    "print(authors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1358ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "print(len(\"Hawking, S., Greene, B., Trump, M.,  Soy., S.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b180b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_First_And_index(text, search_terms):\n",
    "    # Initialisiere mit einem hohen Wert\n",
    "    min_index = float('inf')\n",
    "    end_index = 0\n",
    "    \n",
    "    # Suche jeden Suchbegriff in dem Text und behalte den kleinsten Index\n",
    "    for term in search_terms:\n",
    "        index = text.find(term)\n",
    "        if index != -1 and index < min_index:\n",
    "            min_index = index\n",
    "            end_index = min_index + len(term) - 1\n",
    "    \n",
    "    # Wenn min_index unverändert ist, wurde keiner der Begriffe gefunden\n",
    "    return (min_index, end_index) if min_index != float('inf') else (-1, -1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae188dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
