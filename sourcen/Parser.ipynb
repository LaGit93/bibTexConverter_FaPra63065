{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ef1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wenn man den typen bestimmte hat, sollte man zunächst alle Pflicht-Felder des Bibtex-Eintrages raussuchen und bestimmen.\n",
    "#Dazu die Pflichtattribute in einer Schleife durchgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bde3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "torch.cuda.is_available()\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55d2b1",
   "metadata": {},
   "source": [
    "@book{advanced_physics, author = {Stephen Hawking and Brian Greene and Maria Trump and Susan Soy}, title = {Advanced Concepts in Theoretical Physics}, publisher = {Cambridge University Press}, year = {2025}, edition = {3}, volume = {1}, series = {Advanced Studies in Physics}, address = {Cambridge, UK}, month = {May}, note = {A seminal work in the field of theoretical physics}, isbn = {978-0-521-76948-0}, doi = {10.1017/example.book.2025}, url = {https://www.cambridge.org/advanced_physics}, annote = {Widely cited in the physics community}, abstract = {This book explores cutting-edge theories and concepts in theoretical physics...}, keywords = {Theoretical Physics, Quantum Mechanics, String Theory}, language = {English}, price = {75.00}, size = {600 pages}, lccn = {2020934576}, mrnumber = {MR3070071} }\n",
    "\n",
    "@article{quantum_entanglement,\n",
    "  author        = {Albert Einstein and Boris Podolsky and Nathan Rosen},\n",
    "  title         = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},\n",
    "  journal       = {Physical Review},\n",
    "  year          = {1935},\n",
    "  volume        = {47},\n",
    "  number        = {10},\n",
    "  pages         = {777-780},\n",
    "  month         = {May},\n",
    "  note          = {EPR Paradox paper, fundamental for quantum mechanics},\n",
    "  doi           = {10.1103/PhysRev.47.777},\n",
    "  url           = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},\n",
    "  abstract      = {In this paper, the authors discuss the EPR paradox and challenge the completeness of quantum mechanics...},\n",
    "  keywords      = {Quantum Mechanics, EPR Paradox, Physical Reality},\n",
    "  language      = {English},\n",
    "  publisher     = {American Physical Society}\n",
    "}\n",
    "\n",
    "@inproceedings{deep_learning,\n",
    "  author        = {Geoffrey Hinton and Yoshua Bengio and Yann LeCun},\n",
    "  title         = {Deep Learning for Artificial Intelligence},\n",
    "  booktitle     = {Proceedings of the IEEE International Conference on Neural Networks},\n",
    "  year          = {2021},\n",
    "  editor        = {Jane Smith and John Doe},\n",
    "  volume        = {1},\n",
    "  number = {5},\n",
    "  series        = {Advances in Neural Information Processing},\n",
    "  pages         = {100-120},\n",
    "  address       = {Montreal, Canada},\n",
    "  month         = {December},\n",
    "  organization  = {IEEE},\n",
    "  publisher     = {IEEE Press},\n",
    "  note          = {Keynote paper on recent advancements in deep learning},\n",
    "  isbn          = {978-1-5386-4637-1},\n",
    "  doi           = {10.1109/ICNN.2021.9483948},\n",
    "  url           = {https://ieeexplore.ieee.org/document/9483948},\n",
    "  annote        = {A seminal work on how deep learning transforms AI},\n",
    "  abstract      = {This paper explores cutting-edge deep learning techniques and their impact on the development of artificial intelligence...},\n",
    "  keywords      = {Deep Learning, Artificial Intelligence, Neural Networks},\n",
    "  language      = {English}\n",
    "}\n",
    "\n",
    "\n",
    "@incollection{quantum_computation,\n",
    "  author        = {Michael A. Nielsen and Isaac L. Chuang},\n",
    "  title         = {Quantum Computation and Quantum Information},\n",
    "  booktitle     = {Handbook of Quantum Information Science},\n",
    "  publisher     = {Springer},\n",
    "  year          = {2026},\n",
    "  editor        = {Charles H. Bennett and David P. DiVincenzo},\n",
    "  volume        = {4},\n",
    "  series        = {Quantum Science and Technology},\n",
    "  chapter       = {10},\n",
    "  pages         = {250-300},\n",
    "  address       = {Berlin, Germany},\n",
    "  month         = {October},\n",
    "  note          = {A comprehensive overview of the fundamentals of quantum computation},\n",
    "  isbn          = {978-3-540-88702-7},\n",
    "  doi           = {10.1007/springerreference_303198},\n",
    "  url           = {https://www.springer.com/gp/book/9783540887027},\n",
    "  annote        = {Essential reading for researchers entering the field of quantum information},\n",
    "  abstract      = {This chapter delves into the principles of quantum computing, offering an accessible yet thorough introduction...},\n",
    "  keywords      = {Quantum Computing, Quantum Information, Computational Models},\n",
    "  language      = {English},\n",
    "  price         = {45.00},\n",
    "  size          = {50 pages}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93a7979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(93, 100), match=' (Eds.)'>\n"
     ]
    }
   ],
   "source": [
    "#RegEx zum FInden der Seiten\n",
    "pageFinder = \", (?:pp\\.? )?\\d+-\\d+,\"\n",
    "doi1 = \"https:\\/\\/doi\\.org\"\n",
    "doi2 = \"DOI:\"\n",
    "\n",
    "#if havard und book:\n",
    "volume = \", \\d+,\"\n",
    "number = \", \\d+,\"\n",
    "edition1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "edition2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "\n",
    "#if APA or Havard then\n",
    "volume = \"\\(Vol\\. \\d+\" #für Volume und number noch als Bedingung, dass es nach Titel stehen muss, also Index > Titel-Index\n",
    "number = \"Issue \\d+\"\n",
    "yearApaHavard = \"\\(\\d{4}\\)\"\n",
    "editorRegEx = \" (\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors|edited by )(\\))?\"\n",
    "\n",
    "#editor bei anderen\n",
    "\n",
    "\n",
    "#Beachte: Editor und Edition werden oft mit gleichen Abkürzungen versehen, auch im gleichen Stil! Nur Edition wird oft\n",
    "#klein geschrieben. Daher zustzlich prüfen, ob NER das als Person erkennt.\n",
    "\n",
    "#Beachte: ieee und mla trennen mit , und nicht mit .\n",
    "#if ieee and mla\n",
    "volumeIeeeMla = \", vol\\. \\d+\" \n",
    "noIeeeMla = \", no\\. \\d+\"\n",
    "\n",
    "etAl = \"et al.\"\n",
    "\n",
    "yearACM = \"\\. \\d{4}\\.\"\n",
    "\n",
    "#Man sollte zunächst die Felder extrahieren, die zu 100% sicher erkennen kann wie Volume oder Number, doi, Autoren...-> \n",
    "#Titel, booktitle, series und journal danach\n",
    "\n",
    "#man sollte eine Überdeckungsprüfung machen: Also Bspw. sagt Space im Bereich 25-50 ist eine Orga, Regex sagt, \n",
    "#dort ist ein (Eds.)\n",
    "#zu finden und Huggingface sagt, von 25-45 ist was. Dann sollte der minimalteste Wert und maximalste Wert \n",
    "#genommen werden und so lange nach links und analog nach rechts gehen, bis wieder ein Punkt kommt.\n",
    "# Da erkannte Felder aus dem String entfernt werden, muss der String am Ende leer sein. \n",
    "#Daher sollten zunächste die Dinge ausgeschnitten werden wo der Algo sich am sichersten ist\n",
    "#TODO: Grundsätzliche Reihenfolge der BibTex-Einträge ermitteln.\n",
    "#TODO: Für jede Extrahierung eines Feldes eine eigene Unterroutine schreiben? -> In der Oberschleife sollte Style sein\n",
    "\n",
    "\n",
    "\n",
    "print(re.search(editorRegEx, \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chapter Title. In Johnson, E. F. & Lee, R. H. (Eds.), Book\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ddfd2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hawking, S., Greene, B., Trump, M. and Soy, S. (2025). Advanced Concepts and John Miller in Theoretical Physics (3rd ed., Vol. 1). Cambridge University Press. https://doi.org/10.1017/example.book.2025\"\n",
    "text = \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chat GPT & Google Bard AI: A Review. 2023 International Conference on IoT, Communication and Automation Technology (ICICAT), 1–6. https://doi.org/10.1109/ICICAT57735.2023.10263706\"\n",
    "text = \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chapter Daniel Franklin Title. In Johnson, E. F. & Lee, R. H. (Eds.), Book Title. Publisher Name.\"\n",
    "#text = \"Singh, S. K. and Peterman, P. (2023). Chapter and Title. Trump, M. and Soy, S. Book Title. Publisher Name.\"\n",
    "text = \"S. K. Singh, S. Kumar & P. S. Mehra, (2023). Daniel Franklin in Wonderland. In F. Johnson & L. Lee (Eds.), Book Title. Publisher Name.\"\n",
    "#text = \"Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoît Sagot, Abdelrahman Mohamed, and Emmanuel Dupoux. 2023. Generative spoken dialogue language modeling. Transactions of the Association for Computational Linguistics 11, (2023), 250–266. URL: https://aclanthology.org/2023.tacl-1.15, doi:10.1162/tacl_a_00545\"\n",
    "#text = \"Hinton G, Bengio Y, LeCun Y (2021) Deep Learning for Artificial Intelligence. In: Smith J, Doe J (eds) Proceedings of the IEEE International Conference on Neural Networks. IEEE Press, Montreal, Canada, pp 100–120\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8acde04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score             word  start  end\n",
      "0          PER  0.999326                S      0    1\n",
      "1          PER  0.905312         K. Singh      3   11\n",
      "2          PER  0.976157         S. Kumar     13   21\n",
      "3          PER  0.999140                P     24   25\n",
      "4          PER  0.918666         S. Mehra     27   35\n",
      "5          PER  0.565301  Daniel Franklin     45   60\n",
      "textBetweenNames: . \n",
      "Author: S\n",
      "onlyPunctuation: True\n",
      "onlyAnd: False\n",
      "nothing: False\n",
      "textBetweenNames: , \n",
      "Author: K. Singh\n",
      "onlyPunctuation: True\n",
      "onlyAnd: False\n",
      "nothing: False\n",
      "textBetweenNames:  & \n",
      "Author: S. Kumar\n",
      "onlyPunctuation: False\n",
      "onlyAnd: True\n",
      "nothing: False\n",
      "textBetweenNames: . \n",
      "Author: P\n",
      "onlyPunctuation: True\n",
      "onlyAnd: False\n",
      "nothing: False\n",
      "textBetweenNames: , (2023). \n",
      "Author: S. Mehra\n",
      "onlyPunctuation: False\n",
      "onlyAnd: False\n",
      "nothing: True\n",
      "authorsDetected: True\n",
      "textBetweenNames:  in Wonderland. In F. Johnson & L. Lee (Eds.), Book Title. Publisher Name.\n",
      "Author: Daniel Franklin\n",
      "onlyPunctuation: False\n",
      "onlyAnd: False\n",
      "nothing: True\n",
      "authorsDetected: True\n",
      "surenameFirst: True\n",
      "authors: S. K. Singh, S. Kumar and P. S. Mehra\n",
      "finalAuthors: S. K. Singh and S. Kumar and P. S. Mehra\n",
      "finalEditors: F. Johnson and L. Lee\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def find_First_Term(text, search_terms):\n",
    "    # Initialisiere mit einem hohen Wert\n",
    "    min_index = float('inf')\n",
    "    end_index = 0\n",
    "    andTyp = \"\"\n",
    "    \n",
    "    # Suche jeden Suchbegriff in dem Text und behalte den kleinsten Index\n",
    "    for term in search_terms:\n",
    "        index = text.find(term)\n",
    "        if index != -1 and index < min_index:\n",
    "            min_index = index\n",
    "            end_index = min_index + len(term) - 1\n",
    "            andTyp = term\n",
    "    \n",
    "    # Wenn min_index unverändert ist, wurde keiner der Begriffe gefunden\n",
    "    return (min_index, end_index, andTyp) if min_index != float('inf') else (-1, -1, \"\")\n",
    "\n",
    "def is_SurenameFirst(names):\n",
    "    if re.search(r'^(\\w+\\.)', names) or re.search(r'^(\\w+\\s\\w+(.,)?)+$', names):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def is_NameShortened(df_PER):\n",
    "    for index in df_PER.index.values.tolist():\n",
    "        if \".\" == text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getIndexOfSubstring(regEx, text):\n",
    "    if re.search(regEx, text):\n",
    "        x = re.search(editorRegEx, text)\n",
    "        return (x.start(), x.end())\n",
    "    else:\n",
    "        return(-1,-1)\n",
    "\n",
    "def is_punctuation(s):\n",
    "    allowed_chars = string.punctuation.replace('&', '') + ' '\n",
    "    return all(char in allowed_chars for char in s)\n",
    "\n",
    "def is_Editor(editorRegEx, textBetweenNames, index):\n",
    "    if re.search(editorRegEx, textBetweenNames):\n",
    "        x = re.search(editorRegEx, text)\n",
    "        #print(f'x: {x.start()}')\n",
    "        if is_punctuation(text[index:x.start()]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_Names(authors):\n",
    "    finalAuthors = \"\"\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "    andInAuthors = find_First_Term(authors, search_terms)[2]\n",
    "    surenameFirst = is_SurenameFirst(authors)\n",
    "    print(f'surenameFirst: {surenameFirst}')\n",
    "    if surenameFirst:\n",
    "        #hier völlig egal, ob er einzelne Initialen in ein eigenes Word gesteckt hat, obwohl es noch Nachnamen gib\n",
    "        authors = authors.replace(andInAuthors, \" and \")\n",
    "        print(f'authors: {authors}')\n",
    "        finalAuthors = authors.replace(\", \", \" and \")\n",
    "    elif andInAuthors != \"\":\n",
    "        if \"., \" in authors:\n",
    "            print(\"Fall ., {0}\".format(authors))\n",
    "            search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "            andInAuthors = find_First_Term(authors, search_terms)[2]\n",
    "            authors = authors.replace(andInAuthors, \"., \")\n",
    "            authors = authors.split(\"., \")\n",
    "            authors = [name + \".\" for name in authors]\n",
    "            authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "            for author in authors[:-1]:\n",
    "                buffer = author.split(\", \")\n",
    "                finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "            buffer = authors[-1].split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "        elif \", \" in authors:\n",
    "            print(\"Fall , {0}\".format(authors))\n",
    "            search_terms = [\", and \", \", & \", \" and \", \" & \"]\n",
    "            andInAuthors = find_First_Term(authors, search_terms)[2]\n",
    "            authors = authors.replace(andInAuthors, \", \")\n",
    "            authors = authors.split(\", \")\n",
    "            for i in range(0, len(authors) - 3, 2):\n",
    "                finalAuthors = finalAuthors + authors[i+1] + \" \" + authors[i] + \" and \"\n",
    "            finalAuthors = finalAuthors + authors[len(authors) - 1] + \" \" + authors[len(authors) - 2]\n",
    "    else:\n",
    "        print(\"Fall else {0}\".format(authors))\n",
    "        authors = authors.split(\", \")\n",
    "        finalAuthros = authors[1] + authors[0]\n",
    "    return finalAuthors\n",
    "\n",
    "\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "#index neu setzen, da diese nicht automatich geupdates werden\n",
    "df_PER = df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "df_PER_len = len(df_PER.index)\n",
    "\n",
    "\n",
    "print(df_PER)\n",
    "\n",
    "fullNameInOneWord = False\n",
    "allFullNamesInOneWord = True\n",
    "nameAbgekürzt = False\n",
    "surenameFirst = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "startIndexPER = df_PER[\"start\"].iloc[0]\n",
    "endIndexPER = df_PER[\"end\"].iloc[-1] + 1\n",
    "#Beachte, ein ., auch zwischen Autoren und Titel vorkommen kann\n",
    "authors = text[startIndexPER:endIndexPER]\n",
    "search_terms = []\n",
    "\n",
    "if \"et al.\"in text:\n",
    "    search_terms = [\", et al.\", \" et al.\"]\n",
    "    firstStartIndexAnd, firstEndIndexAnd, andTyp = find_First_Term(text, search_terms)\n",
    "    print(\"find_First_Term {0}, {1}, {2}\".format(firstStartIndexAnd, firstEndIndexAnd, andTyp))\n",
    "\n",
    "\n",
    "\n",
    "#prüfe, wo der letzte Autor in der Autorenkette ist, um Namen auszuschließen, die im Titel sein können\n",
    "#vorher prüfen, wo et al. steht, weil so erkennt man ende sicher\n",
    "#zusäzich kann man noch prüfen, wo das erst \"and\" oder \"&\" kommt\n",
    "\n",
    "search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "editorRegEx = \" (\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\"\n",
    "index_df_PER_List = df_PER.index.values.tolist()\n",
    "onlyPunctuation = False\n",
    "onlyAnd = False\n",
    "editedBy = False\n",
    "authorsDetected = False\n",
    "editorsDetected = False\n",
    "startIndexAuthors = -1\n",
    "endIndexAuthors = -1\n",
    "startIndexEditors = -1\n",
    "endIndexEditors = -1\n",
    "startIndexEditorMarker = -1\n",
    "endIndexEditorMarker = -1\n",
    "chainStartIndex = -1\n",
    "setChainStart = True\n",
    "\n",
    "\n",
    "\n",
    "for index in index_df_PER_List:\n",
    "    #beachte: Hiermit lese ich immer schon vor!\n",
    "    if index < len(index_df_PER_List) - 1:\n",
    "        textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "    else:\n",
    "        textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "    onlyPunctuation = is_punctuation(textBetweenNames)\n",
    "    print(f'textBetweenNames: {textBetweenNames}')\n",
    "    print(f'Author: {text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index]]}')\n",
    "    print(f'onlyPunctuation: {onlyPunctuation}')\n",
    "    firstStartIndex, firstEndIndex, andTyp = find_First_Term(textBetweenNames, search_terms)\n",
    "    onlyAnd = textBetweenNames == andTyp\n",
    "    print(f'onlyAnd: {onlyAnd}')\n",
    "    print(f'nothing: {not onlyAnd and not onlyPunctuation}')\n",
    "    if setChainStart: \n",
    "        chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "        #Solange das auf False, sollen der Substring erweitert werden, also start bleibt konstant\n",
    "        #print(f'chainStartIndex: {chainStartIndex}')\n",
    "        setChainStart = False\n",
    "    #Dann gab es einen Bruch in der Autorenkette. Also bin ich in einer Lücke zwischen den AUtoren\n",
    "    #Dann ist Nächster Block wieder ein Autor\n",
    "    if not onlyPunctuation and not onlyAnd:\n",
    "        setChainStart = True\n",
    "        #Es können auch nur Editoren und keine Autoren vorkommen\n",
    "        authorsDetected = not is_Editor(editorRegEx, textBetweenNames, df_PER[\"end\"].iloc[index])\n",
    "        print(f'authorsDetected: {authorsDetected}')\n",
    "        # endIndexAuthors == -1, damit Autoren im Titel nicht wieder als Autoren erkannt werden\n",
    "        if authorsDetected and endIndexAuthors == -1:\n",
    "            startIndexAuthors = chainStartIndex\n",
    "            endIndexAuthors = df_PER[\"end\"].iloc[index]\n",
    "        #nicht nur ein else, falls Namen im Titel des Buches auftauchen\n",
    "        elif is_Editor(editorRegEx, textBetweenNames, df_PER[\"end\"].iloc[index]):\n",
    "            startIndexEditors = chainStartIndex\n",
    "            endIndexEditors = df_PER[\"end\"].iloc[index]\n",
    "            break\n",
    "\n",
    "\n",
    "if startIndexAuthors > -1:\n",
    "    authors = text[startIndexAuthors:endIndexAuthors]\n",
    "    finalAuthors = extract_Names(authors)\n",
    "\n",
    "if startIndexEditors > -1:\n",
    "    editors = text[startIndexEditors:endIndexEditors]\n",
    "    finalEditors = extract_Names(editors)\n",
    "    indicesEditorMarker = getIndexOfSubstring(editorRegEx, text)\n",
    "\n",
    "print(f'finalAuthors: {finalAuthors}')\n",
    "print(f'finalEditors: {finalEditors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a1358ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firstEndIndex edited by: 68\n",
      "editedBy: False\n",
      "x: <re.Match object; span=(59, 62), match=' ed'>\n"
     ]
    }
   ],
   "source": [
    "        firstStartIndex, firstEndIndex, editorTyp = find_First_Term(text, [\" edited by \"])\n",
    "        print(f'firstEndIndex edited by: {firstEndIndex}')\n",
    "        if firstStartIndex > -1 and index < len(index_df_PER_List) - 1:\n",
    "            print(f'df_PER[\"start\"].iloc[index]: {df_PER[\"start\"].iloc[index + 1]}')\n",
    "            editedBy = firstEndIndex + 1 == df_PER[\"start\"].iloc[index + 1]\n",
    "            #hier kam es oft vor, dass NER die Namen nicht richtig erkennt. Daher Sonderprüfung\n",
    "            if not editedBy:\n",
    "                buffer = text[firstEndIndex + 1:df_PER[\"start\"].iloc[index + 1]]\n",
    "                if re.search(r'^[A-Za-z]\\.([ ][A-Za-z]\\.)*[ ]?$', buffer):\n",
    "                    editedBy = True\n",
    "        print(f'editedBy: {editedBy}')\n",
    "        if editedBy:\n",
    "            startIndexEditorMarker = firstStartIndex\n",
    "            endIndexEditorMarker = firstEndIndex\n",
    "            editorsDetected = True\n",
    "        else:\n",
    "            print(f'x: {re.search(editorRegEx, text)}')\n",
    "            \n",
    "            \n",
    "  \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11af99c",
   "metadata": {},
   "source": [
    "Idee: Nun zunächst Jahr, Volume, Seiten, Edition und URL/DOI extrahieren. Den Rest (also Titel, Publisher, Series), dann nochmal den SpacyParer drüber laufen lassen, weil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc72533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 22, 'edited by')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score          word  start  end\n",
      "0         MISC  0.871492          Book      5    9\n",
      "1          PER  0.999499  Martin Trump     24   36\n"
     ]
    }
   ],
   "source": [
    "s = \"This Book was edited by Martin Trump\" \n",
    "print(find_First_Term(s, [\"edited by\"]))\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text = s\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "print(df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad04c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test, um Datum Zuverlässig erkannt wird\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hinton, G., Bengio, Y. and LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\")\n",
    "for ent in doc.ents:\n",
    "  print(ent, ent.start_char-ent.sent.start_char, ent.end_char-ent.sent.start_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d86d914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "names =  \"Singh, S. K., Kumar, S., & Mehra, P. S.\"\n",
    "\n",
    "x = re.search(r'^(\\w+\\.)', names) or re.search(r'^(\\w+\\s\\w+,)+$', names) or re.search(r'^(\\w+\\s\\w\\.,)+$', names)\n",
    "print(x)\n",
    "\n",
    "text = \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chapter Title. In Johnson, E. F. & Lee, R. H. (Eds.), Book Title. Publisher Name.\"\n",
    "print(text[92])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9aaa11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
