{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ef1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wenn man den typen bestimmte hat, sollte man zunächst alle Pflicht-Felder des Bibtex-Eintrages raussuchen und bestimmen.\n",
    "#Dazu die Pflichtattribute in einer Schleife durchgehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bde3f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "torch.cuda.is_available()\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55d2b1",
   "metadata": {},
   "source": [
    "@book{advanced_physics, author = {Stephen Hawking and Brian Greene and Maria Trump and Susan Soy}, title = {Advanced Concepts in Theoretical Physics}, publisher = {Cambridge University Press}, year = {2025}, edition = {3}, volume = {1}, series = {Advanced Studies in Physics}, address = {Cambridge, UK}, month = {May}, note = {A seminal work in the field of theoretical physics}, isbn = {978-0-521-76948-0}, doi = {10.1017/example.book.2025}, url = {https://www.cambridge.org/advanced_physics}, annote = {Widely cited in the physics community}, abstract = {This book explores cutting-edge theories and concepts in theoretical physics...}, keywords = {Theoretical Physics, Quantum Mechanics, String Theory}, language = {English}, price = {75.00}, size = {600 pages}, lccn = {2020934576}, mrnumber = {MR3070071} }\n",
    "\n",
    "@article{quantum_entanglement,\n",
    "  author        = {Albert Einstein and Boris Podolsky and Nathan Rosen},\n",
    "  title         = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},\n",
    "  journal       = {Physical Review},\n",
    "  year          = {1935},\n",
    "  volume        = {47},\n",
    "  number        = {10},\n",
    "  pages         = {777-780},\n",
    "  month         = {May},\n",
    "  note          = {EPR Paradox paper, fundamental for quantum mechanics},\n",
    "  doi           = {10.1103/PhysRev.47.777},\n",
    "  url           = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},\n",
    "  abstract      = {In this paper, the authors discuss the EPR paradox and challenge the completeness of quantum mechanics...},\n",
    "  keywords      = {Quantum Mechanics, EPR Paradox, Physical Reality},\n",
    "  language      = {English},\n",
    "  publisher     = {American Physical Society}\n",
    "}\n",
    "\n",
    "@inproceedings{deep_learning,\n",
    "  author        = {Geoffrey Hinton and Yoshua Bengio and Yann LeCun},\n",
    "  title         = {Deep Learning for Artificial Intelligence},\n",
    "  booktitle     = {Proceedings of the IEEE International Conference on Neural Networks},\n",
    "  year          = {2021},\n",
    "  editor        = {Jane Smith and John Doe},\n",
    "  volume        = {1},\n",
    "  number = {5},\n",
    "  series        = {Advances in Neural Information Processing},\n",
    "  pages         = {100-120},\n",
    "  address       = {Montreal, Canada},\n",
    "  month         = {December},\n",
    "  organization  = {IEEE},\n",
    "  publisher     = {IEEE Press},\n",
    "  note          = {Keynote paper on recent advancements in deep learning},\n",
    "  isbn          = {978-1-5386-4637-1},\n",
    "  doi           = {10.1109/ICNN.2021.9483948},\n",
    "  url           = {https://ieeexplore.ieee.org/document/9483948},\n",
    "  annote        = {A seminal work on how deep learning transforms AI},\n",
    "  abstract      = {This paper explores cutting-edge deep learning techniques and their impact on the development of artificial intelligence...},\n",
    "  keywords      = {Deep Learning, Artificial Intelligence, Neural Networks},\n",
    "  language      = {English}\n",
    "}\n",
    "\n",
    "\n",
    "@incollection{quantum_computation,\n",
    "  author        = {Michael A. Nielsen and Isaac L. Chuang},\n",
    "  title         = {Quantum Computation and Quantum Information},\n",
    "  booktitle     = {Handbook of Quantum Information Science},\n",
    "  publisher     = {Springer},\n",
    "  year          = {2026},\n",
    "  editor        = {Charles H. Bennett and David P. DiVincenzo},\n",
    "  volume        = {4},\n",
    "  series        = {Quantum Science and Technology},\n",
    "  chapter       = {10},\n",
    "  pages         = {250-300},\n",
    "  address       = {Berlin, Germany},\n",
    "  month         = {October},\n",
    "  note          = {A comprehensive overview of the fundamentals of quantum computation},\n",
    "  isbn          = {978-3-540-88702-7},\n",
    "  doi           = {10.1007/springerreference_303198},\n",
    "  url           = {https://www.springer.com/gp/book/9783540887027},\n",
    "  annote        = {Essential reading for researchers entering the field of quantum information},\n",
    "  abstract      = {This chapter delves into the principles of quantum computing, offering an accessible yet thorough introduction...},\n",
    "  keywords      = {Quantum Computing, Quantum Information, Computational Models},\n",
    "  language      = {English},\n",
    "  price         = {45.00},\n",
    "  size          = {50 pages}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b93a7979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(93, 100), match=' (Eds.)'>\n"
     ]
    }
   ],
   "source": [
    "#RegEx zum FInden der Seiten\n",
    "pageFinder = \", (?:pp\\.? )?\\d+-\\d+,\"\n",
    "doi1 = \"https:\\/\\/doi\\.org\"\n",
    "doi2 = \"DOI:\"\n",
    "\n",
    "#if havard und book:\n",
    "volume = \", \\d+,\"\n",
    "number = \", \\d+,\"\n",
    "edition1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "edition2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "\n",
    "#if APA or Havard then\n",
    "volume = \"\\(Vol\\. \\d+\" #für Volume und number noch als Bedingung, dass es nach Titel stehen muss, also Index > Titel-Index\n",
    "number = \"Issue \\d+\"\n",
    "yearApaHavard = \"\\(\\d{4}\\)\"\n",
    "editorRegEx = \" (\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors|edited by )(\\))?\"\n",
    "\n",
    "#editor bei anderen\n",
    "\n",
    "\n",
    "#Beachte: Editor und Edition werden oft mit gleichen Abkürzungen versehen, auch im gleichen Stil! Nur Edition wird oft\n",
    "#klein geschrieben. Daher zustzlich prüfen, ob NER das als Person erkennt.\n",
    "\n",
    "#Beachte: ieee und mla trennen mit , und nicht mit .\n",
    "#if ieee and mla\n",
    "volumeIeeeMla = \", vol\\. \\d+\" \n",
    "noIeeeMla = \", no\\. \\d+\"\n",
    "\n",
    "etAl = \"et al.\"\n",
    "\n",
    "yearACM = \"\\. \\d{4}\\.\"\n",
    "\n",
    "#Man sollte zunächst die Felder extrahieren, die zu 100% sicher erkennen kann wie Volume oder Number, doi, Autoren...-> \n",
    "#Titel, booktitle, series und journal danach\n",
    "\n",
    "#man sollte eine Überdeckungsprüfung machen: Also Bspw. sagt Space im Bereich 25-50 ist eine Orga, Regex sagt, \n",
    "#dort ist ein (Eds.)\n",
    "#zu finden und Huggingface sagt, von 25-45 ist was. Dann sollte der minimalteste Wert und maximalste Wert \n",
    "#genommen werden und so lange nach links und analog nach rechts gehen, bis wieder ein Punkt kommt.\n",
    "# Da erkannte Felder aus dem String entfernt werden, muss der String am Ende leer sein. \n",
    "#Daher sollten zunächste die Dinge ausgeschnitten werden wo der Algo sich am sichersten ist\n",
    "#TODO: Grundsätzliche Reihenfolge der BibTex-Einträge ermitteln.\n",
    "#TODO: Für jede Extrahierung eines Feldes eine eigene Unterroutine schreiben? -> In der Oberschleife sollte Style sein\n",
    "\n",
    "\n",
    "\n",
    "print(re.search(editorRegEx, \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chapter Title. In Johnson, E. F. & Lee, R. H. (Eds.), Book\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddfd2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Hawking, S., Greene, B., Trump, M. and Soy, S. (2025). Advanced Concepts and John Miller in Theoretical Physics (3rd ed., Vol. 1). Cambridge University Press. https://doi.org/10.1017/example.book.2025\"\n",
    "text = \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chat GPT & Google Bard AI: A Review. 2023 International Conference on IoT, Communication and Automation Technology (ICICAT), 1–6. https://doi.org/10.1109/ICICAT57735.2023.10263706\"\n",
    "text = \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chapter Title. In Johnson, E. F. & Lee, R. H. (Eds.), Book Title. Publisher Name.\"\n",
    "#text = \"Singh, S. K. and Peterman, P. (2023). Chapter and Title. Trump, M. and Soy, S. Book Title. Publisher Name.\"\n",
    "#text = \"S. K. Singh, S. Kumar & P. S. Mehra, (2023). Chapter Title. In E. F. Johnson & R. H. Lee (Eds.), Book Title. Publisher Name.\"\n",
    "#text = \"Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoît Sagot, Abdelrahman Mohamed, and Emmanuel Dupoux. 2023. Generative spoken dialogue language modeling. Transactions of the Association for Computational Linguistics 11, (2023), 250–266. URL: https://aclanthology.org/2023.tacl-1.15, doi:10.1162/tacl_a_00545\"\n",
    "#text = \"Hinton G, Bengio Y, LeCun Y (2021) Deep Learning for Artificial Intelligence. In: Smith J, Doe J (eds) Proceedings of the IEEE International Conference on Neural Networks. IEEE Press, Montreal, Canada, pp 100–120\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8acde04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\larsl\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   entity_group     score     word  start  end\n",
      "0           PER  0.999411    Singh      0    5\n",
      "1           PER  0.996913        S      7    8\n",
      "2           PER  0.997643        K     10   11\n",
      "3           PER  0.999555    Kumar     14   19\n",
      "4           PER  0.998111        S     21   22\n",
      "5           PER  0.987601    Mehra     27   32\n",
      "6           PER  0.998999        P     34   35\n",
      "7           PER  0.998115        S     37   38\n",
      "8           PER  0.999267  Johnson     66   73\n",
      "9           PER  0.990239        E     75   76\n",
      "10          PER  0.965848        F     78   79\n",
      "11          PER  0.998798      Lee     83   86\n",
      "12          PER  0.997356        R     88   89\n",
      "13          PER  0.988298        H     91   92\n",
      "textBetweenNames: , \n",
      "chainStartIndex: 0\n",
      "textBetweenNames: . \n",
      "textBetweenNames: ., \n",
      "textBetweenNames: , \n",
      "textBetweenNames: ., & \n",
      "textBetweenNames: , \n",
      "textBetweenNames: . \n",
      "textBetweenNames: . (2023). Chapter Title. In \n",
      "editedBy: False\n",
      "x: <re.Match object; span=(93, 100), match=' (Eds.)'>\n",
      "textBetweenNames: , \n",
      "chainStartIndex: 66\n",
      "textBetweenNames: . \n",
      "textBetweenNames: . & \n",
      "textBetweenNames: , \n",
      "textBetweenNames: . \n",
      "textBetweenNames: . (Eds.), Book Title. Publisher Name.\n",
      "editedBy: False\n",
      "x: <re.Match object; span=(93, 100), match=' (Eds.)'>\n",
      "x: 93\n",
      "authors: Singh, S. K., Kumar, S., & Mehra, P. S \n",
      "editors: Johnson, E. F. & Lee, R. H \n",
      "Index Editor Marker: 93, 100\n",
      "., & \n",
      "Fall ., Singh, S. K., Kumar, S., & Mehra, P. S\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'startIndexMaxDistanceAuthorNames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 186\u001b[0m\n\u001b[0;32m    182\u001b[0m     authors \u001b[38;5;241m=\u001b[39m authors\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    183\u001b[0m     finalAuthros \u001b[38;5;241m=\u001b[39m authors[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m authors[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 186\u001b[0m text \u001b[38;5;241m=\u001b[39m text[startIndexMaxDistanceAuthorNames \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m:endIndexMaxDistanceAuthorNames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m text[endIndexMaxDistanceAuthorNames:endIndexPER]\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mprint\u001b[39m(finalAuthors)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'startIndexMaxDistanceAuthorNames' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_First_Term(text, search_terms):\n",
    "    # Initialisiere mit einem hohen Wert\n",
    "    min_index = float('inf')\n",
    "    end_index = 0\n",
    "    andTyp = \"\"\n",
    "    \n",
    "    # Suche jeden Suchbegriff in dem Text und behalte den kleinsten Index\n",
    "    for term in search_terms:\n",
    "        index = text.find(term)\n",
    "        if index != -1 and index < min_index:\n",
    "            min_index = index\n",
    "            end_index = min_index + len(term) - 1\n",
    "            andTyp = term\n",
    "    \n",
    "    # Wenn min_index unverändert ist, wurde keiner der Begriffe gefunden\n",
    "    return (min_index, end_index, andTyp) if min_index != float('inf') else (-1, -1, \"\")\n",
    "\n",
    "import string\n",
    "\n",
    "def is_punctuation(s):\n",
    "    allowed_chars = string.punctuation.replace('&', '') + ' '\n",
    "    return all(char in allowed_chars for char in s)\n",
    "\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "\n",
    "#index neu setzen, da diese nicht automatich geupdates werden\n",
    "df_PER = df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "df_PER_len = len(df_PER.index)\n",
    "\n",
    "\n",
    "print(df_PER)\n",
    "\n",
    "fullNameInOneWord = False\n",
    "allFullNamesInOneWord = True\n",
    "nameAbgekürzt = False\n",
    "surenameFirst = False\n",
    "\n",
    "for index in df_PER.index.values.tolist():\n",
    "    if \".\" == text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "        nameAbgekürzt = True\n",
    "for element in text:\n",
    "    if element == \",\":\n",
    "        break\n",
    "    if element in [\".\", \" \"]:\n",
    "        surenameFirst = True\n",
    "\n",
    "startIndexPER = df_PER[\"start\"].iloc[0]\n",
    "endIndexPER = df_PER[\"end\"].iloc[-1] + 1\n",
    "#Beachte, ein ., auch zwischen Autoren und Titel vorkommen kann\n",
    "authors = text[startIndexPER:endIndexPER]\n",
    "search_terms = []\n",
    "\n",
    "if \"et al.\"in text:\n",
    "    search_terms = [\", et al.\", \" et al.\"]\n",
    "    firstStartIndexAnd, firstEndIndexAnd, andTyp = find_First_Term(text, search_terms)\n",
    "    print(\"find_First_Term {0}, {1}, {2}\".format(firstStartIndexAnd, firstEndIndexAnd, andTyp))\n",
    "\n",
    "\n",
    "\n",
    "#prüfe, wo der letzte Autor in der Autorenkette ist, um Namen auszuschließen, die im Titel sein können\n",
    "#vorher prüfen, wo et al. steht, weil so erkennt man ende sicher\n",
    "#zusäzich kann man noch prüfen, wo das erst \"and\" oder \"&\" kommt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "editorRegEx = \" (\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\"\n",
    "index_df_PER_List = df_PER.index.values.tolist()\n",
    "onlyPunctuation = False\n",
    "onlyAnd = False\n",
    "editedBy = False\n",
    "authorsDetected = True\n",
    "editorsDetected = False\n",
    "startIndexAuthors = -1\n",
    "endIndexAuthors = -1\n",
    "startIndexEditors = -1\n",
    "endIndexEditors = -1\n",
    "startIndexEditorMarker = -1\n",
    "endIndexEditorMarker = -1\n",
    "chainStartIndex = -1\n",
    "setChainStart = True\n",
    "\n",
    "for index in index_df_PER_List:\n",
    "    #beachte: Hiermit lese ich immer schon vor!\n",
    "    if index < len(index_df_PER_List) - 1:\n",
    "        textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "    else:\n",
    "        textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "    onlyPunctuation = is_punctuation(textBetweenNames)\n",
    "    print(f'textBetweenNames: {textBetweenNames}')\n",
    "    #print(f'onlyPunctuation: {onlyPunctuation}')\n",
    "    firstStartIndex, firstEndIndex, andTyp = find_First_Term(textBetweenNames, search_terms)\n",
    "    onlyAnd = textBetweenNames == andTyp\n",
    "    #print(f'onlyAnd: {onlyAnd}')\n",
    "    #print(f'nothing: {not onlyAnd and not onlyPunctuation}')\n",
    "    if not onlyPunctuation and not onlyAnd:\n",
    "        firstStartIndex, firstEndIndex, editorTyp = find_First_Term(textBetweenNames, [\"edited by\"])\n",
    "        editedBy = firstEndIndex + 2 == df_PER[\"start\"].iloc[index]\n",
    "        print(f'editedBy: {editedBy}')\n",
    "        if editedBy:\n",
    "            startIndexEditorMarker = firstStartIndex\n",
    "            endIndexEditorMarker = firstEndIndex\n",
    "            editorsDetected = True\n",
    "        else:\n",
    "            print(f'x: {re.search(editorRegEx, text)}')\n",
    "            if re.search(editorRegEx, textBetweenNames):\n",
    "                x = re.search(editorRegEx, text)\n",
    "                print(f'x: {x.start()}')\n",
    "                if is_punctuation(text[df_PER[\"end\"].iloc[index]:x.start()]):\n",
    "                    editorsDetected = True\n",
    "                    startIndexEditorMarker = x.start()\n",
    "                    endIndexEditorMarker = x.end()\n",
    "\n",
    "    \n",
    "    if setChainStart: \n",
    "        chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "        #Solange das auf False, sollen der Substring erweitert werden, also start bleibt konstant\n",
    "        print(f'chainStartIndex: {chainStartIndex}')\n",
    "        setChainStart = False\n",
    "    #Dann gab es einen Bruch in der Autorenkette. Also bin ich in einer Lücke zwischen den AUtoren\n",
    "    #Dann ist Nächster Block wieder ein Autor\n",
    "    if not onlyPunctuation and not onlyAnd:\n",
    "        setChainStart = True\n",
    "        #die Auotren kommen immer zuerst im STring\n",
    "        if authorsDetected:\n",
    "            startIndexAuthors = chainStartIndex\n",
    "            endIndexAuthors = df_PER[\"end\"].iloc[index]\n",
    "            authorsDetected = False\n",
    "        if editorsDetected:\n",
    "            startIndexEditors = chainStartIndex\n",
    "            endIndexEditors = df_PER[\"end\"].iloc[index]\n",
    "            break\n",
    "\n",
    "\n",
    "authors = text[startIndexAuthors:endIndexAuthors]\n",
    "editors = text[startIndexEditors:endIndexEditors]\n",
    "print(\"authors: {0} \".format(authors))\n",
    "print(\"editors: {0} \".format(editors))\n",
    "print(f'Index Editor Marker: {startIndexEditorMarker}, {endIndexEditorMarker}')\n",
    "andInAuthors = find_First_Term(authors, search_terms)[2]\n",
    "print(andInAuthors)\n",
    "andInEditors = find_First_Term(editors, search_terms)[2]\n",
    "finalAuthors = \"\"\n",
    "finalEditors = \"\"\n",
    "\n",
    "if surenameFirst:\n",
    "    #hier völlig egal, ob er einzelne Initialen in ein eigenes Word gesteckt hat, obwohl es noch Nachnamen gib\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "    andInAuthors = find_First_Term(authors, search_terms)[2]\n",
    "    authors = authors.replace(andInAuthors, \" and \")\n",
    "    finalAuthors = authors.replace(\", \", \" and \")\n",
    "elif andInAuthors != \"\":\n",
    "    if \"., \" in authors:\n",
    "        print(\"Fall ., {0}\".format(authors))\n",
    "        search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "        andInAuthors = find_First_Term(authors, search_terms)[2]\n",
    "        authors = authors.replace(andInAuthors, \"., \")\n",
    "        authors = authors.split(\"., \")\n",
    "        authors = [name + \".\" for name in authors]\n",
    "        authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "        for author in authors[:-1]:\n",
    "            buffer = author.split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "        buffer = authors[-1].split(\", \")\n",
    "        finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "    elif \", \" in authors:\n",
    "        print(\"Fall , {0}\".format(authors))\n",
    "        search_terms = [\", and \", \", & \", \" and \", \" & \"]\n",
    "        andInAuthors = find_First_Term(authors, search_terms)[2]\n",
    "        authors = authors.replace(andInAuthors, \", \")\n",
    "        authors = authors.split(\", \")\n",
    "        for i in range(0, len(authors) - 3, 2):\n",
    "            finalAuthors = finalAuthors + authors[i+1] + \" \" + authors[i] + \" and \"\n",
    "        finalAuthors = finalAuthors + authors[len(authors) - 1] + \" \" + authors[len(authors) - 2]\n",
    "else:\n",
    "    print(\"Fall else {0}\".format(authors))\n",
    "    authors = authors.split(\", \")\n",
    "    finalAuthros = authors[1] + authors[0]\n",
    "\n",
    "\n",
    "text = text[startIndexMaxDistanceAuthorNames + 2:endIndexMaxDistanceAuthorNames - 1] + text[endIndexMaxDistanceAuthorNames:endIndexPER]\n",
    "print(finalAuthors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1358ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "s = \"&! \"\n",
    "print(all(char in string.punctuation for char in s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b180b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, False, True, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False]\n",
      "[False, False, False, False, True, False, False, False, False, True, False, False, False, True, False, False, False, False, False, False, True, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, ' and ', ', and ', ' & ', ', & ', '., & ', '., and ']\n"
     ]
    }
   ],
   "source": [
    "def find_First_Term(text, search_terms):\n",
    "    # Initialisiere mit einem hohen Wert\n",
    "    min_index = float('inf')\n",
    "    end_index = 0\n",
    "    andTyp = \"\"\n",
    "    \n",
    "    # Suche jeden Suchbegriff in dem Text und behalte den kleinsten Index\n",
    "    for term in search_terms:\n",
    "        index = text.find(term)\n",
    "        if index != -1 and index < min_index:\n",
    "            min_index = index\n",
    "            end_index = min_index + len(term) - 1\n",
    "            andTyp = term\n",
    "    \n",
    "    # Wenn min_index unverändert ist, wurde keiner der Begriffe gefunden\n",
    "    return (min_index, end_index, andTyp) if min_index != float('inf') else (-1, -1, \"\")\n",
    "\n",
    "import string\n",
    "\n",
    "def is_punctuation(s):\n",
    "    allowed_chars = string.punctuation.replace('&', '') + ' '\n",
    "    return all(char in allowed_chars for char in s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11af99c",
   "metadata": {},
   "source": [
    "Idee: Nun zunächst Jahr, Volume, Seiten, Edition und URL/DOI extrahieren. Den Rest (also Titel, Publisher, Series), dann nochmal den SpacyParer drüber laufen lassen, weil "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc72533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 22, 'edited by')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score          word  start  end\n",
      "0         MISC  0.871492          Book      5    9\n",
      "1          PER  0.999499  Martin Trump     24   36\n"
     ]
    }
   ],
   "source": [
    "s = \"This Book was edited by Martin Trump\" \n",
    "print(find_First_Term(s, [\"edited by\"]))\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "text = s\n",
    "outputs = ner_tagger(text)\n",
    "df_outputs = pd.DataFrame(outputs)\n",
    "print(df_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad04c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test, um Datum Zuverlässig erkannt wird\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hinton, G., Bengio, Y. and LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\")\n",
    "for ent in doc.ents:\n",
    "  print(ent, ent.start_char-ent.sent.start_char, ent.end_char-ent.sent.start_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d86d914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "text = \"Singh, S. K., Kumar, S., & Mehra, P. S. (2023). Chapter Title. In Johnson, E. F. & Lee, R. H. (Eds.), Book Title. Publisher Name.\"\n",
    "print(text[92])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9aaa11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
