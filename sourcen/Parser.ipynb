{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8acde04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "\n",
    "def custom_strip(text, replaceCharacter = []):\n",
    "    \n",
    "    '''\n",
    "    Strip-Funktion, die standardmäßig neben Whitespace auch Zeichen aus string.punctuation und die \n",
    "    Zeichen “ und ” entfernt.\n",
    "    \n",
    "    Parameter:\n",
    "    text: String, der gestripped werden soll.\n",
    "    replaceCharacter: Liste von Zeichen, die für den Strip nicht berücksichtigt werden sollen.\n",
    "    \n",
    "    return: gestrippter String.\n",
    "    '''\n",
    "    \n",
    "    allowed_chars = string.punctuation + string.whitespace + \"“\" + \"”\"\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return text.strip(allowed_chars)\n",
    "\n",
    "\n",
    "def getIndexOfSubstring(text, regEx = [], reverse = False):\n",
    "    \n",
    "    '''\n",
    "    Prüft für eine Liste von RegEx, ob sie im String namens text vorkommen. Pro RegEx wird nur \n",
    "    der erste Match berücksichtigt. Der Match, der den Substring mit der größten Länge ermittelt, kommt zum Zuge. \n",
    "    \n",
    "    Parameter:\n",
    "    text: Text, wo das Auftreten des RegEx geprüft wird.\n",
    "    regEx = []: Liste mit RegEx. \n",
    "    reverse: Ob das erste oder letzte Auftreten eines Matches geprüft maßgeblich ist. Wenn reverse = False, dann wird das\n",
    "    erste Auftreten geprüft.\n",
    "    \n",
    "    return: Substring und den zugehörigen Start- und Endindex.\n",
    "    '''\n",
    "    \n",
    "    length = 0\n",
    "    matches = []\n",
    "    substring = \"\"\n",
    "    for regExElement in regEx:\n",
    "        matches = list(re.finditer(regExElement, text))\n",
    "        if matches:\n",
    "            if reverse:\n",
    "                match = matches[-1]\n",
    "            else:\n",
    "                match = matches[0]\n",
    "            buffer = match.end() - match.start()\n",
    "            if buffer > length:\n",
    "                length = buffer\n",
    "                startIndex = match.start()\n",
    "                endIndex = match.end()\n",
    "                substring = text[match.start():match.end()]\n",
    "    if substring != \"\":\n",
    "        return startIndex, endIndex, substring   \n",
    "    return -1, -1, substring\n",
    "\n",
    "def getSubstringByRegEx(text, regex = []):\n",
    "    \n",
    "    '''\n",
    "    Prüft für eine Liste von RegEx, ob sie im String namens text vorkommen. Der RegEx, der den Substring mit der grötßen\n",
    "    Länge ermittelt, kommt zum Zuge. Dieser Substring wird dann aus dem text ausgeschnitten. Ein RegEx wird\n",
    "    dabei von hinten beginnend in text geprüft und der erste Match zählt.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    regEx = []: Liste mit RegEx. \n",
    "    \n",
    "    return: String ohne Substring (changedText) und ausgeschnittenen Substring\n",
    "    '''\n",
    "    \n",
    "    startIndex, endIndex, substring = getIndexOfSubstring(text, regex, True)\n",
    "    changedText, substring = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "    return changedText, custom_strip(substring)\n",
    "\n",
    "def replaceSubstring (startIndex, endIndex, text, substituteString, ignorePunctuation = [\"&\", \"(\", \")\"]):\n",
    "    \n",
    "    '''\n",
    "    Ersetzt in dem String namens text einen Substring durch einen anderen String namens substituteString.\n",
    "    Die Variablen startIndex und endIndex können dabei noch verändert werden, webb vor der Postion startIndex oder nach der\n",
    "    Position endIndex bestimmte Zeichen folgen, die mit entfernt werden sollen. Die Existenz dieser bestimmten Zeichen \n",
    "    wird mit der Funktion isSpeceficPunctuation geprüft. Mit dem Parameter ignorePunctuation wird auf die Prüfung\n",
    "    bestimmter Zeichen in der Funktion isSpeceficPunctuation verzichtet.\n",
    "    \n",
    "    Parameter:\n",
    "    startIndex: Index, wo der zu ersetztende Substring im String namens text eingefügt werden soll.\n",
    "    endIndex: Index, wo der zu ersetztende Substring im String text enden soll.\n",
    "    text: Text, wo das Auftreten des Substrings geprüft wird.\n",
    "    substituteString: Der einzufügende Substring.\n",
    "    ignorePunctuation: Zeichen, die für die Indexverschiebung nicht berücksichtigt werden sollen.\n",
    "    \n",
    "    return: Substring und den zugehörigen Start- und Endindex.\n",
    "    '''\n",
    "    \n",
    "    if endIndex > 0:\n",
    "        startIndexReplace = 0\n",
    "        endIndexReplace = 0\n",
    "        if startIndex > 0:\n",
    "            for i in range(startIndex, -1, -1):\n",
    "                if isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    startIndexReplace = i + 1\n",
    "                    break\n",
    "        else:\n",
    "            startIndexReplace = 0            \n",
    "        if endIndex < len(text):\n",
    "            for i in range(endIndex-1, len(text), 1):\n",
    "                if isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    endIndexReplace = i + 1\n",
    "                    break\n",
    "                elif i == len(text)-1:\n",
    "                    endIndexReplace = len(text)\n",
    "        else:\n",
    "            endIndexReplace = len(text)\n",
    "        if endIndexReplace > 0:\n",
    "            changedText = text[0:startIndexReplace] + substituteString + text[endIndexReplace:len(text)]\n",
    "            return changedText, text[startIndexReplace:endIndexReplace]\n",
    "    return text, \"\"\n",
    "\n",
    "def is_SurenameFirst(names):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob die Autoren- oder Editornamen mit dem Vornamen beginnen. \n",
    "    \n",
    "    Parameter:\n",
    "    names: Substring vom Literaturstring, der die Autoren- oder Editornamen enthält.\n",
    "    \n",
    "    return: True, wenn die Namen mit dem Vornamen beginnen. Ansonsten False.\n",
    "    '''\n",
    "    \n",
    "    splitedNames = names.split(\" \")\n",
    "    #regex wie w+ erkennt bspw. KEIN è \n",
    "    if splitedNames[0].endswith(\".\"):\n",
    "        return True\n",
    "    splitedNames = names.split(\",\")\n",
    "    if all(\" \" in item.strip() for item in splitedNames):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def is_NameShortened(df_PER):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob die Namen mit einem Punkt abgekürzt sind.\n",
    "    \n",
    "    Parameter:\n",
    "    df_Per: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Person.\n",
    "    \n",
    "    return: True, falls der Name mit einem Punkt abgekürzt ist. Ansonsten False.\n",
    "    '''\n",
    "    \n",
    "    for index in df_PER.index.values.tolist():\n",
    "        if \".\" == text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def isSpeceficPunctuation(text, replaceCharacter = []):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob ein Srting nur aus bestimmten Satzzeichen besteht. Standardmäßig wird string.punctuation + string.whitespace\n",
    "    geprüft.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Text, der geprüft werden soll.\n",
    "    replaceCharacter = []: Liste von Zeichen, die aus der standardmäßigen Prüfung entfernt werden sollen.\n",
    "    '''\n",
    "        \n",
    "    allowed_chars = string.punctuation + string.whitespace\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return all(char in allowed_chars for char in text)\n",
    "\n",
    "def is_Editor(editorRegEx, textBetweenNames, startIndexTextBetweenNames, markerBehind = True):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob in dem String textBetweenNames ein Signalwort für Editoren enthalten ist.\n",
    "    \n",
    "    Parameter:\n",
    "    editorRegEx: RegEx, die Signalwörter für das Auftreten von Editoren erkennen sollen.\n",
    "    textBetweenNames: Substring vom Literaturstring, der geprüft weden soll, ob das Signalwort enthalten ist. Dieser \n",
    "    Substring steht stehts zwischen potententiellen Namen.\n",
    "    startIndexTextBetweenNames: Startindex, wo Substring im original Literaturstring steht. Signalwörter können dabei nicht\n",
    "    Bestandteil von einer Namenskette sein, sondern immer nur zwischen solchen Namensketten. Daher der Name des Parameters.\n",
    "    markerBehind: Ob das Signalwort für Editoren vor oder hinter den Editorennamen im original Literaturstring steht.\n",
    "    \n",
    "    return: Boolean, ob gefunden, und Start- und Endindizies, wo es im original Literaturstring vorkommt.\n",
    "    '''\n",
    "    \n",
    "    startSubstring, endSubstring, substring = getIndexOfSubstring(textBetweenNames, [editorRegEx])\n",
    "    if startIndexTextBetweenNames > -1 and markerBehind:\n",
    "        if isSpeceficPunctuation(textBetweenNames[startIndexTextBetweenNames:startSubstring], [\"&\"]):\n",
    "            ''' Signalwörter, die das Vorliegen von Editoren markieren, werden durch die editorRegEx geprüft.\n",
    "            Es gilt folgende Heuristik: Ein Signalwort für Editoren, das vor oder hinter den Editorenamen steht, \n",
    "            darf nur von bestimmten Satzzeichen/Punkuationen und nicht von Wörtern unterbrochen sein.\n",
    "            Es kann schließlich zufällig sein, dass ein RegEx ein Editor-Signalwort erkennt, \n",
    "            das jedoch eigentlich keins ist, da sie die zuvor genannte Heuristik nicht erfüllen. \n",
    "            \n",
    "            Beispiel: \n",
    "            Gegeben sei folgender Ausschnitt eines Literaturstrings: \"Bennett, C. H., DiVincenzo, D. P., Eds.\"\n",
    "            Das Signalwort \"Eds.\" befindet sich direkt hinter den beiden Autorennamen, da es nur von einem Punkt, Komma und\n",
    "            einem Leerzeichen (also bestimmten Satzzeichen) unterbrochen ist. Also ist die Heuristik erfüllt.\n",
    "            \n",
    "            Dieses Vorgehen gilt für den elif-Teil analog.'''\n",
    "            return True, startSubstring + startIndexTextBetweenNames, endSubstring + startIndexTextBetweenNames\n",
    "    elif startIndexTextBetweenNames > -1:\n",
    "        if isSpeceficPunctuation(textBetweenNames[endSubstring:startIndexTextBetweenNames], [\"&\"]):\n",
    "            return True, startSubstring + startIndexTextBetweenNames, endSubstring + startIndexTextBetweenNames\n",
    "    return False, -1, -1\n",
    "\n",
    "def processNames(authors):\n",
    "    \n",
    "    '''\n",
    "    Bereitet die Autorennamen in ein Standardformat auf.\n",
    "    \n",
    "    Parameter:\n",
    "    authors: Substring aus dem original Literaturstring, der die Autoren enthält.\n",
    "    \n",
    "    return: Standardformat für Autoren.\n",
    "    '''\n",
    "    \n",
    "    finalAuthors = \"\"\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "    surenameFirst = is_SurenameFirst(authors.strip())\n",
    "    authors = custom_strip(authors)\n",
    "    if surenameFirst:\n",
    "        startIndex, endIndex, andInAuthors = getIndexOfSubstring(authors, search_terms)\n",
    "        if startIndex >= 0:\n",
    "            authors = authors.replace(andInAuthors, \" and \")\n",
    "            finalAuthors = authors.replace(\", \", \" and \")\n",
    "        else:\n",
    "            finalAuthors = authors\n",
    "    elif \"., \" in authors:\n",
    "        search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        authors = authors.replace(\"., \", \"#., \")\n",
    "        if andInAuthors != \"\":\n",
    "            authors = authors.replace(andInAuthors, \"#., \")\n",
    "        authors = authors.split(\"., \")\n",
    "        authors = [name.replace(\"#\",\".\") for name in authors]\n",
    "        authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "        for author in authors[:-1]:\n",
    "            buffer = author.split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "        buffer = authors[-1].split(\", \")\n",
    "        finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "    elif \", \" in authors:\n",
    "        search_terms = [\", and \", \", & \", \" and \", \" & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        if andInAuthors != \"\":  \n",
    "            authors = authors.replace(andInAuthors, \", \")\n",
    "        authors = authors.split(\", \")\n",
    "        for i in range(0, len(authors) - 3, 2):\n",
    "            finalAuthors = finalAuthors + authors[i+1] + \" \" + authors[i] + \" and \"\n",
    "        finalAuthors = finalAuthors + authors[len(authors) - 1] + \" \" + authors[len(authors) - 2]\n",
    "    return custom_strip(finalAuthors)\n",
    "\n",
    "def getAuthors(text):\n",
    "    \n",
    "    '''\n",
    "    Schneidet die Autoren aus dem original Literaturstring aus. Es können mehrere Autoren vorliegen und diese\n",
    "    können mit einem \"and\" oder \"&\" verknüpft sein. Die Funktion soll alle Autoren einschließlich dem \"and\" und \"&\"\n",
    "    in einem Block extrahieren. Dieser Block ist sinnbildlich eine Kette von Namen. Die Variable textBetweenNames \n",
    "    beinhaltet die Substrings, die zwischen solchen Namensketten stehen. \n",
    "    Wenn eine solche Kette von Namen gefunden wird, wird setChainStart = True gesetzt und geprüft, \n",
    "    ob es sich auch um Autorennamen handelt.\n",
    "    \n",
    "    Beispiel:\n",
    "    \"Nielsen, M. A.; Chuang, I. L. Quantum Computation and Quantum Information. In Handbook of Quantum Information Science;\n",
    "    Bennett, C. H., DiVincenzo, D. P., Eds.; Quantum Science and Technology; Springer: Berlin, Germany, 2026; Vol. 4, \n",
    "    pp 250–300. https://doi.org/10.1007/springerreference-303198.\"\n",
    "    \n",
    "    Da Autoren und Editoren vorliegen, liegen zwei Abschnitte vor, die textBetweenNames sind, nämlich zwischen Index\n",
    "    28 und 118 sowie zwischen 151 und dem Ende des Strings. Entsprechend liegen zwischen 0 und 27 und 119 bis 150 \n",
    "    Namensketten vor. Ob Sonderzeichen wie der Punkt zur Abkürzung von Nachnamen mit zum Namen gezählt werden, hängt\n",
    "    von der NER ab.\n",
    "\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Autoren und die Autoren.\n",
    "    '''\n",
    "    \n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    \n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    authorsDetected = False\n",
    "    setChainStart = True\n",
    "    startIndexAuthors = -1\n",
    "    endIndexAuthors = -1\n",
    "    chainStartIndex = -1\n",
    "    changedText = \"\"\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    \n",
    "    if not df_PER.empty and df_PER[\"start\"].iloc[0] == 0:\n",
    "        for index in index_df_PER_List:\n",
    "            if index < len(index_df_PER_List) - 1:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "            else:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "            firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "            onlyAnd = textBetweenNames == andTyp\n",
    "            if setChainStart: \n",
    "                chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            if not onlyPunctuation and not onlyAnd:      \n",
    "                '''Wenn ein Substring, der in textBetweenNames gespeichert wird, nicht nur Satzzeichen oder ein \"und\" ist, \n",
    "                dann ist es nicht Bestandteil einer Namenskette und somit ein echter Substring zwischen Autorenketten. \n",
    "                Solch ein Substring heißt im Folgenden \"echtes textBetweenNames\".\n",
    "                \n",
    "                Beispiel: \n",
    "                Der Stringteil \"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner\" ist eine Autorenkette. Der Teil\n",
    "                \", and\" gehört folglich mit zur Autorenkette und ist kein echtes textBetweenNames.\n",
    "                \n",
    "                Weil Autoren immer am Anfang des Literaturstrings stehen gilt: Ist ein echtes TextBetweenNames \n",
    "                erkannt worden, das direkt hinter der Autorenkette mit Stardindex 0 folgt, so muss vor diesem TextBetweenNames\n",
    "                eine Namenskette stehen, die die Autoren enthält. Daher setChainStart = True.\n",
    "                '''    \n",
    "                setChainStart = True\n",
    "                startIndexAuthors = chainStartIndex\n",
    "                endIndexAuthors = df_PER[\"end\"].iloc[index]\n",
    "                break\n",
    "        if startIndexAuthors > -1:\n",
    "            changedText, author = replaceSubstring(startIndexAuthors, endIndexAuthors, text, \".\")\n",
    "            author = processNames(author)\n",
    "            return changedText, author\n",
    "    return text, \"\"\n",
    "\n",
    "def getEditors(text):\n",
    "    \n",
    "    '''\n",
    "    Schneidet die Editoren aus dem original Literaturstring aus. Es können mehrere Editoren vorliegen und diese\n",
    "    können mit einem \"and\" oder \"&\" verknüpft sein. Die Funktion soll alle Editoren einschließlich dem \"and\" und \"&\"\n",
    "    in einem Block extrahieren. Dieser Block ist sinnbildlich eine Kette von Namen.\n",
    "    Wenn eine solche Kette von Namen gefunden wird, wird setChainStart = True gesetzt und geprüft, \n",
    "    ob es sich auch um Editoren handelt.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Editoren und die ausgeschnittenen Editoren.\n",
    "    '''\n",
    "    \n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    editorRegEx = \"\\s*(\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\\s*\"\n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    setChainStart = True\n",
    "    isEditor = False\n",
    "    startIndexEditors = -1\n",
    "    endIndexEditors = -1\n",
    "    chainStartIndex = -1\n",
    "    startIndexEditorMarker = -1\n",
    "    endIndexEditorMarker = -1\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    if not df_PER.empty:\n",
    "        for index in index_df_PER_List:\n",
    "            if index < len(index_df_PER_List) - 1:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "            else:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "            firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "            onlyAnd = textBetweenNames == andTyp\n",
    "            if setChainStart: \n",
    "                chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            if not onlyPunctuation and not onlyAnd:\n",
    "                '''\n",
    "                Zur Erklärung des Codes siehe analoge Implementierung in getAuthors\n",
    "                '''\n",
    "                setChainStart = True\n",
    "                textFromStartUntilFirstName = text[0:df_PER[\"start\"].iloc[0]]\n",
    "                isEditor, startIndexEditorMarker, endIndexEditorMarker = is_Editor(editorRegEx, textFromStartUntilFirstName, 0, False)\n",
    "                if startIndexEditorMarker == -1:\n",
    "                    isEditor, startIndexEditorMarker, endIndexEditorMarker = is_Editor(editorRegEx, textBetweenNames, df_PER[\"end\"].iloc[index])                         \n",
    "                if isEditor:\n",
    "                    startIndexEditors = chainStartIndex\n",
    "                    endIndexEditors = df_PER[\"end\"].iloc[index]\n",
    "                    break\n",
    "    if startIndexEditors > -1:\n",
    "        changedText, editor = replaceSubstring(startIndexEditors, endIndexEditors, text, \".\")\n",
    "        editor = processNames(editor)\n",
    "        if startIndexEditorMarker > -1:\n",
    "            if startIndexEditorMarker > startIndexEditors:\n",
    "                startIndexEditorMarker = startIndexEditorMarker - (len(text) - len(changedText))\n",
    "                endIndexEditorMarker = endIndexEditorMarker - (len(text) - len(changedText))\n",
    "            changedText, buffer = replaceSubstring(startIndexEditorMarker, endIndexEditorMarker, changedText, \".\")\n",
    "        startIndexIn = 0 \n",
    "        if startIndexEditorMarker < startIndexEditors:\n",
    "            startIndexEditors = startIndexEditors - (endIndexEditorMarker - startIndexEditorMarker)\n",
    "            endIndexEditors = endIndexEditors - (endIndexEditorMarker - startIndexEditorMarker)\n",
    "        for i in range(startIndexEditors-1, -1, -1):\n",
    "            if isSpeceficPunctuation(changedText[i], [\":\", \" \"]):\n",
    "                startIndexIn = i + 1\n",
    "                break\n",
    "        changedText, replacedEditorMarker = replaceSubstring(startIndexIn, startIndexEditors, changedText, \".\")\n",
    "        return changedText, editor\n",
    "    return text, \"\"\n",
    "    \n",
    "def getPersonTags(text):\n",
    "    \n",
    "    '''\n",
    "    Gibt die durch Named Entity Recognition erkannten dictionaries vom Typ Person in einem Dataframe zurück.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Person.\n",
    "    '''\n",
    "    \n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty: \n",
    "        return df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getORGTag(text, score):\n",
    "    \n",
    "    '''\n",
    "    Gibt die durch Named Entity Recognition erkannten dictionaries vom Typ Organization in einem Dataframe zurück.\n",
    "    Um in das Dataframe aufgenommen zu werden, muss ein bestimmter Score erreicht sein.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    score: Schwellenwert zwischen 0 und 1.\n",
    "    \n",
    "    return: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Organization.\n",
    "    '''\n",
    "    \n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"ORG\") & (df_outputs[\"score\"] >= score)].reset_index(drop=True).tail(1)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getLOCTag(text):\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    Gibt die durch Named Entity Recognition erkannten dictionaries vom Typ Location in einem Dataframe zurück.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Location.\n",
    "    '''\n",
    "    \n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"LOC\")].reset_index(drop=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getDoi(text):\n",
    "    \n",
    "    '''\n",
    "    Schneidet die DOI aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne DOI und die ausgeschnittene DOI.\n",
    "    '''\n",
    "    \n",
    "    doiUrlRegEx1 = \"https:\\/\\/doi\\.org(\\/[^\\s]*)?$\"\n",
    "    doiUrlRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org)?([^\\s]*)+$\"\n",
    "    changedText, doi  = getSubstringByRegEx(text, [doiUrlRegEx1, doiUrlRegEx2])\n",
    "    httpsDomainRegEx1 = \"https:\\/\\/doi\\.org\\/\"\n",
    "    httpsDomainRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org\\/)?\"\n",
    "    doi, httpsDomain = getSubstringByRegEx(doi, [httpsDomainRegEx1, httpsDomainRegEx2])\n",
    "    return changedText, custom_strip(doi)\n",
    "\n",
    "def getURL(text):\n",
    "        \n",
    "    '''\n",
    "    Schneidet die URL aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne URL und die ausgeschnittene URL.\n",
    "    '''\n",
    "    \n",
    "    urlRegEx = \"(URL:|url:)?\\s*https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?::\\d+)?(?:/[^\\s]*)?\"\n",
    "    changedText, url = getSubstringByRegEx(text, [urlRegEx])\n",
    "    urlPrefixRegEx = r\"(url:\\s*|URL:\\s*)\"\n",
    "    url = re.sub(urlPrefixRegEx, '', url).strip()\n",
    "    return changedText, custom_strip(url)\n",
    "\n",
    "def getDate(text):\n",
    "            \n",
    "    '''\n",
    "    Schneidet Date aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Date und ausgeschnittene Date.\n",
    "    '''\n",
    "    \n",
    "    monthYearRegex = \"(January|Jan\\.?|February|Feb\\.?|March|Mar\\.?|April|Apr\\.?\" \\\n",
    "    \"|May|May\\.?|June|Jun\\.?|July|Jul\\.?|August|Aug\\.?|September|Sep\\.?|Sept\\.?|October|\" \\\n",
    "    \"Oct\\.?|November|Nov\\.?|December|Dec\\.?)\\s\\d{4}\"\n",
    "    changedText, monthYear  = getSubstringByRegEx(text, [monthYearRegex])\n",
    "    if monthYear == \"\":\n",
    "        yearRegEx1 = \"(\\.|,)? \\(\\d{4}\\)(\\.|,|:)\"\n",
    "        yearRegEx2 = \"(\\.|,) \\d{4}(\\.|,|;)\"\n",
    "        changedText, year  = getSubstringByRegEx(text, [yearRegEx1, yearRegEx2])\n",
    "        return changedText, \"\", f'{year}'\n",
    "    monthYear = monthYear.split(' ')\n",
    "    return changedText, f'{monthYear[0]}', f'{monthYear[1]}'\n",
    "\n",
    "def getPage(text):\n",
    "                \n",
    "    '''\n",
    "    Schneidet Page aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Page und ausgeschnittene Page.\n",
    "    '''\n",
    "    \n",
    "    pageRegEx = \"(?:pp\\.? )?\\d+(-|--|–)\\d+\"\n",
    "    changedText, pages = getSubstringByRegEx(text, [pageRegEx])\n",
    "    if pages != \"\":\n",
    "        pages = re.search(r'\\d+(-|--|–)\\d+', pages).group()\n",
    "    return changedText, custom_strip(pages)\n",
    "\n",
    "def getVolumeNumber(text):\n",
    "                    \n",
    "    '''\n",
    "    Schneidet Volume und Number aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Volume und Number und ausgeschnittene Volume und Number.\n",
    "    '''\n",
    "    \n",
    "    volumeAndNumberRegex = \"(\\d+\\(\\d+\\)|\\d+\\.\\d+)\"\n",
    "    startIndex, endIndex, substring = getIndexOfSubstring(text, [volumeAndNumberRegex], True)\n",
    "    if startIndex > -1:\n",
    "        changedText, volumeNumber = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        startIndex, endIndex, volume = getIndexOfSubstring(volumeNumber, [\"\\d+\"])\n",
    "        startIndex, endIndex, number = getIndexOfSubstring(volumeNumber, [\"\\d+\"], True)\n",
    "        return changedText, volume, number\n",
    "    else:\n",
    "        volumeRegEx = \"(V|v)ol\\. \\d+\"\n",
    "        volumeRegEx2 = \", \\d+,\"\n",
    "        volumeRegEx3 = \",? \\d+(:|\\.)\"\n",
    "        number1RegEx = \"no\\. \\d+\"\n",
    "        number2RegEx = \"Issue \\d+\"\n",
    "        number3RegEx = \", \\d+,\"\n",
    "        number4RegEx = \"\\.\\d+\"\n",
    "        startIndex, endIndex, volume = getIndexOfSubstring(text, [volumeRegEx, volumeRegEx2, volumeRegEx3], True)\n",
    "        changedText, substring = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        startIndex, endIndex, number = getIndexOfSubstring(changedText, [number1RegEx, number2RegEx, number3RegEx, number4RegEx], True)\n",
    "        changedText, substring = replaceSubstring(startIndex, endIndex, changedText, \"\")\n",
    "        if volume != \"\":\n",
    "            volume = re.search(r'\\d+', volume).group(0)\n",
    "        if number != \"\":\n",
    "            number = re.search(r'\\d+', number).group(0)\n",
    "        return changedText, volume, number\n",
    "\n",
    "def getEdition(text):\n",
    "                        \n",
    "    '''\n",
    "    Schneidet Edition aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Edition und ausgeschnittene Edition.\n",
    "    '''\n",
    "    \n",
    "    editionRegEx1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "    editionRegEx2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "    changedText, edition = getSubstringByRegEx(text, [editionRegEx1, editionRegEx2])\n",
    "    if edition != \"\":\n",
    "        edition = re.search(r'\\d+', edition).group()\n",
    "    return changedText, custom_strip(edition)\n",
    "\n",
    "def getAddress(text):\n",
    "                            \n",
    "    '''\n",
    "    Schneidet Address aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Address und ausgeschnittene Address.\n",
    "    '''\n",
    "    \n",
    "    df_LOC = getLOCTag(text)\n",
    "    addressFound = False\n",
    "    index_df_Loc_List = df_LOC.index.values.tolist()\n",
    "    textBetweenAddress = \"\"\n",
    "    setChainStart = True\n",
    "    startIndex = 0\n",
    "    endIndex = 0\n",
    "    if not df_LOC.empty:\n",
    "        for index in reversed(index_df_Loc_List):\n",
    "            if index < len(index_df_Loc_List) and index > 0:\n",
    "                textBetweenAddress = text[df_LOC[\"end\"].iloc[index-1]:df_LOC[\"start\"].iloc[index]]\n",
    "            else:\n",
    "                textBetweenAddress = text[:df_LOC[\"start\"].iloc[index]]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenAddress, [])\n",
    "            if setChainStart: \n",
    "                chainEnIndex = df_LOC[\"end\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            if not onlyPunctuation:\n",
    "                startIndex = df_LOC[\"start\"].iloc[index]\n",
    "                endIndex = chainEnIndex\n",
    "                break\n",
    "        address = text[startIndex:endIndex]\n",
    "        if startIndex > 2 and endIndex < len(text) - 1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                addressFound = True\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                addressFound = True\n",
    "        if addressFound:\n",
    "            changedText, address = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "            return changedText, custom_strip(address)\n",
    "    return text, \"\"  \n",
    "\n",
    "def getPublisher(text, doi):\n",
    "                                \n",
    "    '''\n",
    "    Schneidet Publisher aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    doi: DOI, um Publisher in externen Datenbanken zu suchen.\n",
    "    \n",
    "    return: Literaturstring ohne Publisher und ausgeschnittenen Publisher.\n",
    "    '''\n",
    "    \n",
    "    publisher = \"\"\n",
    "    if doi != \"\":\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            publisher = data['message'].get('publisher', 'Publisher not found')\n",
    "    if publisher != \"\":\n",
    "        startIndex, endIndex, publisher = getIndexOfSubstring(text, [publisher], True)\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, publisher\n",
    "        changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        if publisher != \"\":\n",
    "            return changedText, custom_strip(publisher)\n",
    "    df_ORG = getORGTag(text, 0.8)\n",
    "    if not df_ORG.empty:\n",
    "        startIndex = df_ORG[\"start\"].iloc[0]\n",
    "        endIndex = df_ORG[\"end\"].iloc[0]\n",
    "        publisher = text[startIndex:endIndex]\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "    return text, \"\"\n",
    "    \n",
    "def getTitel(text):\n",
    "                                    \n",
    "    '''\n",
    "    Schneidet Titel aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Titel und ausgeschnittenen Titel.\n",
    "    '''\n",
    "    \n",
    "    ignoreCharacters = [\"?\", \"!\", \"(\", \")\", \"“\", \"”\", \"\\\"\"]\n",
    "    text = custom_strip(text, ignoreCharacters)\n",
    "    ignoreCharacters = [\"?\", \":\", \"-\", \"(\", \")\", \"“\", \"”\", \"\\\"\"]\n",
    "    limit = len(text) - 1\n",
    "    i = 0\n",
    "    maxIndex = 0\n",
    "    while i < limit:\n",
    "        if (i + 2 < limit) and not (text[i] == \",\" and text[i+1] == \" \" and not isSpeceficPunctuation(text[i+2])):\n",
    "            if isSpeceficPunctuation(text[i], ignoreCharacters) and isSpeceficPunctuation(text[i+1], ignoreCharacters):\n",
    "                text = text[:i] + \".\" + text[i+2:]\n",
    "                i = i - 1\n",
    "                limit = limit - 1\n",
    "        i = i +1\n",
    "    \n",
    "    ignoreCharacters = [\"?\", \"!\", \"(\", \")\"]\n",
    "    if text[0] == \"“\":\n",
    "        text = text.rsplit('”', 1)\n",
    "        return custom_strip(text[0], ignoreCharacters), custom_strip(text[1], ignoreCharacters), \"\"\n",
    "    elif text[0] == \"\\\"\":\n",
    "        text = text.rsplit(\"\\\"\", 1)\n",
    "        return custom_strip(text[0], ignoreCharacters), custom_strip(text[1], ignoreCharacters), \"\"\n",
    "    elif text.count(\".\") == 1:\n",
    "        text = text.split(\".\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    elif text.count(\".\") == 2:\n",
    "        text = text.split(\".\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), custom_strip(text[2])\n",
    "    else:\n",
    "        for index, element in enumerate(text):\n",
    "            if isSpeceficPunctuation(element, [\".\", \",\", \" \", \"(\", \")\", \":\"]):\n",
    "                if maxIndex < index:\n",
    "                    maxIndex = index\n",
    "        if maxIndex > 0:\n",
    "            text = text.split(text[maxIndex])\n",
    "            return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    if maxIndex == 0 and text.count(\",\") == 1: \n",
    "        text = text.split(\",\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    elif maxIndex == 0 and text.count(\",\") > 1:\n",
    "        text = text.rsplit(',', 1)\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    return custom_strip(text), \"\", \"\"\n",
    "    \n",
    "    \n",
    "def getKey(author, year):\n",
    "                                        \n",
    "    '''\n",
    "    Erzeugt einen Key aus dem Nachnamen des ersten Autors und dem Jahr.\n",
    "    \n",
    "    Parameter:\n",
    "    author: Autoren.\n",
    "    year: Jahr.\n",
    "    \n",
    "    return: Erzeugter Schlüssel.\n",
    "    '''\n",
    "    \n",
    "    lastNameFirstAuthor = author.split(\" and \")[0].strip().split(\" \")[-1]\n",
    "    return f'{lastNameFirstAuthor}_{year}'\n",
    "\n",
    "\n",
    "def create_bibtex(text):\n",
    "    address = \"\"\n",
    "    author = \"\"\n",
    "    booktitle = \"\"\n",
    "    chapter = \"\"\n",
    "    doi = \"\"\n",
    "    edition = \"\"\n",
    "    editor = \"\"\n",
    "    howpublished = \"\"\n",
    "    isbn = \"\"\n",
    "    journal = \"\"\n",
    "    key = \"\"\n",
    "    month = \"\"\n",
    "    note = \"\"\n",
    "    number = \"\"\n",
    "    organization = \"\"\n",
    "    pages = \"\"\n",
    "    publisher = \"\"\n",
    "    school = \"\"\n",
    "    series = \"\"\n",
    "    title = \"\"\n",
    "    url = \"\"\n",
    "    volume = \"\"\n",
    "    year = \"\"\n",
    "    key = \"\"\n",
    "    isBook = False\n",
    "    isProceedings = False\n",
    "    isInProceedings = False\n",
    "    isIncollection = False\n",
    "    isArticle = False\n",
    "    \n",
    "    text, author = getAuthors(text)\n",
    "    text, editor = getEditors(text)\n",
    "    text, doi = getDoi(text)\n",
    "    text, url = getURL(text)\n",
    "    text, month, year = getDate(text)\n",
    "    text, pages = getPage(text)\n",
    "    text, volume, number = getVolumeNumber(text)\n",
    "    text, edition = getEdition(text)\n",
    "    text, address = getAddress(text)\n",
    "    text, publisher = getPublisher(text, doi)\n",
    "    school = publisher\n",
    "    title, booktitle, series = getTitel(text)\n",
    "    journal = booktitle\n",
    "    if author != \"\":\n",
    "        key = getKey(author, year)\n",
    "    else:\n",
    "        key = getKey(editor, year)\n",
    "    \n",
    "    bookFields = [author, title, publisher, year, volume, number, \\\n",
    "                  series, address, edition, month, note, key, editor, \\\n",
    "                  howpublished, organization, chapter, pages, isbn, url]\n",
    "    inproceedingsFields = [author, title, booktitle, year, editor, volume, \\\n",
    "                            number, series, pages, address, month, organization, \\\n",
    "                            publisher, note, key, doi, url]\n",
    "    proceedingsFields = [title, year, editor, volume, number, series, \\\n",
    "                          address, month, organization, publisher, note, key, doi, url]\n",
    "    incollectionFields = [author, title, booktitle, publisher, year, editor, \\\n",
    "                           volume, number, series, chapter, pages, address, \\\n",
    "                           edition, month, note, key, doi, url]\n",
    "    articleFields = [author, title, journal, year, volume, number, \\\n",
    "                      pages, month, note, key, doi, url]\n",
    "    phdthesisFields = [author, title, publisher, year, address, month, \\\n",
    "                        note, key, doi, url]\n",
    "    \n",
    "    bookFieldsString = [\"author\", \"title\", \"publisher\", \"year\", \"volume\", \"number\", \\\n",
    "                  \"series\", \"address\", \"edition\", \"month\", \"note\", \"key\", \"editor\", \\\n",
    "                  \"howpublished\", \"organization\", \"chapter\", \"pages\", \"isbn\", \"url\"]\n",
    "    inproceedingsFieldsString  = [\"author\", \"title\", \"booktitle\", \"year\", \"editor\", \"volume\", \\\n",
    "                            \"number\", \"series\", \"pages\", \"address\", \"month\", \"organization\", \\\n",
    "                            \"publisher\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    proceedingsFieldsString  = [\"title\", \"year\", \"editor\", \"volume\", \"number\", \"series\", \\\n",
    "                          \"address\", \"month\", \"organization\", \"publisher\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    incollectionFieldsString  = [\"author\", \"title\", \"booktitle\", \"publisher\", \"year\", \"editor\", \\\n",
    "                           \"volume\", \"number\", \"series\", \"chapter\", \"pages\", \"address\", \\\n",
    "                           \"edition\", \"month\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    articleFieldsString  = [\"author\", \"title\", \"journal\", \"year\", \"volume\", \"number\", \\\n",
    "                      \"pages\", \"month\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    phdthesisFieldsString  = [\"author\", \"title\", \"school\", \"year\", \"address\", \"month\", \\\n",
    "                        \"note\", \"key\", \"doi\", \"url\"]\n",
    "\n",
    "    model = \"LaLaf93/LiteratureTyp_recognizer\"\n",
    "    classifier = pipeline(\"text-classification\", model=model)\n",
    "    literatureType = classifier(title + \".\" + booktitle)[0]['label']\n",
    "    \n",
    "    bibTex = \"@\"\n",
    "    if literatureType == \"book\":\n",
    "        zippedFieldsValues = zip(bookFieldsString, bookFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"book{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"proceedings\":\n",
    "        zippedFieldsValues = zip(proceedingsFieldsString, proceedingsFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"proceedings{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"inproceedings\":\n",
    "        zippedFieldsValues = zip(inproceedingsFieldsString, inproceedingsFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"inproceedings{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"incollection\":\n",
    "        zippedFieldsValues = zip(incollectionFieldsString, incollectionFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"incollection{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"article\":\n",
    "        zippedFieldsValues = zip(articleFieldsString, articleFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"article{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    else:\n",
    "        zippedFieldsValues = zip(phdthesisFieldsString, phdthesisFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"phdthesis{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    \n",
    "    bibTex += '}'\n",
    "\n",
    "    return bibTex \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1358ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@inproceedings{LeCun_1998, \n",
      "author={Yann LeCun and Léon Bottou and Yoshua Bengio and Patrick Haffner},\n",
      "title={Gradient-based learning applied to document recognition},\n",
      "booktitle={In Proceedings of the IEEE, IEEE},\n",
      "year={1998},\n",
      "editor={},\n",
      "volume={},\n",
      "number={},\n",
      "series={},\n",
      "pages={},\n",
      "address={New York, USA},\n",
      "month={},\n",
      "organization={},\n",
      "publisher={},\n",
      "note={},\n",
      "key={LeCun_1998},\n",
      "doi={10.1109/5.726791},\n",
      "url={},\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#text=\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "#text = \"M. A. Nielsen and I. L. Chuang, “Quantum Computation and Quantum Information,” in Handbook of Quantum Information Science, vol. 4, C. H. Bennett and D. P. DiVincenzo, Eds. Berlin, Germany: Springer, 2026, pp. 250–300. doi: 10.1007/springerreference-303198.\"\n",
    "#text = \"Nielsen, M. A.; Chuang, I. L. Quantum Computation and Quantum Information. In Handbook of Quantum Information Science; Bennett, C. H., DiVincenzo, D. P., Eds.; Quantum Science and Technology; Springer: Berlin, Germany, 2026; Vol. 4, pp 250–300. https://doi.org/10.1007/springerreference-303198.\"\n",
    "\n",
    "#BUG: startIndexReplace={-1} ist hier bei getYear! Deswegen doppelter String drin\n",
    "#text = \"\"\"Alahmed, Y., Abadla, R., Badri, A. A., & Ameen, N. (2023). “How Does ChatGPT Work” Examining Functionality, To The Creative AI CHATGPT on X’s (Twitter) Platform. 2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS), 1–7. https://doi.org/10.1109/SNAMS60348.2023.10375450\"\"\"\n",
    "#text = \"David Mertz, Regular Expression Puzzles and AI Coding Assistants: 24 puzzles solved by the author, with and without assistance from Copilot, ChatGPT and more , Manning, 2023.\"\n",
    "#text = \"\"\"Mohammed Baziyad, Ibrahim Kamel, and Tamer Rabie. 2023. On the Linguistic Limitations of ChatGPT: An Experimental Case Study. In 2023 International Symposium on Networks, Computers and Communications (ISNCC), 1–6. DOI:https://doi.org/10.1109/ISNCC58260.2023.10323661\"\"\"\n",
    "#text = \"\"\"K. M. Caramancion, \"Harnessing the Power of ChatGPT to Decimate Mis/Disinformation: Using ChatGPT for Fake News Detection,\" 2023 IEEE World AI IoT Congress (AIIoT), Seattle, WA, USA, 2023, pp. 0042-0046, doi: 10.1109/AIIoT58121.2023.10174450.\"\"\"\n",
    "#text = \"\"\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence? In: J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\"\"\"\n",
    "#text = \"\"\"A. Einstein, B. Podolsky, and N. Rosen, “Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?,” Physical Review, vol. 47, no. 10, pp. 777–780, May 1935, doi: 10.1103/PhysRev.47.777.\"\"\"\n",
    "#text = \"\"\"Badaro, G., Saeed, M., & Papotti, P. (2023). Transformers for tabular data representation: a survey of models and applications. Transactions of the Association for Computational Linguistics, 11, pp. 227–249. URL: https://aclanthology.org/2023.tacl-1.14, doi:10.1162/tacl_a_00544\"\"\"\n",
    "#text = \"\"\"Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics 11, (2023), 1–17. URL: https://aclanthology.org/2023.tacl-1.1, doi:10.1162/tacl_a_00530\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. (2017). Joint prediction of word alignment with alignment types. Transactions of the Association for Computational Linguistics, 5, pp. 501–514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. 2017. \"Joint Prediction of Word Alignment with Alignment Types.\" Transactions of the Association for Computational Linguistics, 5: 501–514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076.\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. 2017. ‘Joint Prediction of Word Alignment with Alignment Types.\" Transactions of the Association forComputational Linguistics, 5: 501-514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076.\"\"\"\n",
    "#text = \"\"\"Devika K, Hariprasath .s.b, Haripriya B, Vigneshwar E, Premjith B, and Bharathi Raja Chakravarthi. From dataset to detection: a comprehensive approach to combating Malayalam fake news. In Bharathi Raja Chakravarthi, Ruba Priyadharshini, Anand Kumar Madasamy, Sajeetha Thavareesan, Elizabeth Sherly, Rajeswari Nadarajan, and Manikandan Ravikiran, editors, Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 16–23, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL: https://aclanthology.org/2024.dravidianlangtech-1.3.\"\"\"\n",
    "#text = \"\"\"R, Jairam, G, Jyothish, and B, Premjith. \"A few-shot multi-accented speech classification for Indian languages using transformers and LLM's fine-tuning approaches.\" Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages. Eds. Chakravarthi, Bharathi Raja, Priyadharshini, Ruba, Madasamy, Anand Kumar, Thavareesan, Sajeetha, Sherly, Elizabeth, Nadarajan, Rajeswari, and Ravikiran, Manikandan. St. Julian's, Malta: Association for Computational Linguistics, 2024. 1–9. URL: https://aclanthology.org/2024.dravidianlangtech-1.1\"\"\"\n",
    "#text = \"\"\"Rozovskaya, Alla, Roth, Dan, and Sammons, Mark. \"Adapting to learner errors with minimal supervision.\" Computational Linguistics 43.4 (2017): 723–760. https://aclanthology.org/J17-4002, doi:10.1162/COLI_a_00299\"\"\"\n",
    "#text = \"\"\"Eds. Alonso, Jose M., and Catala, Alejandro. Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019) (Vol. 1, Issue 5, pp. 100–120). 2019. URL: https://aclanthology.org/W19-8400\"\"\"\n",
    "#text = \"\"\"Hawking, S., Greene, B., Trump, M., & Soy, S. (2025). Advanced Concepts in Theoretical Physics (3rd ed., Vol. 1). Cambridge University Press. https://doi.org/10.1017/example.book.2025\"\"\"\n",
    "#text = \"\"\"LeCun, Yann, et al. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE, vol. 86, no. 11, IEEE, 1998, pp. 2278–324, doi:10.1109/5.726791.\"\"\"\n",
    "text = \"\"\"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, IEEE, New York, USA, 2278–2324. DOI:https://doi.org/10.1109/5.726791\"\"\"\n",
    "#text = \"\"\"Stuart Russell and Peter Norvig. 2016. Artificial Intelligence: A Modern Approach (3rd ed.). Pearson, Upper Saddle River, NJ, USA.\"\"\"\n",
    "#text = \"\"\"Neele Falk, Sara Papi, and Mike Zhang. 2024. The Impact of Integration Step on Integrated Gradients. Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop. Association for Computational Linguistics, St. Julian's, Malta, (March 2024). URL: https://aclanthology.org/2024.eacl-srw.0\"\"\"\n",
    "print(create_bibtex(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbff7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
