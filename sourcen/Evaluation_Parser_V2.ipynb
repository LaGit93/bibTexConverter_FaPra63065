{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8acde04b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "\n",
    "def custom_strip(text, replaceCharacter = []):\n",
    "    \n",
    "    '''\n",
    "    Strip-Funktion, die standardmäßig neben Whitespace auch Zeichen aus string.punctuation und die \n",
    "    Zeichen “ und ” entfernt.\n",
    "    \n",
    "    Parameter:\n",
    "    text: String, der gestripped werden soll.\n",
    "    replaceCharacter: Liste von Zeichen, die für den Strip nicht berücksichtigt werden sollen.\n",
    "    \n",
    "    return: gestrippter String.\n",
    "    '''\n",
    "    \n",
    "    allowed_chars = string.punctuation + string.whitespace + \"“\" + \"”\"\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return text.strip(allowed_chars)\n",
    "\n",
    "\n",
    "def getIndexOfSubstring(text, regEx = [], reverse = False):\n",
    "    \n",
    "    '''\n",
    "    Prüft für eine Liste von RegEx, ob sie im String namens text vorkommen. Pro RegEx wird nur \n",
    "    der erste Match berücksichtigt. Der Match, der den Substring mit der größten Länge ermittelt, kommt zum Zuge. \n",
    "    \n",
    "    Parameter:\n",
    "    text: Text, wo das Auftreten des RegEx geprüft wird.\n",
    "    regEx = []: Liste mit RegEx. \n",
    "    reverse: Ob das erste oder letzte Auftreten eines Matches geprüft maßgeblich ist. Wenn reverse = False, dann wird das\n",
    "    erste Auftreten geprüft.\n",
    "    \n",
    "    return: Substring und den zugehörigen Start- und Endindex.\n",
    "    '''\n",
    "    \n",
    "    length = 0\n",
    "    matches = []\n",
    "    substring = \"\"\n",
    "    for regExElement in regEx:\n",
    "        matches = list(re.finditer(regExElement, text))\n",
    "        if matches:\n",
    "            if reverse:\n",
    "                match = matches[-1]\n",
    "            else:\n",
    "                match = matches[0]\n",
    "            buffer = match.end() - match.start()\n",
    "            if buffer > length:\n",
    "                length = buffer\n",
    "                startIndex = match.start()\n",
    "                endIndex = match.end()\n",
    "                substring = text[match.start():match.end()]\n",
    "    if substring != \"\":\n",
    "        return startIndex, endIndex, substring   \n",
    "    return -1, -1, substring\n",
    "\n",
    "def getSubstringByRegEx(text, regex = []):\n",
    "    \n",
    "    '''\n",
    "    Prüft für eine Liste von RegEx, ob sie im String namens text vorkommen. Der RegEx, der den Substring mit der grötßen\n",
    "    Länge ermittelt, kommt zum Zuge. Dieser Substring wird dann aus dem text ausgeschnitten. Ein RegEx wird\n",
    "    dabei von hinten beginnend in text geprüft und der erste Match zählt.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    regEx = []: Liste mit RegEx. \n",
    "    \n",
    "    return: String ohne Substring (changedText) und ausgeschnittenen Substring\n",
    "    '''\n",
    "    \n",
    "    startIndex, endIndex, substring = getIndexOfSubstring(text, regex, True)\n",
    "    changedText, substring = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "    return changedText, custom_strip(substring)\n",
    "\n",
    "def replaceSubstring (startIndex, endIndex, text, substituteString, ignorePunctuation = [\"&\", \"(\", \")\"]):\n",
    "    \n",
    "    '''\n",
    "    Ersetzt in dem String namens text einen Substring durch einen anderen String namens substituteString.\n",
    "    Die Variablen startIndex und endIndex können dabei noch verändert werden, webb vor der Postion startIndex oder nach der\n",
    "    Position endIndex bestimmte Zeichen folgen, die mit entfernt werden sollen. Die Existenz dieser bestimmten Zeichen \n",
    "    wird mit der Funktion isSpeceficPunctuation geprüft. Mit dem Parameter ignorePunctuation wird auf die Prüfung\n",
    "    bestimmter Zeichen in der Funktion isSpeceficPunctuation verzichtet.\n",
    "    \n",
    "    Parameter:\n",
    "    startIndex: Index, wo der zu ersetztende Substring im String namens text eingefügt werden soll.\n",
    "    endIndex: Index, wo der zu ersetztende Substring im String text enden soll.\n",
    "    text: Text, wo das Auftreten des Substrings geprüft wird.\n",
    "    substituteString: Der einzufügende Substring.\n",
    "    ignorePunctuation: Zeichen, die für die Indexverschiebung nicht berücksichtigt werden sollen.\n",
    "    \n",
    "    return: Substring und den zugehörigen Start- und Endindex.\n",
    "    '''\n",
    "    \n",
    "    if endIndex > 0:\n",
    "        startIndexReplace = 0\n",
    "        endIndexReplace = 0\n",
    "        if startIndex > 0:\n",
    "            for i in range(startIndex, -1, -1):\n",
    "                if isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    startIndexReplace = i + 1\n",
    "                    break\n",
    "        else:\n",
    "            startIndexReplace = 0            \n",
    "        if endIndex < len(text):\n",
    "            for i in range(endIndex-1, len(text), 1):\n",
    "                if isSpeceficPunctuation(text[i], ignorePunctuation):\n",
    "                    endIndexReplace = i + 1\n",
    "                    break\n",
    "                elif i == len(text)-1:\n",
    "                    endIndexReplace = len(text)\n",
    "        else:\n",
    "            endIndexReplace = len(text)\n",
    "        if endIndexReplace > 0:\n",
    "            changedText = text[0:startIndexReplace] + substituteString + text[endIndexReplace:len(text)]\n",
    "            return changedText, text[startIndexReplace:endIndexReplace]\n",
    "    return text, \"\"\n",
    "\n",
    "def is_SurenameFirst(names):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob die Autoren- oder Editornamen mit dem Vornamen beginnen. \n",
    "    \n",
    "    Parameter:\n",
    "    names: Substring vom Literaturstring, der die Autoren- oder Editornamen enthält.\n",
    "    \n",
    "    return: True, wenn die Namen mit dem Vornamen beginnen. Ansonsten False.\n",
    "    '''\n",
    "    \n",
    "    splitedNames = names.split(\" \")\n",
    "    #regex wie w+ erkennt bspw. KEIN è \n",
    "    if splitedNames[0].endswith(\".\"):\n",
    "        return True\n",
    "    splitedNames = names.split(\",\")\n",
    "    if all(\" \" in item.strip() for item in splitedNames):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "def is_NameShortened(df_PER):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob die Namen mit einem Punkt abgekürzt sind.\n",
    "    \n",
    "    Parameter:\n",
    "    df_Per: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Person.\n",
    "    \n",
    "    return: True, falls der Name mit einem Punkt abgekürzt ist. Ansonsten False.\n",
    "    '''\n",
    "    \n",
    "    for index in df_PER.index.values.tolist():\n",
    "        if \".\" == text[df_PER[\"end\"].iloc[index]] and len(text[df_PER[\"start\"].iloc[index]:df_PER[\"end\"].iloc[index] + 1]) == 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def isSpeceficPunctuation(text, replaceCharacter = []):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob ein Srting nur aus bestimmten Satzzeichen besteht. Standardmäßig wird string.punctuation + string.whitespace\n",
    "    geprüft.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Text, der geprüft werden soll.\n",
    "    replaceCharacter = []: Liste von Zeichen, die aus der standardmäßigen Prüfung entfernt werden sollen.\n",
    "    '''\n",
    "        \n",
    "    allowed_chars = string.punctuation + string.whitespace\n",
    "    for character in replaceCharacter:\n",
    "        allowed_chars = allowed_chars.replace(character, '')\n",
    "    return all(char in allowed_chars for char in text)\n",
    "\n",
    "def is_Editor(editorRegEx, textBetweenNames, startIndexTextBetweenNames, markerBehind = True):\n",
    "    \n",
    "    '''\n",
    "    Prüft, ob in dem String textBetweenNames ein Signalwort für Editoren enthalten ist.\n",
    "    \n",
    "    Parameter:\n",
    "    editorRegEx: RegEx, die Signalwörter für das Auftreten von Editoren erkennen sollen.\n",
    "    textBetweenNames: Substring vom Literaturstring, der geprüft weden soll, ob das Signalwort enthalten ist. Dieser \n",
    "    Substring steht stehts zwischen potententiellen Namen.\n",
    "    startIndexTextBetweenNames: Startindex, wo Substring im original Literaturstring steht. Signalwörter können dabei nicht\n",
    "    Bestandteil von einer Namenskette sein, sondern immer nur zwischen solchen Namensketten. Daher der Name des Parameters.\n",
    "    markerBehind: Ob das Signalwort für Editoren vor oder hinter den Editorennamen im original Literaturstring steht.\n",
    "    \n",
    "    return: Boolean, ob gefunden, und Start- und Endindizies, wo es im original Literaturstring vorkommt.\n",
    "    '''\n",
    "    \n",
    "    startSubstring, endSubstring, substring = getIndexOfSubstring(textBetweenNames, [editorRegEx])\n",
    "    if startIndexTextBetweenNames > -1 and markerBehind:\n",
    "        if isSpeceficPunctuation(textBetweenNames[startIndexTextBetweenNames:startSubstring], [\"&\"]):\n",
    "            ''' Signalwörter, die das Vorliegen von Editoren markieren, werden durch die editorRegEx geprüft.\n",
    "            Es gilt folgende Heuristik: Ein Signalwort für Editoren, das vor oder hinter den Editorenamen steht, \n",
    "            darf nur von bestimmten Satzzeichen/Punkuationen und nicht von Wörtern unterbrochen sein.\n",
    "            Es kann schließlich zufällig sein, dass ein RegEx ein Editor-Signalwort erkennt, \n",
    "            das jedoch eigentlich keins ist, da sie die zuvor genannte Heuristik nicht erfüllen. \n",
    "            \n",
    "            Beispiel: \n",
    "            Gegeben sei folgender Ausschnitt eines Literaturstrings: \"Bennett, C. H., DiVincenzo, D. P., Eds.\"\n",
    "            Das Signalwort \"Eds.\" befindet sich direkt hinter den beiden Autorennamen, da es nur von einem Punkt, Komma und\n",
    "            einem Leerzeichen (also bestimmten Satzzeichen) unterbrochen ist. Also ist die Heuristik erfüllt.\n",
    "            \n",
    "            Dieses Vorgehen gilt für den elif-Teil analog.'''\n",
    "            return True, startSubstring + startIndexTextBetweenNames, endSubstring + startIndexTextBetweenNames\n",
    "    elif startIndexTextBetweenNames > -1:\n",
    "        if isSpeceficPunctuation(textBetweenNames[endSubstring:startIndexTextBetweenNames], [\"&\"]):\n",
    "            return True, startSubstring + startIndexTextBetweenNames, endSubstring + startIndexTextBetweenNames\n",
    "    return False, -1, -1\n",
    "\n",
    "def processNames(authors):\n",
    "    \n",
    "    '''\n",
    "    Bereitet die Autorennamen in ein Standardformat auf.\n",
    "    \n",
    "    Parameter:\n",
    "    authors: Substring aus dem original Literaturstring, der die Autoren enthält.\n",
    "    \n",
    "    return: Standardformat für Autoren.\n",
    "    '''\n",
    "    \n",
    "    finalAuthors = \"\"\n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \"]\n",
    "    surenameFirst = is_SurenameFirst(authors.strip())\n",
    "    authors = custom_strip(authors)\n",
    "    if surenameFirst:\n",
    "        startIndex, endIndex, andInAuthors = getIndexOfSubstring(authors, search_terms)\n",
    "        if startIndex >= 0:\n",
    "            authors = authors.replace(andInAuthors, \" and \")\n",
    "            finalAuthors = authors.replace(\", \", \" and \")\n",
    "        else:\n",
    "            finalAuthors = authors\n",
    "    elif \"., \" in authors:\n",
    "        search_terms = [\"., and \", \"., & \", \". and \", \". & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        authors = authors.replace(\"., \", \"#., \")\n",
    "        if andInAuthors != \"\":\n",
    "            authors = authors.replace(andInAuthors, \"#., \")\n",
    "        authors = authors.split(\"., \")\n",
    "        authors = [name.replace(\"#\",\".\") for name in authors]\n",
    "        authors = [name.replace(\"..\",\".\") for name in authors]\n",
    "        for author in authors[:-1]:\n",
    "            buffer = author.split(\", \")\n",
    "            finalAuthors = finalAuthors + buffer[1] + \" \" + buffer[0] + \" and \"\n",
    "        buffer = authors[-1].split(\", \")\n",
    "        finalAuthors = finalAuthors + buffer[1] + \" \" +  buffer[0]\n",
    "    elif \", \" in authors:\n",
    "        search_terms = [\", and \", \", & \", \" and \", \" & \"]\n",
    "        andInAuthors = getIndexOfSubstring(authors, search_terms)[2]\n",
    "        if andInAuthors != \"\":  \n",
    "            authors = authors.replace(andInAuthors, \", \")\n",
    "        authors = authors.split(\", \")\n",
    "        for i in range(0, len(authors) - 3, 2):\n",
    "            finalAuthors = finalAuthors + authors[i+1] + \" \" + authors[i] + \" and \"\n",
    "        finalAuthors = finalAuthors + authors[len(authors) - 1] + \" \" + authors[len(authors) - 2]\n",
    "    return custom_strip(finalAuthors)\n",
    "\n",
    "def getAuthors(text):\n",
    "    \n",
    "    '''\n",
    "    Schneidet die Autoren aus dem original Literaturstring aus. Es können mehrere Autoren vorliegen und diese\n",
    "    können mit einem \"and\" oder \"&\" verknüpft sein. Die Funktion soll alle Autoren einschließlich dem \"and\" und \"&\"\n",
    "    in einem Block extrahieren. Dieser Block ist sinnbildlich eine Kette von Namen. Die Variable textBetweenNames \n",
    "    beinhaltet die Substrings, die zwischen solchen Namensketten stehen. \n",
    "    Wenn eine solche Kette von Namen gefunden wird, wird setChainStart = True gesetzt und geprüft, \n",
    "    ob es sich auch um Autorennamen handelt.\n",
    "    \n",
    "    Beispiel:\n",
    "    \"Nielsen, M. A.; Chuang, I. L. Quantum Computation and Quantum Information. In Handbook of Quantum Information Science;\n",
    "    Bennett, C. H., DiVincenzo, D. P., Eds.; Quantum Science and Technology; Springer: Berlin, Germany, 2026; Vol. 4, \n",
    "    pp 250–300. https://doi.org/10.1007/springerreference-303198.\"\n",
    "    \n",
    "    Da Autoren und Editoren vorliegen, liegen zwei Abschnitte vor, die textBetweenNames sind, nämlich zwischen Index\n",
    "    28 und 118 sowie zwischen 151 und dem Ende des Strings. Entsprechend liegen zwischen 0 und 27 und 119 bis 150 \n",
    "    Namensketten vor. Ob Sonderzeichen wie der Punkt zur Abkürzung von Nachnamen mit zum Namen gezählt werden, hängt\n",
    "    von der NER ab.\n",
    "\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Autoren und die Autoren.\n",
    "    '''\n",
    "    \n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    \n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    authorsDetected = False\n",
    "    setChainStart = True\n",
    "    startIndexAuthors = -1\n",
    "    endIndexAuthors = -1\n",
    "    chainStartIndex = -1\n",
    "    changedText = \"\"\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    \n",
    "    if not df_PER.empty and df_PER[\"start\"].iloc[0] == 0:\n",
    "        for index in index_df_PER_List:\n",
    "            if index < len(index_df_PER_List) - 1:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "            else:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "            firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "            onlyAnd = textBetweenNames == andTyp\n",
    "            if setChainStart: \n",
    "                chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            if not onlyPunctuation and not onlyAnd:      \n",
    "                '''Wenn ein Substring, der in textBetweenNames gespeichert wird, nicht nur Satzzeichen oder ein \"und\" ist, \n",
    "                dann ist es nicht Bestandteil einer Namenskette und somit ein echter Substring zwischen Autorenketten. \n",
    "                Solch ein Substring heißt im Folgenden \"echtes textBetweenNames\".\n",
    "                \n",
    "                Beispiel: \n",
    "                Der Stringteil \"Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner\" ist eine Autorenkette. Der Teil\n",
    "                \", and\" gehört folglich mit zur Autorenkette und ist kein echtes textBetweenNames.\n",
    "                \n",
    "                Weil Autoren immer am Anfang des Literaturstrings stehen gilt: Ist ein echtes TextBetweenNames \n",
    "                erkannt worden, das direkt hinter der Autorenkette mit Stardindex 0 folgt, so muss vor diesem TextBetweenNames\n",
    "                eine Namenskette stehen, die die Autoren enthält. Daher setChainStart = True.\n",
    "                '''    \n",
    "                setChainStart = True\n",
    "                startIndexAuthors = chainStartIndex\n",
    "                endIndexAuthors = df_PER[\"end\"].iloc[index]\n",
    "                break\n",
    "        if startIndexAuthors > -1:\n",
    "            changedText, author = replaceSubstring(startIndexAuthors, endIndexAuthors, text, \".\")\n",
    "            author = processNames(author)\n",
    "            return changedText, author\n",
    "    return text, \"\"\n",
    "\n",
    "def getEditors(text):\n",
    "    \n",
    "    '''\n",
    "    Schneidet die Editoren aus dem original Literaturstring aus. Es können mehrere Editoren vorliegen und diese\n",
    "    können mit einem \"and\" oder \"&\" verknüpft sein. Die Funktion soll alle Editoren einschließlich dem \"and\" und \"&\"\n",
    "    in einem Block extrahieren. Dieser Block ist sinnbildlich eine Kette von Namen.\n",
    "    Wenn eine solche Kette von Namen gefunden wird, wird setChainStart = True gesetzt und geprüft, \n",
    "    ob es sich auch um Editoren handelt.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Editoren und die ausgeschnittenen Editoren.\n",
    "    '''\n",
    "    \n",
    "    search_terms = [\" and \", \", and \", \" & \", \", & \", \"., & \", \"., and \", \". and \", \". & \"]\n",
    "    editorRegEx = \"\\s*(\\()?(Eds\\.|Eds|Ed|ed|Ed\\.|ed\\.|eds\\.|editor|editors)(\\))?\\s*\"\n",
    "    onlyPunctuation = False\n",
    "    onlyAnd = False\n",
    "    setChainStart = True\n",
    "    isEditor = False\n",
    "    startIndexEditors = -1\n",
    "    endIndexEditors = -1\n",
    "    chainStartIndex = -1\n",
    "    startIndexEditorMarker = -1\n",
    "    endIndexEditorMarker = -1\n",
    "    \n",
    "    df_PER = getPersonTags(text)\n",
    "    index_df_PER_List = df_PER.index.values.tolist()\n",
    "    if not df_PER.empty:\n",
    "        for index in index_df_PER_List:\n",
    "            if index < len(index_df_PER_List) - 1:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:df_PER[\"start\"].iloc[index + 1]]\n",
    "            else:\n",
    "                textBetweenNames = text[df_PER[\"end\"].iloc[index]:]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenNames, [\"&\"])\n",
    "            firstStartIndex, firstEndIndex, andTyp = getIndexOfSubstring(textBetweenNames, search_terms)\n",
    "            onlyAnd = textBetweenNames == andTyp\n",
    "            if setChainStart: \n",
    "                chainStartIndex = df_PER[\"start\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            if not onlyPunctuation and not onlyAnd:\n",
    "                '''\n",
    "                Zur Erklärung des Codes siehe analoge Implementierung in getAuthors\n",
    "                '''\n",
    "                setChainStart = True\n",
    "                textFromStartUntilFirstName = text[0:df_PER[\"start\"].iloc[0]]\n",
    "                isEditor, startIndexEditorMarker, endIndexEditorMarker = is_Editor(editorRegEx, textFromStartUntilFirstName, 0, False)\n",
    "                if startIndexEditorMarker == -1:\n",
    "                    isEditor, startIndexEditorMarker, endIndexEditorMarker = is_Editor(editorRegEx, textBetweenNames, df_PER[\"end\"].iloc[index])                         \n",
    "                if isEditor:\n",
    "                    startIndexEditors = chainStartIndex\n",
    "                    endIndexEditors = df_PER[\"end\"].iloc[index]\n",
    "                    break\n",
    "    if startIndexEditors > -1:\n",
    "        changedText, editor = replaceSubstring(startIndexEditors, endIndexEditors, text, \".\")\n",
    "        editor = processNames(editor)\n",
    "        if startIndexEditorMarker > -1:\n",
    "            if startIndexEditorMarker > startIndexEditors:\n",
    "                startIndexEditorMarker = startIndexEditorMarker - (len(text) - len(changedText))\n",
    "                endIndexEditorMarker = endIndexEditorMarker - (len(text) - len(changedText))\n",
    "            changedText, buffer = replaceSubstring(startIndexEditorMarker, endIndexEditorMarker, changedText, \".\")\n",
    "        startIndexIn = 0 \n",
    "        if startIndexEditorMarker < startIndexEditors:\n",
    "            startIndexEditors = startIndexEditors - (endIndexEditorMarker - startIndexEditorMarker)\n",
    "            endIndexEditors = endIndexEditors - (endIndexEditorMarker - startIndexEditorMarker)\n",
    "        for i in range(startIndexEditors-1, -1, -1):\n",
    "            if isSpeceficPunctuation(changedText[i], [\":\", \" \"]):\n",
    "                startIndexIn = i + 1\n",
    "                break\n",
    "        changedText, replacedEditorMarker = replaceSubstring(startIndexIn, startIndexEditors, changedText, \".\")\n",
    "        return changedText, editor\n",
    "    return text, \"\"\n",
    "    \n",
    "def getPersonTags(text):\n",
    "    \n",
    "    '''\n",
    "    Gibt die durch Named Entity Recognition erkannten dictionaries vom Typ Person in einem Dataframe zurück.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Person.\n",
    "    '''\n",
    "    \n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty: \n",
    "        return df_outputs[df_outputs[\"entity_group\"] == \"PER\"].reset_index(drop=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getORGTag(text, score):\n",
    "    \n",
    "    '''\n",
    "    Gibt die durch Named Entity Recognition erkannten dictionaries vom Typ Organization in einem Dataframe zurück.\n",
    "    Um in das Dataframe aufgenommen zu werden, muss ein bestimmter Score erreicht sein.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    score: Schwellenwert zwischen 0 und 1.\n",
    "    \n",
    "    return: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Organization.\n",
    "    '''\n",
    "    \n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"ORG\") & (df_outputs[\"score\"] >= score)].reset_index(drop=True).tail(1)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getLOCTag(text):\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    Gibt die durch Named Entity Recognition erkannten dictionaries vom Typ Location in einem Dataframe zurück.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Dataframe der durch Named Entity Recognition erkannten dictionaries vom Typ Location.\n",
    "    '''\n",
    "    \n",
    "    ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "    outputs = ner_tagger(text)\n",
    "    df_outputs = pd.DataFrame(outputs)\n",
    "    if not df_outputs.empty:\n",
    "        return df_outputs[(df_outputs[\"entity_group\"] == \"LOC\")].reset_index(drop=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def getDoi(text):\n",
    "    \n",
    "    '''\n",
    "    Schneidet die DOI aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne DOI und die ausgeschnittene DOI.\n",
    "    '''\n",
    "    \n",
    "    doiUrlRegEx1 = \"https:\\/\\/doi\\.org(\\/[^\\s]*)?$\"\n",
    "    doiUrlRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org)?([^\\s]*)+$\"\n",
    "    changedText, doi  = getSubstringByRegEx(text, [doiUrlRegEx1, doiUrlRegEx2])\n",
    "    httpsDomainRegEx1 = \"https:\\/\\/doi\\.org\\/\"\n",
    "    httpsDomainRegEx2 = \"(DOI|doi):\\s?(https:\\/\\/doi\\.org\\/)?\"\n",
    "    doi, httpsDomain = getSubstringByRegEx(doi, [httpsDomainRegEx1, httpsDomainRegEx2])\n",
    "    return changedText, custom_strip(doi)\n",
    "\n",
    "def getURL(text):\n",
    "        \n",
    "    '''\n",
    "    Schneidet die URL aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne URL und die ausgeschnittene URL.\n",
    "    '''\n",
    "    \n",
    "    urlRegEx = \"(URL:|url:)?\\s*https?://[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?::\\d+)?(?:/[^\\s]*)?\"\n",
    "    changedText, url = getSubstringByRegEx(text, [urlRegEx])\n",
    "    urlPrefixRegEx = r\"(url:\\s*|URL:\\s*)\"\n",
    "    url = re.sub(urlPrefixRegEx, '', url).strip()\n",
    "    return changedText, custom_strip(url)\n",
    "\n",
    "def getDate(text):\n",
    "            \n",
    "    '''\n",
    "    Schneidet Date aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Date und ausgeschnittene Date.\n",
    "    '''\n",
    "    \n",
    "    monthYearRegex = \"(January|Jan\\.?|February|Feb\\.?|March|Mar\\.?|April|Apr\\.?\" \\\n",
    "    \"|May|May\\.?|June|Jun\\.?|July|Jul\\.?|August|Aug\\.?|September|Sep\\.?|Sept\\.?|October|\" \\\n",
    "    \"Oct\\.?|November|Nov\\.?|December|Dec\\.?)\\s\\d{4}\"\n",
    "    changedText, monthYear  = getSubstringByRegEx(text, [monthYearRegex])\n",
    "    if monthYear == \"\":\n",
    "        yearRegEx1 = \"(\\.|,)? \\(\\d{4}\\)(\\.|,|:)\"\n",
    "        yearRegEx2 = \"(\\.|,) \\d{4}(\\.|,|;)\"\n",
    "        changedText, year  = getSubstringByRegEx(text, [yearRegEx1, yearRegEx2])\n",
    "        return changedText, \"\", f'{year}'\n",
    "    monthYear = monthYear.split(' ')\n",
    "    return changedText, f'{monthYear[0]}', f'{monthYear[1]}'\n",
    "\n",
    "def getPage(text):\n",
    "                \n",
    "    '''\n",
    "    Schneidet Page aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Page und ausgeschnittene Page.\n",
    "    '''\n",
    "    \n",
    "    pageRegEx = \"(?:pp\\.? )?\\d+(-|--|–)\\d+\"\n",
    "    changedText, pages = getSubstringByRegEx(text, [pageRegEx])\n",
    "    if pages != \"\":\n",
    "        pages = re.search(r'\\d+(-|--|–)\\d+', pages).group()\n",
    "    return changedText, custom_strip(pages)\n",
    "\n",
    "def getVolumeNumber(text):\n",
    "                    \n",
    "    '''\n",
    "    Schneidet Volume und Number aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Volume und Number und ausgeschnittene Volume und Number.\n",
    "    '''\n",
    "    \n",
    "    volumeAndNumberRegex = \"(\\d+\\(\\d+\\)|\\d+\\.\\d+)\"\n",
    "    startIndex, endIndex, substring = getIndexOfSubstring(text, [volumeAndNumberRegex], True)\n",
    "    if startIndex > -1:\n",
    "        changedText, volumeNumber = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        startIndex, endIndex, volume = getIndexOfSubstring(volumeNumber, [\"\\d+\"])\n",
    "        startIndex, endIndex, number = getIndexOfSubstring(volumeNumber, [\"\\d+\"], True)\n",
    "        return changedText, volume, number\n",
    "    else:\n",
    "        volumeRegEx = \"(V|v)ol\\. \\d+\"\n",
    "        volumeRegEx2 = \", \\d+,\"\n",
    "        volumeRegEx3 = \",? \\d+(:|\\.)\"\n",
    "        number1RegEx = \"no\\. \\d+\"\n",
    "        number2RegEx = \"Issue \\d+\"\n",
    "        number3RegEx = \", \\d+,\"\n",
    "        number4RegEx = \"\\.\\d+\"\n",
    "        startIndex, endIndex, volume = getIndexOfSubstring(text, [volumeRegEx, volumeRegEx2, volumeRegEx3], True)\n",
    "        changedText, substring = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        startIndex, endIndex, number = getIndexOfSubstring(changedText, [number1RegEx, number2RegEx, number3RegEx, number4RegEx], True)\n",
    "        changedText, substring = replaceSubstring(startIndex, endIndex, changedText, \"\")\n",
    "        if volume != \"\":\n",
    "            volume = re.search(r'\\d+', volume).group(0)\n",
    "        if number != \"\":\n",
    "            number = re.search(r'\\d+', number).group(0)\n",
    "        return changedText, volume, number\n",
    "\n",
    "def getEdition(text):\n",
    "                        \n",
    "    '''\n",
    "    Schneidet Edition aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Edition und ausgeschnittene Edition.\n",
    "    '''\n",
    "    \n",
    "    editionRegEx1 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) ed\\.\"\n",
    "    editionRegEx2 = \"(?:[1-9]\\d*th|11th|12th|13th|[1-9]\\d*(?:st|nd|rd)) edn\\.\"\n",
    "    changedText, edition = getSubstringByRegEx(text, [editionRegEx1, editionRegEx2])\n",
    "    if edition != \"\":\n",
    "        edition = re.search(r'\\d+', edition).group()\n",
    "    return changedText, custom_strip(edition)\n",
    "\n",
    "def getAddress(text):\n",
    "                            \n",
    "    '''\n",
    "    Schneidet Address aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Address und ausgeschnittene Address.\n",
    "    '''\n",
    "    \n",
    "    df_LOC = getLOCTag(text)\n",
    "    addressFound = False\n",
    "    index_df_Loc_List = df_LOC.index.values.tolist()\n",
    "    textBetweenAddress = \"\"\n",
    "    setChainStart = True\n",
    "    startIndex = 0\n",
    "    endIndex = 0\n",
    "    if not df_LOC.empty:\n",
    "        for index in reversed(index_df_Loc_List):\n",
    "            if index < len(index_df_Loc_List) and index > 0:\n",
    "                textBetweenAddress = text[df_LOC[\"end\"].iloc[index-1]:df_LOC[\"start\"].iloc[index]]\n",
    "            else:\n",
    "                textBetweenAddress = text[:df_LOC[\"start\"].iloc[index]]\n",
    "            onlyPunctuation = isSpeceficPunctuation(textBetweenAddress, [])\n",
    "            if setChainStart: \n",
    "                chainEnIndex = df_LOC[\"end\"].iloc[index]\n",
    "                setChainStart = False\n",
    "            if not onlyPunctuation:\n",
    "                startIndex = df_LOC[\"start\"].iloc[index]\n",
    "                endIndex = chainEnIndex\n",
    "                break\n",
    "        address = text[startIndex:endIndex]\n",
    "        if startIndex > 2 and endIndex < len(text) - 1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                addressFound = True\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                addressFound = True\n",
    "        if addressFound:\n",
    "            changedText, address = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "            return changedText, custom_strip(address)\n",
    "    return text, \"\"  \n",
    "\n",
    "def getPublisher(text, doi):\n",
    "                                \n",
    "    '''\n",
    "    Schneidet Publisher aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    doi: DOI, um Publisher in externen Datenbanken zu suchen.\n",
    "    \n",
    "    return: Literaturstring ohne Publisher und ausgeschnittenen Publisher.\n",
    "    '''\n",
    "    \n",
    "    publisher = \"\"\n",
    "    if doi != \"\":\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            publisher = data['message'].get('publisher', 'Publisher not found')\n",
    "    if publisher != \"\":\n",
    "        startIndex, endIndex, publisher = getIndexOfSubstring(text, [publisher], True)\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, publisher\n",
    "        changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "        if publisher != \"\":\n",
    "            return changedText, custom_strip(publisher)\n",
    "    df_ORG = getORGTag(text, 0.8)\n",
    "    if not df_ORG.empty:\n",
    "        startIndex = df_ORG[\"start\"].iloc[0]\n",
    "        endIndex = df_ORG[\"end\"].iloc[0]\n",
    "        publisher = text[startIndex:endIndex]\n",
    "        if endIndex < len(text) -1:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]) and isSpeceficPunctuation(text[endIndex + 1]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "        else:\n",
    "            if isSpeceficPunctuation(text[startIndex - 2]):\n",
    "                changedText, publisher = replaceSubstring(startIndex, endIndex, text, \"\")\n",
    "                return changedText, custom_strip(publisher)\n",
    "    return text, \"\"\n",
    "    \n",
    "def getTitel(text):\n",
    "                                    \n",
    "    '''\n",
    "    Schneidet Titel aus dem Literaturstring aus.\n",
    "    \n",
    "    Parameter:\n",
    "    text: Literaturstring.\n",
    "    \n",
    "    return: Literaturstring ohne Titel und ausgeschnittenen Titel.\n",
    "    '''\n",
    "    \n",
    "    ignoreCharacters = [\"?\", \"!\", \"(\", \")\", \"“\", \"”\", \"\\\"\"]\n",
    "    text = custom_strip(text, ignoreCharacters)\n",
    "    ignoreCharacters = [\"?\", \":\", \"-\", \"(\", \")\", \"“\", \"”\", \"\\\"\"]\n",
    "    limit = len(text) - 1\n",
    "    i = 0\n",
    "    maxIndex = 0\n",
    "    while i < limit:\n",
    "        if (i + 2 < limit) and not (text[i] == \",\" and text[i+1] == \" \" and not isSpeceficPunctuation(text[i+2])):\n",
    "            if isSpeceficPunctuation(text[i], ignoreCharacters) and isSpeceficPunctuation(text[i+1], ignoreCharacters):\n",
    "                text = text[:i] + \".\" + text[i+2:]\n",
    "                i = i - 1\n",
    "                limit = limit - 1\n",
    "        i = i +1\n",
    "    \n",
    "    ignoreCharacters = [\"?\", \"!\", \"(\", \")\"]\n",
    "    if text[0] == \"“\":\n",
    "        text = text.rsplit('”', 1)\n",
    "        return custom_strip(text[0], ignoreCharacters), custom_strip(text[1], ignoreCharacters), \"\"\n",
    "    elif text[0] == \"\\\"\":\n",
    "        text = text.rsplit(\"\\\"\", 1)\n",
    "        return custom_strip(text[0], ignoreCharacters), custom_strip(text[1], ignoreCharacters), \"\"\n",
    "    elif text.count(\".\") == 1:\n",
    "        text = text.split(\".\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    elif text.count(\".\") == 2:\n",
    "        text = text.split(\".\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), custom_strip(text[2])\n",
    "    else:\n",
    "        for index, element in enumerate(text):\n",
    "            if isSpeceficPunctuation(element, [\".\", \",\", \" \", \"(\", \")\", \":\"]):\n",
    "                if maxIndex < index:\n",
    "                    maxIndex = index\n",
    "        if maxIndex > 0:\n",
    "            text = text.split(text[maxIndex])\n",
    "            return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    if maxIndex == 0 and text.count(\",\") == 1: \n",
    "        text = text.split(\",\")\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    elif maxIndex == 0 and text.count(\",\") > 1:\n",
    "        text = text.rsplit(',', 1)\n",
    "        return custom_strip(text[0]), custom_strip(text[1]), \"\"\n",
    "    return custom_strip(text), \"\", \"\"\n",
    "    \n",
    "    \n",
    "def getKey(author, year):\n",
    "                                        \n",
    "    '''\n",
    "    Erzeugt einen Key aus dem Nachnamen des ersten Autors und dem Jahr.\n",
    "    \n",
    "    Parameter:\n",
    "    author: Autoren.\n",
    "    year: Jahr.\n",
    "    \n",
    "    return: Erzeugter Schlüssel.\n",
    "    '''\n",
    "    \n",
    "    lastNameFirstAuthor = author.split(\" and \")[0].strip().split(\" \")[-1]\n",
    "    return f'{lastNameFirstAuthor}_{year}'\n",
    "\n",
    "\n",
    "def create_bibtex(text):\n",
    "    address = \"\"\n",
    "    author = \"\"\n",
    "    booktitle = \"\"\n",
    "    chapter = \"\"\n",
    "    doi = \"\"\n",
    "    edition = \"\"\n",
    "    editor = \"\"\n",
    "    howpublished = \"\"\n",
    "    isbn = \"\"\n",
    "    journal = \"\"\n",
    "    key = \"\"\n",
    "    month = \"\"\n",
    "    note = \"\"\n",
    "    number = \"\"\n",
    "    organization = \"\"\n",
    "    pages = \"\"\n",
    "    publisher = \"\"\n",
    "    school = \"\"\n",
    "    series = \"\"\n",
    "    title = \"\"\n",
    "    url = \"\"\n",
    "    volume = \"\"\n",
    "    year = \"\"\n",
    "    key = \"\"\n",
    "    isBook = False\n",
    "    isProceedings = False\n",
    "    isInProceedings = False\n",
    "    isIncollection = False\n",
    "    isArticle = False\n",
    "    \n",
    "    text, author = getAuthors(text)\n",
    "    text, editor = getEditors(text)\n",
    "    text, doi = getDoi(text)\n",
    "    text, url = getURL(text)\n",
    "    text, month, year = getDate(text)\n",
    "    text, pages = getPage(text)\n",
    "    text, volume, number = getVolumeNumber(text)\n",
    "    text, edition = getEdition(text)\n",
    "    text, address = getAddress(text)\n",
    "    text, publisher = getPublisher(text, doi)\n",
    "    school = publisher\n",
    "    title, booktitle, series = getTitel(text)\n",
    "    journal = booktitle\n",
    "    if author != \"\":\n",
    "        key = getKey(author, year)\n",
    "    else:\n",
    "        key = getKey(editor, year)\n",
    "    \n",
    "    bookFields = [author, title, publisher, year, volume, number, \\\n",
    "                  series, address, edition, month, note, key, editor, \\\n",
    "                  howpublished, organization, chapter, pages, isbn, url]\n",
    "    inproceedingsFields = [author, title, booktitle, year, editor, volume, \\\n",
    "                            number, series, pages, address, month, organization, \\\n",
    "                            publisher, note, key, doi, url]\n",
    "    proceedingsFields = [title, year, editor, volume, number, series, \\\n",
    "                          address, month, organization, publisher, note, key, doi, url]\n",
    "    incollectionFields = [author, title, booktitle, publisher, year, editor, \\\n",
    "                           volume, number, series, chapter, pages, address, \\\n",
    "                           edition, month, note, key, doi, url]\n",
    "    articleFields = [author, title, journal, year, volume, number, \\\n",
    "                      pages, month, note, key, doi, url]\n",
    "    phdthesisFields = [author, title, publisher, year, address, month, \\\n",
    "                        note, key, doi, url]\n",
    "    \n",
    "    bookFieldsString = [\"author\", \"title\", \"publisher\", \"year\", \"volume\", \"number\", \\\n",
    "                  \"series\", \"address\", \"edition\", \"month\", \"note\", \"key\", \"editor\", \\\n",
    "                  \"howpublished\", \"organization\", \"chapter\", \"pages\", \"isbn\", \"url\"]\n",
    "    inproceedingsFieldsString  = [\"author\", \"title\", \"booktitle\", \"year\", \"editor\", \"volume\", \\\n",
    "                            \"number\", \"series\", \"pages\", \"address\", \"month\", \"organization\", \\\n",
    "                            \"publisher\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    proceedingsFieldsString  = [\"title\", \"year\", \"editor\", \"volume\", \"number\", \"series\", \\\n",
    "                          \"address\", \"month\", \"organization\", \"publisher\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    incollectionFieldsString  = [\"author\", \"title\", \"booktitle\", \"publisher\", \"year\", \"editor\", \\\n",
    "                           \"volume\", \"number\", \"series\", \"chapter\", \"pages\", \"address\", \\\n",
    "                           \"edition\", \"month\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    articleFieldsString  = [\"author\", \"title\", \"journal\", \"year\", \"volume\", \"number\", \\\n",
    "                      \"pages\", \"month\", \"note\", \"key\", \"doi\", \"url\"]\n",
    "    phdthesisFieldsString  = [\"author\", \"title\", \"school\", \"year\", \"address\", \"month\", \\\n",
    "                        \"note\", \"key\", \"doi\", \"url\"]\n",
    "\n",
    "    model = \"LaLaf93/LiteratureTyp_recognizer\"\n",
    "    classifier = pipeline(\"text-classification\", model=model)\n",
    "    literatureType = classifier(title + \".\" + booktitle)[0]['label']\n",
    "    \n",
    "    bibTex = \"@\"\n",
    "    if literatureType == \"book\":\n",
    "        zippedFieldsValues = zip(bookFieldsString, bookFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"book{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"proceedings\":\n",
    "        zippedFieldsValues = zip(proceedingsFieldsString, proceedingsFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"proceedings{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"inproceedings\":\n",
    "        zippedFieldsValues = zip(inproceedingsFieldsString, inproceedingsFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"inproceedings{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"incollection\":\n",
    "        zippedFieldsValues = zip(incollectionFieldsString, incollectionFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"incollection{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    elif literatureType == \"article\":\n",
    "        zippedFieldsValues = zip(articleFieldsString, articleFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"article{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    else:\n",
    "        zippedFieldsValues = zip(phdthesisFieldsString, phdthesisFields)\n",
    "        zippedList = list(zippedFieldsValues)\n",
    "        bibTex += f\"phdthesis{{{key}, \\n\"\n",
    "        for field in zippedList:\n",
    "            bibTex += f'{field[0]}={{{field[1]}}},\\n' \n",
    "    \n",
    "    bibTex += '}'\n",
    "\n",
    "    return bibTex \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1358ef",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\David\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@incollection{Oudah_, \n",
      "author={Nabeel Oudah and Maher Faik Esmaile and Estabraq Abdulredaa},\n",
      "title={Optical character recognition using active contour segmentation},\n",
      "booktitle={2018},\n",
      "publisher={Journal of Engineering},\n",
      "year={},\n",
      "editor={},\n",
      "volume={24},\n",
      "number={1},\n",
      "series={},\n",
      "chapter={},\n",
      "pages={146-158},\n",
      "address={},\n",
      "edition={},\n",
      "month={},\n",
      "note={},\n",
      "key={Oudah_},\n",
      "doi={},\n",
      "url={},\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Beispiele\n",
    "\n",
    "text=\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence. In J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). Physical Review. https://doi.org/10.1109/ICNN.2021.9483948\"\n",
    "#text = \"M. A. Nielsen and I. L. Chuang, “Quantum Computation and Quantum Information,” in Handbook of Quantum Information Science, vol. 4, C. H. Bennett and D. P. DiVincenzo, Eds. Berlin, Germany: Springer, 2026, pp. 250–300. doi: 10.1007/springerreference-303198.\"\n",
    "#text = \"Nielsen, M. A.; Chuang, I. L. Quantum Computation and Quantum Information. In Handbook of Quantum Information Science; Bennett, C. H., DiVincenzo, D. P., Eds.; Quantum Science and Technology; Springer: Berlin, Germany, 2026; Vol. 4, pp 250–300. https://doi.org/10.1007/springerreference-303198.\"\n",
    "\n",
    "#BUG: startIndexReplace={-1} ist hier bei getYear! Deswegen doppelter String drin\n",
    "text = \"\"\"Alahmed, Y., Abadla, R., Badri, A. A., & Ameen, N. (2023). “How Does ChatGPT Work” Examining Functionality, To The Creative AI CHATGPT on X’s (Twitter) Platform. 2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS), 1–7. https://doi.org/10.1109/SNAMS60348.2023.10375450\"\"\"\n",
    "text = \"David Mertz, Regular Expression Puzzles and AI Coding Assistants: 24 puzzles solved by the author, with and without assistance from Copilot, ChatGPT and more , Manning, 2023.\"\n",
    "text = \"\"\"Mohammed Baziyad, Ibrahim Kamel, and Tamer Rabie. 2023. On the Linguistic Limitations of ChatGPT: An Experimental Case Study. In 2023 International Symposium on Networks, Computers and Communications (ISNCC), 1–6. DOI:https://doi.org/10.1109/ISNCC58260.2023.10323661\"\"\"\n",
    "#text = \"\"\"K. M. Caramancion, \"Harnessing the Power of ChatGPT to Decimate Mis/Disinformation: Using ChatGPT for Fake News Detection,\" 2023 IEEE World AI IoT Congress (AIIoT), Seattle, WA, USA, 2023, pp. 0042-0046, doi: 10.1109/AIIoT58121.2023.10174450.\"\"\"\n",
    "#text = \"\"\"Hinton, G., Bengio, Y., & LeCun, Y. (2021). Deep Learning for Artificial Intelligence? In: J. Smith & J. Doe (Eds.), Proceedings of the IEEE International Conference on Neural Networks (Vol. 1, Issue 5, pp. 100–120). IEEE Press. https://doi.org/10.1109/ICNN.2021.9483948\"\"\"\n",
    "#text = \"\"\"A. Einstein, B. Podolsky, and N. Rosen, “Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?,” Physical Review, vol. 47, no. 10, pp. 777–780, May 1935, doi: 10.1103/PhysRev.47.777.\"\"\"\n",
    "#text = \"\"\"Badaro, G., Saeed, M., & Papotti, P. (2023). Transformers for tabular data representation: a survey of models and applications. Transactions of the Association for Computational Linguistics, 11, pp. 227–249. URL: https://aclanthology.org/2023.tacl-1.14, doi:10.1162/tacl_a_00544\"\"\"\n",
    "#text = \"\"\"Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics 11, (2023), 1–17. URL: https://aclanthology.org/2023.tacl-1.1, doi:10.1162/tacl_a_00530\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. (2017). Joint prediction of word alignment with alignment types. Transactions of the Association for Computational Linguistics, 5, pp. 501–514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. 2017. \"Joint Prediction of Word Alignment with Alignment Types.\" Transactions of the Association for Computational Linguistics, 5: 501–514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076.\"\"\"\n",
    "#text = \"\"\"Mansouri Bigvand, A., Bu, T., & Sarkar, A. 2017. ‘Joint Prediction of Word Alignment with Alignment Types.\" Transactions of the Association forComputational Linguistics, 5: 501-514. URL: https://aclanthology.org/Q17-1035, doi:10.1162/tacl_a_00076.\"\"\"\n",
    "#text = \"\"\"Devika K, Hariprasath .s.b, Haripriya B, Vigneshwar E, Premjith B, and Bharathi Raja Chakravarthi. From dataset to detection: a comprehensive approach to combating Malayalam fake news. In Bharathi Raja Chakravarthi, Ruba Priyadharshini, Anand Kumar Madasamy, Sajeetha Thavareesan, Elizabeth Sherly, Rajeswari Nadarajan, and Manikandan Ravikiran, editors, Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 16–23, St. Julian's, Malta, March 2024. Association for Computational Linguistics. URL: https://aclanthology.org/2024.dravidianlangtech-1.3.\"\"\"\n",
    "#text = \"\"\"R, Jairam, G, Jyothish, and B, Premjith. \"A few-shot multi-accented speech classification for Indian languages using transformers and LLM's fine-tuning approaches.\" Proceedings of the Fourth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages. Eds. Chakravarthi, Bharathi Raja, Priyadharshini, Ruba, Madasamy, Anand Kumar, Thavareesan, Sajeetha, Sherly, Elizabeth, Nadarajan, Rajeswari, and Ravikiran, Manikandan. St. Julian's, Malta: Association for Computational Linguistics, 2024. 1–9. URL: https://aclanthology.org/2024.dravidianlangtech-1.1\"\"\"\n",
    "text = \"\"\"Nabeel Oudah, Maher Faik Esmaile, and Estabraq Abdulredaa. Optical character recognition using active contour segmentation. Journal of Engineering, 24(1):146-158, 2018\"\"\"\n",
    "#text = \"\"\"Eds. Alonso, Jose M., and Catala, Alejandro. Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019). 2019. URL: https://aclanthology.org/W19-8400\"\"\"\n",
    "print(create_bibtex(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8b8a9e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Referenzstring</th>\n",
       "      <th>Style</th>\n",
       "      <th>Literaturtyp</th>\n",
       "      <th>BibTeX</th>\n",
       "      <th>Modellergebnis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yuan Zhang, Regina Barzilay, and Tommi Jaakkol...</td>\n",
       "      <td>acm</td>\n",
       "      <td>article</td>\n",
       "      <td>@article{zhang-etal-2017-aspect,\\n    title = ...</td>\n",
       "      <td>@article{Zhang_2017, \\nauthor={Yuan Zhang and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ryan J. Gallagher, Kyle Reing, David Kale, and...</td>\n",
       "      <td>acm</td>\n",
       "      <td>article</td>\n",
       "      <td>@article{gallagher-etal-2017-anchored,\\n    ti...</td>\n",
       "      <td>@article{Gallagher_2017, \\nauthor={Ryan J. Gal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shafiq Joty, Francisco Guzmán, Lluís Màrquez, ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>article</td>\n",
       "      <td>@article{joty-etal-2017-discourse,\\n    title ...</td>\n",
       "      <td>@article{Joty_2017, \\nauthor={Shafiq Joty and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alla Rozovskaya, Dan Roth, and Mark Sammons. 2...</td>\n",
       "      <td>acm</td>\n",
       "      <td>article</td>\n",
       "      <td>@article{rozovskaya-etal-2017-adapting,\\n    t...</td>\n",
       "      <td>@article{Rozovskaya_2017, \\nauthor={Alla Rozov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ákos Kádár, Grzegorz Chrupała, and Afra Alisha...</td>\n",
       "      <td>acm</td>\n",
       "      <td>article</td>\n",
       "      <td>@article{kadar-etal-2017-representation,\\n    ...</td>\n",
       "      <td>@article{Kádár_2017, \\nauthor={Ákos Kádár and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Magda Ševčíková, Zdeněk Žabokrtský, Eleonora L...</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-international-resources,\\...</td>\n",
       "      <td>@phdthesis{Ševčíková_2019, \\nauthor={Magda Šev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Jose M. Alonso and Alejandro Catala (Eds.). 20...</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-interactive,\\n    title =...</td>\n",
       "      <td>@phdthesis{Alonso_2019, \\nauthor={Jose M. Alon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Yoshinobu Kano, Claus Aranha, Michimasa Inaba,...</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-international-ai,\\n    ti...</td>\n",
       "      <td>@phdthesis{Kano_2019, \\nauthor={Yoshinobu Kano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Anusha Balakrishnan, Vera Demberg, Chandra Kha...</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-discourse-structure,\\n   ...</td>\n",
       "      <td>@phdthesis{Balakrishnan_2019, \\nauthor={Anusha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Alexandre Rademaker and Francis Tyers (Eds.). ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-universal,\\n    title = \"...</td>\n",
       "      <td>@phdthesis{Rademaker_2019, \\nauthor={Alexandre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Xinying Chen and Ramon Ferrer-i-Cancho (Eds.)....</td>\n",
       "      <td>acm</td>\n",
       "      <td>proceedings</td>\n",
       "      <td>@proceedings{ws-2019-quantitative,\\n    title ...</td>\n",
       "      <td>@phdthesis{Chen_2019, \\nauthor={Xinying Chen a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>Talia Tseriotou, Ryan Chan, Adam Tsakalidis, I...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{tseriotou-etal-2024-sig,\\n    t...</td>\n",
       "      <td>@inproceedings{Tseriotou_2024, \\nauthor={Talia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>Jairam R, Jyothish G, and Premjith B. 2024. A ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{r-etal-2024-shot,\\n    title = ...</td>\n",
       "      <td>@inproceedings{R_2024, \\nauthor={Jairam R and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Christeena Varghese, Sergey Koshelev, and Ivan...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{varghese-etal-2024-neural,\\n   ...</td>\n",
       "      <td>@phdthesis{Varghese_2024, \\nauthor={Christeena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>Devika K, Hariprasath .s.b, Haripriya B, Vigne...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{k-etal-2024-dataset,\\n    title...</td>\n",
       "      <td>@phdthesis{K_2024, \\nauthor={Hariprasath Devik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>Girma Bade, Olga Kolesnikova, Grigori Sidorov,...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{bade-etal-2024-social,\\n    tit...</td>\n",
       "      <td>@phdthesis{Bade_2024, \\nauthor={Girma Bade and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>Vigneshwar Lakshminarayanan and Emily Prud'hom...</td>\n",
       "      <td>acm</td>\n",
       "      <td>inproceedings</td>\n",
       "      <td>@inproceedings{lakshminarayanan-prudhommeaux-2...</td>\n",
       "      <td>@phdthesis{Lakshminarayanan_2024, \\nauthor={Vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Bill Worzel and Rick L. Riolo. 2003. Genetic P...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{worzel:2002:GPTP,\\n  author =   ...</td>\n",
       "      <td>@incollection{Worzel_2003, \\nauthor={Bill Worz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>Bill Worzel and Duncan MacLean. 2005. Content ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{worzel:2005:GPTP,\\n  author =   ...</td>\n",
       "      <td>@incollection{Worzel_2005, \\nauthor={Bill Worz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>W. P. Worzel, A. Almal, and C. D. MacLean. 200...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{Worzel:2006:GPTP,\\n  author =   ...</td>\n",
       "      <td>@incollection{Worzel_2006, \\nauthor={W. P. Wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>Gary I. Wu. 1999. Evolution of Infanticide and...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{wu:1999:EIYNAO,\\n  author =     ...</td>\n",
       "      <td>@incollection{Wu_1999, \\nauthor={Gary I. Wu},\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Taro Yabuki and Hitoshi Iba. 2004. Genetic pro...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{yabuki04genetic,\\n  author =    ...</td>\n",
       "      <td>@incollection{Yabuki_2004, \\nauthor={Taro Yabu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Mei-Wan Yang. 1999. Space Configuration using ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>incollection</td>\n",
       "      <td>@InCollection{yang:1999:SCGA,\\n  author =     ...</td>\n",
       "      <td>@incollection{Yang_1999, \\nauthor={Mei-Wan Yan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>Kareen Bock, Anemone Widmer, Therese Neff, and...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Bock.2005,\\n author = {Bock, Kareen and ...</td>\n",
       "      <td>@book{Bock_2005, \\nauthor={Kareen Bock and Ane...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>Thilde Boecker, Aileen Leib, Marika Mihm, and ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Boecker.1990,\\n author = {Boecker, Thild...</td>\n",
       "      <td>@book{Boecker_1990, \\nauthor={Thilde Boecker a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>Vincenz Böhmer, Nadine Kempkes, and Siegulf Ha...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Bohmer.2007,\\n author = {B{\\\"o}hmer, Vin...</td>\n",
       "      <td>@book{Böhmer_2007, \\nauthor={Vincenz Böhmer an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>Trautchen Bohner, Nikolaus-Jonny Sehr, and Arm...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Bohner.2009,\\n author = {Bohner, Trautch...</td>\n",
       "      <td>@book{Bohner_2009, \\nauthor={Trautchen Bohner ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>Ingetrud Bohnet, Heidegunde Specht, Steve März...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Bohnet.2012,\\n author = {Bohnet, Ingetru...</td>\n",
       "      <td>@book{Bohnet_2012, \\nauthor={Ingetrud Bohnet a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>Bärbel Bolduan, Heinzhermann Reinwald, Gretlie...</td>\n",
       "      <td>acm</td>\n",
       "      <td>book</td>\n",
       "      <td>@book{Bolduan.2021,\\n author = {Bolduan, B{\\\"a...</td>\n",
       "      <td>@book{Bolduan_2021, \\nauthor={Bärbel Bolduan a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Maëlyss Berns. 2019. Vermeidung von Interferen...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Berns.2019,\\n author = {Berns, Ma{\\...</td>\n",
       "      <td>@phdthesis{Berns_2019, \\nauthor={Maëlyss Berns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Mélys Berthot. 2017. Efficient storage and ana...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Berthot.2017,\\n author = {Berthot, ...</td>\n",
       "      <td>@incollection{Berthot_2017, \\nauthor={Mélys Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>Cléa Bertot. 2012. Harvesting and summarizing ...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Bertot.2012,\\n author = {Bertot, Cl...</td>\n",
       "      <td>@incollection{Bertot_2012, \\nauthor={Cléa Bert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>Anaëlle Bettam. 2015. Cardiac motion and funct...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Bettam.2015,\\n author = {Bettam, An...</td>\n",
       "      <td>@incollection{Bettam_2015, \\nauthor={Anaëlle B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>Aloïs Bette. 2019. Similarity Search Algorithm...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Bette.2019,\\n author = {Bette, Alo{...</td>\n",
       "      <td>@phdthesis{Bette_2019, \\nauthor={Aloïs Bette},...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>Mélissandre Bidder. 2018. Real-time event dete...</td>\n",
       "      <td>acm</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>@phdthesis{Bidder.2018,\\n author = {Bidder, M{...</td>\n",
       "      <td>@phdthesis{Bidder_2018, \\nauthor={Mélissandre ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Referenzstring Style   Literaturtyp  \\\n",
       "1     Yuan Zhang, Regina Barzilay, and Tommi Jaakkol...   acm        article   \n",
       "2     Ryan J. Gallagher, Kyle Reing, David Kale, and...   acm        article   \n",
       "3     Shafiq Joty, Francisco Guzmán, Lluís Màrquez, ...   acm        article   \n",
       "4     Alla Rozovskaya, Dan Roth, and Mark Sammons. 2...   acm        article   \n",
       "5     Ákos Kádár, Grzegorz Chrupała, and Afra Alisha...   acm        article   \n",
       "300   Magda Ševčíková, Zdeněk Žabokrtský, Eleonora L...   acm    proceedings   \n",
       "301   Jose M. Alonso and Alejandro Catala (Eds.). 20...   acm    proceedings   \n",
       "302   Yoshinobu Kano, Claus Aranha, Michimasa Inaba,...   acm    proceedings   \n",
       "303   Anusha Balakrishnan, Vera Demberg, Chandra Kha...   acm    proceedings   \n",
       "304   Alexandre Rademaker and Francis Tyers (Eds.). ...   acm    proceedings   \n",
       "305   Xinying Chen and Ramon Ferrer-i-Cancho (Eds.)....   acm    proceedings   \n",
       "600   Talia Tseriotou, Ryan Chan, Adam Tsakalidis, I...   acm  inproceedings   \n",
       "601   Jairam R, Jyothish G, and Premjith B. 2024. A ...   acm  inproceedings   \n",
       "602   Christeena Varghese, Sergey Koshelev, and Ivan...   acm  inproceedings   \n",
       "603   Devika K, Hariprasath .s.b, Haripriya B, Vigne...   acm  inproceedings   \n",
       "604   Girma Bade, Olga Kolesnikova, Grigori Sidorov,...   acm  inproceedings   \n",
       "605   Vigneshwar Lakshminarayanan and Emily Prud'hom...   acm  inproceedings   \n",
       "900   Bill Worzel and Rick L. Riolo. 2003. Genetic P...   acm   incollection   \n",
       "901   Bill Worzel and Duncan MacLean. 2005. Content ...   acm   incollection   \n",
       "902   W. P. Worzel, A. Almal, and C. D. MacLean. 200...   acm   incollection   \n",
       "903   Gary I. Wu. 1999. Evolution of Infanticide and...   acm   incollection   \n",
       "904   Taro Yabuki and Hitoshi Iba. 2004. Genetic pro...   acm   incollection   \n",
       "905   Mei-Wan Yang. 1999. Space Configuration using ...   acm   incollection   \n",
       "1200  Kareen Bock, Anemone Widmer, Therese Neff, and...   acm           book   \n",
       "1201  Thilde Boecker, Aileen Leib, Marika Mihm, and ...   acm           book   \n",
       "1202  Vincenz Böhmer, Nadine Kempkes, and Siegulf Ha...   acm           book   \n",
       "1203  Trautchen Bohner, Nikolaus-Jonny Sehr, and Arm...   acm           book   \n",
       "1204  Ingetrud Bohnet, Heidegunde Specht, Steve März...   acm           book   \n",
       "1205  Bärbel Bolduan, Heinzhermann Reinwald, Gretlie...   acm           book   \n",
       "1500  Maëlyss Berns. 2019. Vermeidung von Interferen...   acm      phdthesis   \n",
       "1501  Mélys Berthot. 2017. Efficient storage and ana...   acm      phdthesis   \n",
       "1502  Cléa Bertot. 2012. Harvesting and summarizing ...   acm      phdthesis   \n",
       "1503  Anaëlle Bettam. 2015. Cardiac motion and funct...   acm      phdthesis   \n",
       "1504  Aloïs Bette. 2019. Similarity Search Algorithm...   acm      phdthesis   \n",
       "1505  Mélissandre Bidder. 2018. Real-time event dete...   acm      phdthesis   \n",
       "\n",
       "                                                 BibTeX  \\\n",
       "1     @article{zhang-etal-2017-aspect,\\n    title = ...   \n",
       "2     @article{gallagher-etal-2017-anchored,\\n    ti...   \n",
       "3     @article{joty-etal-2017-discourse,\\n    title ...   \n",
       "4     @article{rozovskaya-etal-2017-adapting,\\n    t...   \n",
       "5     @article{kadar-etal-2017-representation,\\n    ...   \n",
       "300   @proceedings{ws-2019-international-resources,\\...   \n",
       "301   @proceedings{ws-2019-interactive,\\n    title =...   \n",
       "302   @proceedings{ws-2019-international-ai,\\n    ti...   \n",
       "303   @proceedings{ws-2019-discourse-structure,\\n   ...   \n",
       "304   @proceedings{ws-2019-universal,\\n    title = \"...   \n",
       "305   @proceedings{ws-2019-quantitative,\\n    title ...   \n",
       "600   @inproceedings{tseriotou-etal-2024-sig,\\n    t...   \n",
       "601   @inproceedings{r-etal-2024-shot,\\n    title = ...   \n",
       "602   @inproceedings{varghese-etal-2024-neural,\\n   ...   \n",
       "603   @inproceedings{k-etal-2024-dataset,\\n    title...   \n",
       "604   @inproceedings{bade-etal-2024-social,\\n    tit...   \n",
       "605   @inproceedings{lakshminarayanan-prudhommeaux-2...   \n",
       "900   @InCollection{worzel:2002:GPTP,\\n  author =   ...   \n",
       "901   @InCollection{worzel:2005:GPTP,\\n  author =   ...   \n",
       "902   @InCollection{Worzel:2006:GPTP,\\n  author =   ...   \n",
       "903   @InCollection{wu:1999:EIYNAO,\\n  author =     ...   \n",
       "904   @InCollection{yabuki04genetic,\\n  author =    ...   \n",
       "905   @InCollection{yang:1999:SCGA,\\n  author =     ...   \n",
       "1200  @book{Bock.2005,\\n author = {Bock, Kareen and ...   \n",
       "1201  @book{Boecker.1990,\\n author = {Boecker, Thild...   \n",
       "1202  @book{Bohmer.2007,\\n author = {B{\\\"o}hmer, Vin...   \n",
       "1203  @book{Bohner.2009,\\n author = {Bohner, Trautch...   \n",
       "1204  @book{Bohnet.2012,\\n author = {Bohnet, Ingetru...   \n",
       "1205  @book{Bolduan.2021,\\n author = {Bolduan, B{\\\"a...   \n",
       "1500  @phdthesis{Berns.2019,\\n author = {Berns, Ma{\\...   \n",
       "1501  @phdthesis{Berthot.2017,\\n author = {Berthot, ...   \n",
       "1502  @phdthesis{Bertot.2012,\\n author = {Bertot, Cl...   \n",
       "1503  @phdthesis{Bettam.2015,\\n author = {Bettam, An...   \n",
       "1504  @phdthesis{Bette.2019,\\n author = {Bette, Alo{...   \n",
       "1505  @phdthesis{Bidder.2018,\\n author = {Bidder, M{...   \n",
       "\n",
       "                                         Modellergebnis  \n",
       "1     @article{Zhang_2017, \\nauthor={Yuan Zhang and ...  \n",
       "2     @article{Gallagher_2017, \\nauthor={Ryan J. Gal...  \n",
       "3     @article{Joty_2017, \\nauthor={Shafiq Joty and ...  \n",
       "4     @article{Rozovskaya_2017, \\nauthor={Alla Rozov...  \n",
       "5     @article{Kádár_2017, \\nauthor={Ákos Kádár and ...  \n",
       "300   @phdthesis{Ševčíková_2019, \\nauthor={Magda Šev...  \n",
       "301   @phdthesis{Alonso_2019, \\nauthor={Jose M. Alon...  \n",
       "302   @phdthesis{Kano_2019, \\nauthor={Yoshinobu Kano...  \n",
       "303   @phdthesis{Balakrishnan_2019, \\nauthor={Anusha...  \n",
       "304   @phdthesis{Rademaker_2019, \\nauthor={Alexandre...  \n",
       "305   @phdthesis{Chen_2019, \\nauthor={Xinying Chen a...  \n",
       "600   @inproceedings{Tseriotou_2024, \\nauthor={Talia...  \n",
       "601   @inproceedings{R_2024, \\nauthor={Jairam R and ...  \n",
       "602   @phdthesis{Varghese_2024, \\nauthor={Christeena...  \n",
       "603   @phdthesis{K_2024, \\nauthor={Hariprasath Devik...  \n",
       "604   @phdthesis{Bade_2024, \\nauthor={Girma Bade and...  \n",
       "605   @phdthesis{Lakshminarayanan_2024, \\nauthor={Vi...  \n",
       "900   @incollection{Worzel_2003, \\nauthor={Bill Worz...  \n",
       "901   @incollection{Worzel_2005, \\nauthor={Bill Worz...  \n",
       "902   @incollection{Worzel_2006, \\nauthor={W. P. Wor...  \n",
       "903   @incollection{Wu_1999, \\nauthor={Gary I. Wu},\\...  \n",
       "904   @incollection{Yabuki_2004, \\nauthor={Taro Yabu...  \n",
       "905   @incollection{Yang_1999, \\nauthor={Mei-Wan Yan...  \n",
       "1200  @book{Bock_2005, \\nauthor={Kareen Bock and Ane...  \n",
       "1201  @book{Boecker_1990, \\nauthor={Thilde Boecker a...  \n",
       "1202  @book{Böhmer_2007, \\nauthor={Vincenz Böhmer an...  \n",
       "1203  @book{Bohner_2009, \\nauthor={Trautchen Bohner ...  \n",
       "1204  @book{Bohnet_2012, \\nauthor={Ingetrud Bohnet a...  \n",
       "1205  @book{Bolduan_2021, \\nauthor={Bärbel Bolduan a...  \n",
       "1500  @phdthesis{Berns_2019, \\nauthor={Maëlyss Berns...  \n",
       "1501  @incollection{Berthot_2017, \\nauthor={Mélys Be...  \n",
       "1502  @incollection{Bertot_2012, \\nauthor={Cléa Bert...  \n",
       "1503  @incollection{Bettam_2015, \\nauthor={Anaëlle B...  \n",
       "1504  @phdthesis{Bette_2019, \\nauthor={Aloïs Bette},...  \n",
       "1505  @phdthesis{Bidder_2018, \\nauthor={Mélissandre ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Ausführung der Codezelle hat 255.6526 Sekunden gedauert.\n"
     ]
    }
   ],
   "source": [
    "# Einlesen von Testdaten für eine vorläufige Evaluation\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Startzeit erfassen\n",
    "start_time = time.time()\n",
    "\n",
    "# Unterdrücken aller Warnungen\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Setze das Logging-Level für Transformers auf ERROR\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Pfad zur CSV-Datei\n",
    "csv_file_path = 'Trainingsdaten/Testdaten/test_acm.csv'\n",
    "\n",
    "# Einlesen der CSV-Datei in einen Pandas DataFrame mit dem Delimiter |\n",
    "df = pd.read_csv(csv_file_path, delimiter='|')\n",
    "df = df.iloc[list(range(1, 6)) + list(range(300, 306)) + list(range(600, 606)) + list(range(900, 906)) + list(range(1200, 1206)) + list(range(1500, 1506))]\n",
    "\n",
    "# Neue Spalte für Ergebnisse oder Fehler erstellen\n",
    "df['Modellergebnis'] = None\n",
    "\n",
    "# Fehlerbehandlung und Anwenden der Funktion\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        result = create_bibtex(row['Referenzstring'])\n",
    "        df.at[index, 'Modellergebnis'] = result\n",
    "    except Exception as e:\n",
    "        df.at[index, 'Modellergebnis'] = f\"Fehler: {e}\"\n",
    "\n",
    "display(df)\n",
    "\n",
    "df.to_csv('Trainingsdaten/Testdaten/test_acm_result3.csv', sep='|', index=False)\n",
    "#print(create_bibtex(text))\n",
    "\n",
    "# Endzeit erfassen\n",
    "end_time = time.time()\n",
    "\n",
    "# Ausführungszeit berechnen\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Die Ausführung der Codezelle hat {execution_time:.4f} Sekunden gedauert.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1cf33fa-012f-4140-982b-1b3b61591cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: acm, phdthesis\n",
      "  Style Literaturtyp        Zeit\n",
      "0   acm    phdthesis  523.157764\n",
      "Start: acm, article\n",
      "  Style Literaturtyp        Zeit\n",
      "0   acm      article  521.422992\n",
      "Start: acm, book\n",
      "  Style Literaturtyp        Zeit\n",
      "0   acm         book  469.933614\n",
      "Start: acm, inproceedings\n",
      "  Style   Literaturtyp        Zeit\n",
      "0   acm  inproceedings  747.082604\n",
      "Start: acm, proceedings\n",
      "  Style Literaturtyp        Zeit\n",
      "0   acm  proceedings  557.440192\n",
      "Start: acm, incollection\n",
      "  Style  Literaturtyp        Zeit\n",
      "0   acm  incollection  543.404095\n",
      "Start: apa, phdthesis\n",
      "  Style Literaturtyp        Zeit\n",
      "0   apa    phdthesis  517.857665\n",
      "Start: apa, article\n",
      "  Style Literaturtyp        Zeit\n",
      "0   apa      article  511.662193\n",
      "Start: apa, book\n",
      "  Style Literaturtyp        Zeit\n",
      "0   apa         book  460.855118\n",
      "Start: apa, inproceedings\n",
      "  Style   Literaturtyp        Zeit\n",
      "0   apa  inproceedings  750.054719\n",
      "Start: apa, proceedings\n",
      "  Style Literaturtyp        Zeit\n",
      "0   apa  proceedings  566.505181\n",
      "Start: apa, incollection\n",
      "  Style  Literaturtyp        Zeit\n",
      "0   apa  incollection  561.860037\n",
      "Start: harvard, phdthesis\n",
      "     Style Literaturtyp        Zeit\n",
      "0  harvard    phdthesis  526.631533\n",
      "Start: harvard, article\n",
      "     Style Literaturtyp        Zeit\n",
      "0  harvard      article  513.078955\n",
      "Start: harvard, book\n",
      "     Style Literaturtyp        Zeit\n",
      "0  harvard         book  471.085774\n",
      "Start: harvard, inproceedings\n",
      "     Style   Literaturtyp        Zeit\n",
      "0  harvard  inproceedings  754.353433\n",
      "Start: harvard, proceedings\n",
      "     Style Literaturtyp        Zeit\n",
      "0  harvard  proceedings  553.671736\n",
      "Start: harvard, incollection\n",
      "     Style  Literaturtyp        Zeit\n",
      "0  harvard  incollection  538.892081\n",
      "Start: ieee, phdthesis\n",
      "  Style Literaturtyp        Zeit\n",
      "0  ieee    phdthesis  519.877536\n",
      "Start: ieee, article\n",
      "  Style Literaturtyp        Zeit\n",
      "0  ieee      article  518.589685\n",
      "Start: ieee, book\n",
      "  Style Literaturtyp        Zeit\n",
      "0  ieee         book  468.427188\n",
      "Start: ieee, inproceedings\n",
      "  Style   Literaturtyp        Zeit\n",
      "0  ieee  inproceedings  744.451716\n",
      "Start: ieee, proceedings\n",
      "  Style Literaturtyp        Zeit\n",
      "0  ieee  proceedings  547.658993\n",
      "Start: ieee, incollection\n",
      "  Style  Literaturtyp       Zeit\n",
      "0  ieee  incollection  548.99464\n",
      "Start: mla, phdthesis\n",
      "  Style Literaturtyp        Zeit\n",
      "0   mla    phdthesis  510.618855\n",
      "Start: mla, article\n",
      "  Style Literaturtyp        Zeit\n",
      "0   mla      article  507.333954\n",
      "Start: mla, book\n",
      "  Style Literaturtyp        Zeit\n",
      "0   mla         book  470.602994\n",
      "Start: mla, inproceedings\n",
      "  Style   Literaturtyp        Zeit\n",
      "0   mla  inproceedings  746.117066\n",
      "Start: mla, proceedings\n",
      "  Style Literaturtyp        Zeit\n",
      "0   mla  proceedings  573.924512\n",
      "Start: mla, incollection\n",
      "  Style  Literaturtyp        Zeit\n",
      "0   mla  incollection  527.784579\n",
      "Start: plain, phdthesis\n",
      "   Style Literaturtyp        Zeit\n",
      "0  plain    phdthesis  510.709909\n",
      "Start: plain, article\n",
      "   Style Literaturtyp        Zeit\n",
      "0  plain      article  508.224816\n",
      "Start: plain, book\n",
      "   Style Literaturtyp        Zeit\n",
      "0  plain         book  471.831339\n",
      "Start: plain, inproceedings\n",
      "   Style   Literaturtyp        Zeit\n",
      "0  plain  inproceedings  717.278988\n",
      "Start: plain, proceedings\n",
      "   Style Literaturtyp        Zeit\n",
      "0  plain  proceedings  544.956441\n",
      "Start: plain, incollection\n",
      "   Style  Literaturtyp        Zeit\n",
      "0  plain  incollection  534.501984\n"
     ]
    }
   ],
   "source": [
    "# Einlesen aller Testdaten und Prozessierung von 36 x 100 Beispielen\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Unterdrücken aller Warnungen\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Setze das Logging-Level für Transformers auf ERROR\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Einlesen der CSV-Datei in einen Pandas DataFrame mit dem Delimiter |\n",
    "df_acm = pd.read_csv('Trainingsdaten/Testdaten/test_acm.csv', delimiter='|')\n",
    "df_apa = pd.read_csv('Trainingsdaten/Testdaten/test_apa.csv', delimiter='|')\n",
    "df_harv = pd.read_csv('Trainingsdaten/Testdaten/test_harv.csv', delimiter='|')\n",
    "df_ieee = pd.read_csv('Trainingsdaten/Testdaten/test_ieee.csv', delimiter='|')\n",
    "df_mla = pd.read_csv('Trainingsdaten/Testdaten/test_mla.csv', delimiter='|')\n",
    "df_plain = pd.read_csv('Trainingsdaten/Testdaten/test_plain.csv', delimiter='|')\n",
    "df = pd.concat([df_acm, df_apa, df_harv, df_ieee, df_mla, df_plain])\n",
    "\n",
    "styles = ['acm', 'apa', 'harvard', 'ieee', 'mla', 'plain']\n",
    "types = ['phdthesis', 'article', 'book', 'inproceedings', 'proceedings', 'incollection']\n",
    "df_result = pd.DataFrame()\n",
    "df_time = pd.DataFrame()\n",
    "\n",
    "# Durchlaufen der Stile und Literaturtypen und Filterung der ersten 100 Beispiele pro Attributkombination\n",
    "# Konvertierung von je 100 Beispielen plus Zeitmessung\n",
    "for style in styles:\n",
    "    for type in types:\n",
    "        print(\"Start: \" + style + \", \" + type)\n",
    "        df_temp = df[(df[\"Style\"]==style) & (df[\"Literaturtyp\"]==type)]\n",
    "        df_temp = df_temp.head(100)\n",
    "\n",
    "        # Neue Spalte für Ergebnisse oder Fehler erstellen\n",
    "        df_temp['Modellergebnis'] = None\n",
    "\n",
    "        # Startzeit erfassen\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Fehlerbehandlung und Anwenden des Modells\n",
    "        for index, row in df_temp.iterrows():\n",
    "            try:\n",
    "                result = create_bibtex(row['Referenzstring'])\n",
    "                df_temp.at[index, 'Modellergebnis'] = result\n",
    "            except Exception as e:\n",
    "                df_temp.at[index, 'Modellergebnis'] = f\"Fehler: {e}\"\n",
    "\n",
    "        # Endzeit erfassen\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Ausführungszeit berechnen (in Sekunden)\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "        # Temporäres DataFrame an das Haupt-DataFrame anhängen\n",
    "        df_result = pd.concat([df_result, df_temp])\n",
    "        #display(df_temp)\n",
    "\n",
    "        df_time_temp = pd.DataFrame({'Style': [style], 'Literaturtyp': [type], 'Zeit': [execution_time]})\n",
    "        print(df_time_temp)\n",
    "        df_time = pd.concat([df_time, df_time_temp])\n",
    "\n",
    "# Export der Ergebnisse\n",
    "df_result.to_csv('Trainingsdaten/Testdaten/test_result_V2.csv', sep='|', index=False)\n",
    "df_time.to_csv('Trainingsdaten/Testdaten/test_time_V2.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651ff6f-247d-45bc-b111-ce270d66be7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
